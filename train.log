Traceback (most recent call last):
  File "train.py", line 6, in <module>
    from options.train_options import TrainOptions
  File "E:\Study\视听导\project\Deep-MIML-Network\options\train_options.py", line 4, in <module>
    from .base_options import BaseOptions
  File "E:\Study\视听导\project\Deep-MIML-Network\options\base_options.py", line 7, in <module>
    import torch
ModuleNotFoundError: No module named 'torch'
Traceback (most recent call last):
  File "train.py", line 6, in <module>
    from options.train_options import TrainOptions
  File "E:\Study\视听导\project\Deep-MIML-Network\options\train_options.py", line 4, in <module>
    from .base_options import BaseOptions
  File "E:\Study\视听导\project\Deep-MIML-Network\options\base_options.py", line 7, in <module>
    import torch
ModuleNotFoundError: No module named 'torch'
Traceback (most recent call last):
  File "train.py", line 9, in <module>
    from tensorboardX import SummaryWriter
ModuleNotFoundError: No module named 'tensorboardX'
usage: train.py [-h] [--K K] [--L L] [--F F] [--num_of_bases NUM_OF_BASES]
                [--num_of_fc NUM_OF_FC] [--with_batchnorm]
                [--fc_dimension FC_DIMENSION] --HDF5FileRoot HDF5FILEROOT
                [--gpu_ids GPU_IDS] [--name NAME]
                [--checkpoints_dir CHECKPOINTS_DIR]
                [--model {MIML,KL_divergence}] [--batchSize BATCHSIZE]
                [--nThreads NTHREADS] [--norm {l1,l2,max,none}]
                [--dataset {musicInstruments,animals,vehicles,all}]
                [--using_multi_labels]
                [--multi_label_threshold MULTI_LABEL_THRESHOLD]
                [--selected_classes] [--zeroCenterInput]
                [--display_freq DISPLAY_FREQ]
                [--save_epoch_freq SAVE_EPOCH_FREQ]
                [--save_latest_freq SAVE_LATEST_FREQ] [--continue_train]
                [--epoch_count EPOCH_COUNT] [--learning_rate LEARNING_RATE]
                [--learning_rate_decrease_itr LEARNING_RATE_DECREASE_ITR]
                [--decay_factor DECAY_FACTOR] [--niter NITER]
                [--init_type {normal,xavier,kaiming,orthogonal}]
                [--measure_time] [--validation_on]
                [--validation_freq VALIDATION_FREQ]
                [--validation_batches VALIDATION_BATCHES] [--with_softmax]
train.py: error: unrecognized arguments: --using_multilabels
D:\Anaconda3\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
------------ Options -------------
F: 2401
HDF5FileRoot: D:/Git/datasets/
K: 4
L: 15
batchSize: 256
checkpoints_dir: checkpoints
continue_train: True
dataset: musicInstruments
decay_factor: 0.94
display_freq: 10
epoch_count: 0
fc_dimension: 1024
gpu_ids: []
init_type: normal
isTrain: True
learning_rate: 0.001
learning_rate_decrease_itr: 5
measure_time: False
mode: train
model: MIML
multi_label_threshold: 0.3
nThreads: 2
name: deepMIML
niter: 300
norm: max
num_of_bases: 25
num_of_fc: 1
save_epoch_freq: 5
save_latest_freq: 500
selected_classes: True
using_multi_labels: True
validation_batches: 10
validation_freq: 50
validation_on: True
with_batchnorm: True
with_softmax: False
zeroCenterInput: False
-------------- End ----------------
CustomDatasetDataLoader
dataset [MIMLDataset] was created
Traceback (most recent call last):
  File "train.py", line 13, in <module>
    data_loader = CreateDataLoader(opt)
  File "E:\Study\视听导\project\Deep-MIML-Network\data\data_loader.py", line 8, in CreateDataLoader
    data_loader.initialize(opt)
  File "E:\Study\视听导\project\Deep-MIML-Network\data\custom_dataset_data_loader.py", line 29, in initialize
    self.dataset = CreateDataset(opt)
  File "E:\Study\视听导\project\Deep-MIML-Network\data\custom_dataset_data_loader.py", line 19, in CreateDataset
    dataset.initialize(opt)
  File "E:\Study\视听导\project\Deep-MIML-Network\data\MIML_dataset.py", line 89, in initialize
    h5f = h5py.File(h5f_path, 'r')
  File "D:\Anaconda3\lib\site-packages\h5py\_hl\files.py", line 269, in __init__
    fid = make_fid(name, mode, userblock_size, fapl, swmr=swmr)
  File "D:\Anaconda3\lib\site-packages\h5py\_hl\files.py", line 99, in make_fid
    fid = h5f.open(name, flags, fapl=fapl)
  File "h5py\_objects.pyx", line 54, in h5py._objects.with_phil.wrapper
  File "h5py\_objects.pyx", line 55, in h5py._objects.with_phil.wrapper
  File "h5py\h5f.pyx", line 78, in h5py.h5f.open
OSError: Unable to open file (unable to open file: name = 'D:/Git/datasets/train.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)
D:\Anaconda3\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
------------ Options -------------
F: 2401
HDF5FileRoot: D:/Git/datasets/
K: 4
L: 15
batchSize: 256
checkpoints_dir: checkpoints
continue_train: True
dataset: musicInstruments
decay_factor: 0.94
display_freq: 10
epoch_count: 0
fc_dimension: 1024
gpu_ids: []
init_type: normal
isTrain: True
learning_rate: 0.001
learning_rate_decrease_itr: 5
measure_time: False
mode: train
model: MIML
multi_label_threshold: 0.3
nThreads: 2
name: deepMIML
niter: 300
norm: max
num_of_bases: 25
num_of_fc: 1
save_epoch_freq: 5
save_latest_freq: 500
selected_classes: True
using_multi_labels: True
validation_batches: 10
validation_freq: 50
validation_on: True
with_batchnorm: True
with_softmax: False
zeroCenterInput: False
-------------- End ----------------
CustomDatasetDataLoader
dataset [MIMLDataset] was created
Traceback (most recent call last):
  File "train.py", line 13, in <module>
    data_loader = CreateDataLoader(opt)
  File "E:\Study\Vision\project\Deep-MIML-Network\data\data_loader.py", line 8, in CreateDataLoader
    data_loader.initialize(opt)
  File "E:\Study\Vision\project\Deep-MIML-Network\data\custom_dataset_data_loader.py", line 29, in initialize
    self.dataset = CreateDataset(opt)     #根据opt.model创建一个数据集类“MIMLDataset/KLdataset”，并初始化（包括读取h5文件，放入self.bases,self.labels）
  File "E:\Study\Vision\project\Deep-MIML-Network\data\custom_dataset_data_loader.py", line 19, in CreateDataset
    dataset.initialize(opt)
  File "E:\Study\Vision\project\Deep-MIML-Network\data\MIML_dataset.py", line 89, in initialize
    h5f = h5py.File(h5f_path, 'r')
  File "D:\Anaconda3\lib\site-packages\h5py\_hl\files.py", line 269, in __init__
    fid = make_fid(name, mode, userblock_size, fapl, swmr=swmr)
  File "D:\Anaconda3\lib\site-packages\h5py\_hl\files.py", line 99, in make_fid
    fid = h5f.open(name, flags, fapl=fapl)
  File "h5py\_objects.pyx", line 54, in h5py._objects.with_phil.wrapper
  File "h5py\_objects.pyx", line 55, in h5py._objects.with_phil.wrapper
  File "h5py\h5f.pyx", line 78, in h5py.h5f.open
OSError: Unable to open file (unable to open file: name = 'D:/Git/datasets/train.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)
D:\Anaconda3\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
------------ Options -------------
F: 2401
HDF5FileRoot: e:StudyVisionprojectDeep-MIML-Networkdataset
K: 4
L: 15
batchSize: 256
checkpoints_dir: checkpoints
continue_train: True
dataset: musicInstruments
decay_factor: 0.94
display_freq: 10
epoch_count: 0
fc_dimension: 1024
gpu_ids: []
init_type: normal
isTrain: True
learning_rate: 0.001
learning_rate_decrease_itr: 5
measure_time: False
mode: train
model: MIML
multi_label_threshold: 0.3
nThreads: 2
name: deepMIML
niter: 300
norm: max
num_of_bases: 25
num_of_fc: 1
save_epoch_freq: 5
save_latest_freq: 500
selected_classes: True
using_multi_labels: True
validation_batches: 10
validation_freq: 50
validation_on: True
with_batchnorm: True
with_softmax: False
zeroCenterInput: False
-------------- End ----------------
CustomDatasetDataLoader
dataset [MIMLDataset] was created
Traceback (most recent call last):
  File "train.py", line 13, in <module>
    data_loader = CreateDataLoader(opt)
  File "E:\Study\Vision\project\Deep-MIML-Network\data\data_loader.py", line 8, in CreateDataLoader
    data_loader.initialize(opt)
  File "E:\Study\Vision\project\Deep-MIML-Network\data\custom_dataset_data_loader.py", line 29, in initialize
    self.dataset = CreateDataset(opt)     #根据opt.model创建一个数据集类“MIMLDataset/KLdataset”，并初始化（包括读取h5文件，放入self.bases,self.labels）
  File "E:\Study\Vision\project\Deep-MIML-Network\data\custom_dataset_data_loader.py", line 19, in CreateDataset
    dataset.initialize(opt)
  File "E:\Study\Vision\project\Deep-MIML-Network\data\MIML_dataset.py", line 89, in initialize
    h5f = h5py.File(h5f_path, 'r')
  File "D:\Anaconda3\lib\site-packages\h5py\_hl\files.py", line 269, in __init__
    fid = make_fid(name, mode, userblock_size, fapl, swmr=swmr)
  File "D:\Anaconda3\lib\site-packages\h5py\_hl\files.py", line 99, in make_fid
    fid = h5f.open(name, flags, fapl=fapl)
  File "h5py\_objects.pyx", line 54, in h5py._objects.with_phil.wrapper
  File "h5py\_objects.pyx", line 55, in h5py._objects.with_phil.wrapper
  File "h5py\h5f.pyx", line 78, in h5py.h5f.open
OSError: Unable to open file (unable to open file: name = 'e:StudyVisionprojectDeep-MIML-Networkdataset\train.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)
D:\Anaconda3\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
------------ Options -------------
F: 2401
HDF5FileRoot: e:\Study\Vision\project\Deep-MIML-Network\dataset
K: 4
L: 15
batchSize: 256
checkpoints_dir: checkpoints
continue_train: True
dataset: musicInstruments
decay_factor: 0.94
display_freq: 10
epoch_count: 0
fc_dimension: 1024
gpu_ids: []
init_type: normal
isTrain: True
learning_rate: 0.001
learning_rate_decrease_itr: 5
measure_time: False
mode: train
model: MIML
multi_label_threshold: 0.3
nThreads: 2
name: deepMIML
niter: 300
norm: max
num_of_bases: 25
num_of_fc: 1
save_epoch_freq: 5
save_latest_freq: 500
selected_classes: True
using_multi_labels: True
validation_batches: 10
validation_freq: 50
validation_on: True
with_batchnorm: True
with_softmax: False
zeroCenterInput: False
-------------- End ----------------
CustomDatasetDataLoader
dataset [MIMLDataset] was created
#training images = 15
CustomDatasetDataLoader
dataset [MIMLDataset] was created
Traceback (most recent call last):
  File "train.py", line 22, in <module>
    data_loader_val = CreateDataLoader(opt)
  File "E:\Study\Vision\project\Deep-MIML-Network\data\data_loader.py", line 8, in CreateDataLoader
    data_loader.initialize(opt)
  File "E:\Study\Vision\project\Deep-MIML-Network\data\custom_dataset_data_loader.py", line 29, in initialize
    self.dataset = CreateDataset(opt)     #根据opt.model创建一个数据集类“MIMLDataset/KLdataset”，并初始化（包括读取h5文件，放入self.bases,self.labels）
  File "E:\Study\Vision\project\Deep-MIML-Network\data\custom_dataset_data_loader.py", line 19, in CreateDataset
    dataset.initialize(opt)
  File "E:\Study\Vision\project\Deep-MIML-Network\data\MIML_dataset.py", line 89, in initialize
    h5f = h5py.File(h5f_path, 'r')
  File "D:\Anaconda3\lib\site-packages\h5py\_hl\files.py", line 269, in __init__
    fid = make_fid(name, mode, userblock_size, fapl, swmr=swmr)
  File "D:\Anaconda3\lib\site-packages\h5py\_hl\files.py", line 99, in make_fid
    fid = h5f.open(name, flags, fapl=fapl)
  File "h5py\_objects.pyx", line 54, in h5py._objects.with_phil.wrapper
  File "h5py\_objects.pyx", line 55, in h5py._objects.with_phil.wrapper
  File "h5py\h5f.pyx", line 78, in h5py.h5f.open
OSError: Unable to open file (unable to open file: name = 'e:\Study\Vision\project\Deep-MIML-Network\dataset\val.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)
D:\Anaconda3\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
E:\Study\Vision\project\Deep-MIML-Network\models\networks.py:13: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.
  init.normal(m.weight.data, 0.0, 0.02)
E:\Study\Vision\project\Deep-MIML-Network\models\networks.py:17: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.
  init.normal(m.weight.data, 1.0, 0.02)
E:\Study\Vision\project\Deep-MIML-Network\models\networks.py:18: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.
  init.constant(m.bias.data, 0.0)
Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 114, in _main
    prepare(preparation_data)
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 225, in prepare
    _fixup_main_from_path(data['init_main_from_path'])
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 277, in _fixup_main_from_path
    run_name="__mp_main__")
  File "D:\Anaconda3\lib\runpy.py", line 263, in run_path
    pkg_name=pkg_name, script_name=fname)
  File "D:\Anaconda3\lib\runpy.py", line 96, in _run_module_code
    mod_name, mod_spec, pkg_name, script_name)
  File "D:\Anaconda3\lib\runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "E:\Study\Vision\project\Deep-MIML-Network\train.py", line 65, in <module>
    for i, data in enumerate(dataset):
  File "E:\Study\Vision\project\Deep-MIML-Network\data\custom_dataset_data_loader.py", line 43, in __iter__
    for i, data in enumerate(self.dataloader):
  File "D:\Anaconda3\lib\site-packages\torch\utils\data\dataloader.py", line 819, in __iter__
    return _DataLoaderIter(self)
  File "D:\Anaconda3\lib\site-packages\torch\utils\data\dataloader.py", line 560, in __init__
    w.start()
  File "D:\Anaconda3\lib\multiprocessing\process.py", line 105, in start
    self._popen = self._Popen(self)
  File "D:\Anaconda3\lib\multiprocessing\context.py", line 223, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "D:\Anaconda3\lib\multiprocessing\context.py", line 322, in _Popen
    return Popen(process_obj)
  File "D:\Anaconda3\lib\multiprocessing\popen_spawn_win32.py", line 33, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 143, in get_preparation_data
    _check_not_importing_main()
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 136, in _check_not_importing_main
    is not going to be frozen to produce an executable.''')
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.
------------ Options -------------
F: 2401
HDF5FileRoot: e:\Study\Vision\project\Deep-MIML-Network\dataset
K: 4
L: 15
batchSize: 256
checkpoints_dir: checkpoints
continue_train: True
dataset: musicInstruments
decay_factor: 0.94
display_freq: 10
epoch_count: 0
fc_dimension: 1024
gpu_ids: []
init_type: normal
isTrain: True
learning_rate: 0.001
learning_rate_decrease_itr: 5
measure_time: False
mode: train
model: MIML
multi_label_threshold: 0.3
nThreads: 2
name: deepMIML
niter: 300
norm: max
num_of_bases: 25
num_of_fc: 1
save_epoch_freq: 5
save_latest_freq: 500
selected_classes: True
using_multi_labels: True
validation_batches: 10
validation_freq: 50
validation_on: False
with_batchnorm: True
with_softmax: False
zeroCenterInput: False
-------------- End ----------------
CustomDatasetDataLoader
dataset [MIMLDataset] was created
#training images = 15
MIML
initialization method [normal]
model [MIMLModel] was created
starting from epoch  0
D:\Anaconda3\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
E:\Study\Vision\project\Deep-MIML-Network\models\networks.py:13: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.
  init.normal(m.weight.data, 0.0, 0.02)
E:\Study\Vision\project\Deep-MIML-Network\models\networks.py:17: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.
  init.normal(m.weight.data, 1.0, 0.02)
E:\Study\Vision\project\Deep-MIML-Network\models\networks.py:18: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.
  init.constant(m.bias.data, 0.0)
Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 114, in _main
    prepare(preparation_data)
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 225, in prepare
    _fixup_main_from_path(data['init_main_from_path'])
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 277, in _fixup_main_from_path
    run_name="__mp_main__")
  File "D:\Anaconda3\lib\runpy.py", line 263, in run_path
    pkg_name=pkg_name, script_name=fname)
  File "D:\Anaconda3\lib\runpy.py", line 96, in _run_module_code
    mod_name, mod_spec, pkg_name, script_name)
  File "D:\Anaconda3\lib\runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "E:\Study\Vision\project\Deep-MIML-Network\train.py", line 65, in <module>
    for i, data in enumerate(dataset):
  File "E:\Study\Vision\project\Deep-MIML-Network\data\custom_dataset_data_loader.py", line 43, in __iter__
    for i, data in enumerate(self.dataloader):
  File "D:\Anaconda3\lib\site-packages\torch\utils\data\dataloader.py", line 819, in __iter__
    return _DataLoaderIter(self)
  File "D:\Anaconda3\lib\site-packages\torch\utils\data\dataloader.py", line 560, in __init__
    w.start()
  File "D:\Anaconda3\lib\multiprocessing\process.py", line 105, in start
    self._popen = self._Popen(self)
  File "D:\Anaconda3\lib\multiprocessing\context.py", line 223, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "D:\Anaconda3\lib\multiprocessing\context.py", line 322, in _Popen
    return Popen(process_obj)
  File "D:\Anaconda3\lib\multiprocessing\popen_spawn_win32.py", line 33, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 143, in get_preparation_data
    _check_not_importing_main()
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 136, in _check_not_importing_main
    is not going to be frozen to produce an executable.''')
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.
------------ Options -------------
F: 2401
HDF5FileRoot: e:\Study\Vision\project\Deep-MIML-Network\dataset
K: 4
L: 15
batchSize: 256
checkpoints_dir: checkpoints
continue_train: True
dataset: musicInstruments
decay_factor: 0.94
display_freq: 10
epoch_count: 0
fc_dimension: 1024
gpu_ids: []
init_type: normal
isTrain: True
learning_rate: 0.001
learning_rate_decrease_itr: 5
measure_time: False
mode: train
model: MIML
multi_label_threshold: 0.3
nThreads: 2
name: deepMIML
niter: 300
norm: max
num_of_bases: 25
num_of_fc: 1
save_epoch_freq: 5
save_latest_freq: 500
selected_classes: True
using_multi_labels: True
validation_batches: 10
validation_freq: 50
validation_on: False
with_batchnorm: True
with_softmax: False
zeroCenterInput: False
-------------- End ----------------
CustomDatasetDataLoader
dataset [MIMLDataset] was created
#training images = 15
MIML
initialization method [normal]
model [MIMLModel] was created
starting from epoch  0
D:\Anaconda3\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
------------ Options -------------
F: 2401
HDF5FileRoot: e:\Study\Vision\project\Deep-MIML-Network\dataset
K: 4
L: 15
batchSize: 256
checkpoints_dir: checkpoints
continue_train: True
dataset: musicInstruments
decay_factor: 0.94
display_freq: 10
epoch_count: 0
fc_dimension: 1024
gpu_ids: []
init_type: normal
isTrain: True
learning_rate: 0.001
learning_rate_decrease_itr: 5
measure_time: False
mode: train
model: KL_divergence
multi_label_threshold: 0.3
nThreads: 2
name: deepMIML
niter: 300
norm: max
num_of_bases: 25
num_of_fc: 1
save_epoch_freq: 5
save_latest_freq: 500
selected_classes: True
using_multi_labels: True
validation_batches: 10
validation_freq: 50
validation_on: False
with_batchnorm: True
with_softmax: False
zeroCenterInput: False
-------------- End ----------------
CustomDatasetDataLoader
dataset [KLDataset] was created
#training images = 15
KL_divergence
loading epoch 0 model to continue training!
Traceback (most recent call last):
  File "train.py", line 35, in <module>
    model = create_model(opt)
  File "E:\Study\Vision\project\Deep-MIML-Network\models\models.py", line 15, in create_model
    model.initialize(opt)
  File "E:\Study\Vision\project\Deep-MIML-Network\models\KL_divergence.py", line 41, in initialize
    self.load_network(self.BasesNet, opt.epoch_count)
AttributeError: 'KLModel' object has no attribute 'load_network'
D:\Anaconda3\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
E:\Study\Vision\project\Deep-MIML-Network\models\networks.py:13: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.
  init.normal(m.weight.data, 0.0, 0.02)
E:\Study\Vision\project\Deep-MIML-Network\models\networks.py:17: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.
  init.normal(m.weight.data, 1.0, 0.02)
E:\Study\Vision\project\Deep-MIML-Network\models\networks.py:18: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.
  init.constant(m.bias.data, 0.0)
Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 114, in _main
    prepare(preparation_data)
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 225, in prepare
    _fixup_main_from_path(data['init_main_from_path'])
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 277, in _fixup_main_from_path
    run_name="__mp_main__")
  File "D:\Anaconda3\lib\runpy.py", line 263, in run_path
    pkg_name=pkg_name, script_name=fname)
  File "D:\Anaconda3\lib\runpy.py", line 96, in _run_module_code
    mod_name, mod_spec, pkg_name, script_name)
  File "D:\Anaconda3\lib\runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "E:\Study\Vision\project\Deep-MIML-Network\train.py", line 65, in <module>
    for i, data in enumerate(dataset):
  File "E:\Study\Vision\project\Deep-MIML-Network\data\custom_dataset_data_loader.py", line 43, in __iter__
    for i, data in enumerate(self.dataloader):
  File "D:\Anaconda3\lib\site-packages\torch\utils\data\dataloader.py", line 819, in __iter__
    return _DataLoaderIter(self)
  File "D:\Anaconda3\lib\site-packages\torch\utils\data\dataloader.py", line 560, in __init__
    w.start()
  File "D:\Anaconda3\lib\multiprocessing\process.py", line 105, in start
    self._popen = self._Popen(self)
  File "D:\Anaconda3\lib\multiprocessing\context.py", line 223, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "D:\Anaconda3\lib\multiprocessing\context.py", line 322, in _Popen
    return Popen(process_obj)
  File "D:\Anaconda3\lib\multiprocessing\popen_spawn_win32.py", line 33, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 143, in get_preparation_data
    _check_not_importing_main()
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 136, in _check_not_importing_main
    is not going to be frozen to produce an executable.''')
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.
------------ Options -------------
F: 2401
HDF5FileRoot: e:\Study\Vision\project\Deep-MIML-Network\dataset
K: 4
L: 15
batchSize: 256
checkpoints_dir: checkpoints
continue_train: True
dataset: musicInstruments
decay_factor: 0.94
display_freq: 10
epoch_count: 0
fc_dimension: 1024
gpu_ids: []
init_type: normal
isTrain: True
learning_rate: 0.001
learning_rate_decrease_itr: 5
measure_time: False
mode: train
model: MIML
multi_label_threshold: 0.3
nThreads: 2
name: deepMIML
niter: 300
norm: max
num_of_bases: 25
num_of_fc: 1
save_epoch_freq: 5
save_latest_freq: 500
selected_classes: True
using_multi_labels: True
validation_batches: 10
validation_freq: 50
validation_on: False
with_batchnorm: True
with_softmax: False
zeroCenterInput: False
-------------- End ----------------
CustomDatasetDataLoader
dataset [MIMLDataset] was created
#training images = 15
MIML
initialization method [normal]
model [MIMLModel] was created
starting from epoch  0
D:\Anaconda3\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
E:\Study\Vision\project\Deep-MIML-Network\models\networks.py:13: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.
  init.normal(m.weight.data, 0.0, 0.02)
E:\Study\Vision\project\Deep-MIML-Network\models\networks.py:17: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.
  init.normal(m.weight.data, 1.0, 0.02)
E:\Study\Vision\project\Deep-MIML-Network\models\networks.py:18: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.
  init.constant(m.bias.data, 0.0)
Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 114, in _main
    prepare(preparation_data)
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 225, in prepare
    _fixup_main_from_path(data['init_main_from_path'])
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 277, in _fixup_main_from_path
    run_name="__mp_main__")
  File "D:\Anaconda3\lib\runpy.py", line 263, in run_path
    pkg_name=pkg_name, script_name=fname)
  File "D:\Anaconda3\lib\runpy.py", line 96, in _run_module_code
    mod_name, mod_spec, pkg_name, script_name)
  File "D:\Anaconda3\lib\runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "E:\Study\Vision\project\Deep-MIML-Network\train.py", line 65, in <module>
    for i, data in enumerate(dataset):
  File "E:\Study\Vision\project\Deep-MIML-Network\data\custom_dataset_data_loader.py", line 43, in __iter__
    for i, data in enumerate(self.dataloader):
  File "D:\Anaconda3\lib\site-packages\torch\utils\data\dataloader.py", line 819, in __iter__
    return _DataLoaderIter(self)
  File "D:\Anaconda3\lib\site-packages\torch\utils\data\dataloader.py", line 560, in __init__
    w.start()
  File "D:\Anaconda3\lib\multiprocessing\process.py", line 105, in start
    self._popen = self._Popen(self)
  File "D:\Anaconda3\lib\multiprocessing\context.py", line 223, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "D:\Anaconda3\lib\multiprocessing\context.py", line 322, in _Popen
    return Popen(process_obj)
  File "D:\Anaconda3\lib\multiprocessing\popen_spawn_win32.py", line 33, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 143, in get_preparation_data
    _check_not_importing_main()
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 136, in _check_not_importing_main
    is not going to be frozen to produce an executable.''')
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.
------------ Options -------------
F: 2401
HDF5FileRoot: e:\Study\Vision\project\Deep-MIML-Network\dataset
K: 4
L: 15
batchSize: 256
checkpoints_dir: checkpoints
continue_train: True
dataset: musicInstruments
decay_factor: 0.94
display_freq: 10
epoch_count: 0
fc_dimension: 1024
gpu_ids: []
init_type: normal
isTrain: True
learning_rate: 0.001
learning_rate_decrease_itr: 5
measure_time: False
mode: train
model: MIML
multi_label_threshold: 0.3
nThreads: 2
name: deepMIML
niter: 300
norm: max
num_of_bases: 25
num_of_fc: 1
save_epoch_freq: 5
save_latest_freq: 500
selected_classes: True
using_multi_labels: True
validation_batches: 10
validation_freq: 50
validation_on: False
with_batchnorm: True
with_softmax: False
zeroCenterInput: False
-------------- End ----------------
CustomDatasetDataLoader
dataset [MIMLDataset] was created
#training images = 15
MIML
initialization method [normal]
model [MIMLModel] was created
starting from epoch  0
D:\Anaconda3\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
E:\Study\Vision\project\Deep-MIML-Network\models\networks.py:13: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.
  init.normal(m.weight.data, 0.0, 0.02)
E:\Study\Vision\project\Deep-MIML-Network\models\networks.py:17: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.
  init.normal(m.weight.data, 1.0, 0.02)
E:\Study\Vision\project\Deep-MIML-Network\models\networks.py:18: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.
  init.constant(m.bias.data, 0.0)
Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 114, in _main
    prepare(preparation_data)
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 225, in prepare
    _fixup_main_from_path(data['init_main_from_path'])
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 277, in _fixup_main_from_path
    run_name="__mp_main__")
  File "D:\Anaconda3\lib\runpy.py", line 263, in run_path
    pkg_name=pkg_name, script_name=fname)
  File "D:\Anaconda3\lib\runpy.py", line 96, in _run_module_code
    mod_name, mod_spec, pkg_name, script_name)
  File "D:\Anaconda3\lib\runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "E:\Study\Vision\project\Deep-MIML-Network\train.py", line 65, in <module>
    for i, data in enumerate(dataset):
  File "E:\Study\Vision\project\Deep-MIML-Network\data\custom_dataset_data_loader.py", line 43, in __iter__
    for i, data in enumerate(self.dataloader):
  File "D:\Anaconda3\lib\site-packages\torch\utils\data\dataloader.py", line 819, in __iter__
    return _DataLoaderIter(self)
  File "D:\Anaconda3\lib\site-packages\torch\utils\data\dataloader.py", line 560, in __init__
    w.start()
  File "D:\Anaconda3\lib\multiprocessing\process.py", line 105, in start
    self._popen = self._Popen(self)
  File "D:\Anaconda3\lib\multiprocessing\context.py", line 223, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "D:\Anaconda3\lib\multiprocessing\context.py", line 322, in _Popen
    return Popen(process_obj)
  File "D:\Anaconda3\lib\multiprocessing\popen_spawn_win32.py", line 33, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 143, in get_preparation_data
    _check_not_importing_main()
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 136, in _check_not_importing_main
    is not going to be frozen to produce an executable.''')
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.
------------ Options -------------
F: 2401
HDF5FileRoot: e:\Study\Vision\project\Deep-MIML-Network\dataset
K: 4
L: 15
batchSize: 256
checkpoints_dir: checkpoints
continue_train: True
dataset: musicInstruments
decay_factor: 0.94
display_freq: 10
epoch_count: 0
fc_dimension: 1024
gpu_ids: [0]
init_type: normal
isTrain: True
learning_rate: 0.001
learning_rate_decrease_itr: 5
measure_time: False
mode: train
model: MIML
multi_label_threshold: 0.3
nThreads: 1
name: deepMIML
niter: 300
norm: max
num_of_bases: 25
num_of_fc: 1
save_epoch_freq: 5
save_latest_freq: 500
selected_classes: True
using_multi_labels: True
validation_batches: 10
validation_freq: 50
validation_on: False
with_batchnorm: True
with_softmax: False
zeroCenterInput: False
-------------- End ----------------
CustomDatasetDataLoader
dataset [MIMLDataset] was created
#training images = 15
MIML
initialization method [normal]
model [MIMLModel] was created
starting from epoch  0
D:\Anaconda3\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
E:\Study\Vision\project\Deep-MIML-Network\models\networks.py:13: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.
  init.normal(m.weight.data, 0.0, 0.02)
E:\Study\Vision\project\Deep-MIML-Network\models\networks.py:17: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.
  init.normal(m.weight.data, 1.0, 0.02)
E:\Study\Vision\project\Deep-MIML-Network\models\networks.py:18: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.
  init.constant(m.bias.data, 0.0)
Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 114, in _main
    prepare(preparation_data)
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 225, in prepare
    _fixup_main_from_path(data['init_main_from_path'])
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 277, in _fixup_main_from_path
    run_name="__mp_main__")
  File "D:\Anaconda3\lib\runpy.py", line 263, in run_path
    pkg_name=pkg_name, script_name=fname)
  File "D:\Anaconda3\lib\runpy.py", line 96, in _run_module_code
    mod_name, mod_spec, pkg_name, script_name)
  File "D:\Anaconda3\lib\runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "E:\Study\Vision\project\Deep-MIML-Network\train.py", line 65, in <module>
    for i, data in enumerate(dataset):
  File "E:\Study\Vision\project\Deep-MIML-Network\data\custom_dataset_data_loader.py", line 43, in __iter__
    for i, data in enumerate(self.dataloader):
  File "D:\Anaconda3\lib\site-packages\torch\utils\data\dataloader.py", line 819, in __iter__
    return _DataLoaderIter(self)
  File "D:\Anaconda3\lib\site-packages\torch\utils\data\dataloader.py", line 560, in __init__
    w.start()
  File "D:\Anaconda3\lib\multiprocessing\process.py", line 105, in start
    self._popen = self._Popen(self)
  File "D:\Anaconda3\lib\multiprocessing\context.py", line 223, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "D:\Anaconda3\lib\multiprocessing\context.py", line 322, in _Popen
    return Popen(process_obj)
  File "D:\Anaconda3\lib\multiprocessing\popen_spawn_win32.py", line 33, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 143, in get_preparation_data
    _check_not_importing_main()
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 136, in _check_not_importing_main
    is not going to be frozen to produce an executable.''')
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.
------------ Options -------------
F: 2401
HDF5FileRoot: e:\Study\Vision\project\Deep-MIML-Network\dataset
K: 4
L: 15
batchSize: 256
checkpoints_dir: checkpoints
continue_train: True
dataset: musicInstruments
decay_factor: 0.94
display_freq: 10
epoch_count: 0
fc_dimension: 1024
gpu_ids: [0]
init_type: normal
isTrain: True
learning_rate: 0.001
learning_rate_decrease_itr: 5
measure_time: False
mode: train
model: MIML
multi_label_threshold: 0.3
nThreads: 1
name: deepMIML
niter: 300
norm: max
num_of_bases: 25
num_of_fc: 1
save_epoch_freq: 5
save_latest_freq: 500
selected_classes: True
using_multi_labels: True
validation_batches: 10
validation_freq: 50
validation_on: False
with_batchnorm: True
with_softmax: False
zeroCenterInput: False
-------------- End ----------------
CustomDatasetDataLoader
dataset [MIMLDataset] was created
#training images = 15
MIML
initialization method [normal]
model [MIMLModel] was created
starting from epoch  0
------------ Options -------------
F: 2401
HDF5FileRoot: e:\Study\Vision\project\Deep-MIML-Network\dataset
K: 4
L: 15
batchSize: 256
checkpoints_dir: checkpoints
continue_train: True
dataset: musicInstruments
decay_factor: 0.94
display_freq: 10
epoch_count: 0
fc_dimension: 1024
gpu_ids: [0]
init_type: normal
isTrain: True
learning_rate: 0.001
learning_rate_decrease_itr: 5
measure_time: False
mode: train
model: MIML
multi_label_threshold: 0.3
nThreads: 1
name: deepMIML
niter: 300
norm: max
num_of_bases: 25
num_of_fc: 1
save_epoch_freq: 5
save_latest_freq: 500
selected_classes: True
using_multi_labels: True
validation_batches: 10
validation_freq: 50
validation_on: False
with_batchnorm: True
with_softmax: False
zeroCenterInput: False
-------------- End ----------------
CustomDatasetDataLoader
Traceback (most recent call last):
  File "train.py", line 13, in <module>
    data_loader = CreateDataLoader(opt)
  File "E:\Study\Vision\project\Deep-MIML-Network\data\data_loader.py", line 8, in CreateDataLoader
    data_loader.initialize(opt)
  File "E:\Study\Vision\project\Deep-MIML-Network\data\custom_dataset_data_loader.py", line 29, in initialize
    self.dataset = CreateDataset(opt)     #根据opt.model创建一个数据集类“MIMLDataset/KLdataset”，并初始化（包括读取h5文件，放入self.bases,self.labels）
  File "E:\Study\Vision\project\Deep-MIML-Network\data\custom_dataset_data_loader.py", line 10, in CreateDataset
    from data.MIML_dataset import MIMLDataset
  File "E:\Study\Vision\project\Deep-MIML-Network\data\MIML_dataset.py", line 92
    print('MIML dataset initialize suceed')
                                          ^
TabError: inconsistent use of tabs and spaces in indentation
D:\Anaconda3\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
E:\Study\Vision\project\Deep-MIML-Network\models\networks.py:13: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.
  init.normal(m.weight.data, 0.0, 0.02)
E:\Study\Vision\project\Deep-MIML-Network\models\networks.py:17: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.
  init.normal(m.weight.data, 1.0, 0.02)
E:\Study\Vision\project\Deep-MIML-Network\models\networks.py:18: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.
  init.constant(m.bias.data, 0.0)
Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 114, in _main
    prepare(preparation_data)
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 225, in prepare
    _fixup_main_from_path(data['init_main_from_path'])
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 277, in _fixup_main_from_path
    run_name="__mp_main__")
  File "D:\Anaconda3\lib\runpy.py", line 263, in run_path
    pkg_name=pkg_name, script_name=fname)
  File "D:\Anaconda3\lib\runpy.py", line 96, in _run_module_code
    mod_name, mod_spec, pkg_name, script_name)
  File "D:\Anaconda3\lib\runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "E:\Study\Vision\project\Deep-MIML-Network\train.py", line 65, in <module>
    for i, data in enumerate(dataset):
  File "E:\Study\Vision\project\Deep-MIML-Network\data\custom_dataset_data_loader.py", line 43, in __iter__
    for i, data in enumerate(self.dataloader):
  File "D:\Anaconda3\lib\site-packages\torch\utils\data\dataloader.py", line 819, in __iter__
    return _DataLoaderIter(self)
  File "D:\Anaconda3\lib\site-packages\torch\utils\data\dataloader.py", line 560, in __init__
    w.start()
  File "D:\Anaconda3\lib\multiprocessing\process.py", line 105, in start
    self._popen = self._Popen(self)
  File "D:\Anaconda3\lib\multiprocessing\context.py", line 223, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "D:\Anaconda3\lib\multiprocessing\context.py", line 322, in _Popen
    return Popen(process_obj)
  File "D:\Anaconda3\lib\multiprocessing\popen_spawn_win32.py", line 33, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 143, in get_preparation_data
    _check_not_importing_main()
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 136, in _check_not_importing_main
    is not going to be frozen to produce an executable.''')
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.
------------ Options -------------
F: 2401
HDF5FileRoot: e:\Study\Vision\project\Deep-MIML-Network\dataset
K: 4
L: 15
batchSize: 256
checkpoints_dir: checkpoints
continue_train: True
dataset: musicInstruments
decay_factor: 0.94
display_freq: 10
epoch_count: 0
fc_dimension: 1024
gpu_ids: [0]
init_type: normal
isTrain: True
learning_rate: 0.001
learning_rate_decrease_itr: 5
measure_time: False
mode: train
model: MIML
multi_label_threshold: 0.3
nThreads: 1
name: deepMIML
niter: 300
norm: max
num_of_bases: 25
num_of_fc: 1
save_epoch_freq: 5
save_latest_freq: 500
selected_classes: True
using_multi_labels: True
validation_batches: 10
validation_freq: 50
validation_on: False
with_batchnorm: True
with_softmax: False
zeroCenterInput: False
-------------- End ----------------
CustomDatasetDataLoader
dataset [MIMLDataset] was created
MIML dataset initialize suceed
#training images = 15
MIML
initialization method [normal]
model [MIMLModel] was created
starting from epoch  0
D:\Anaconda3\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
E:\Study\Vision\project\Deep-MIML-Network\models\networks.py:13: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.
  init.normal(m.weight.data, 0.0, 0.02)
E:\Study\Vision\project\Deep-MIML-Network\models\networks.py:17: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.
  init.normal(m.weight.data, 1.0, 0.02)
E:\Study\Vision\project\Deep-MIML-Network\models\networks.py:18: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.
  init.constant(m.bias.data, 0.0)
Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 114, in _main
    prepare(preparation_data)
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 225, in prepare
    _fixup_main_from_path(data['init_main_from_path'])
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 277, in _fixup_main_from_path
    run_name="__mp_main__")
  File "D:\Anaconda3\lib\runpy.py", line 263, in run_path
    pkg_name=pkg_name, script_name=fname)
  File "D:\Anaconda3\lib\runpy.py", line 96, in _run_module_code
    mod_name, mod_spec, pkg_name, script_name)
  File "D:\Anaconda3\lib\runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "E:\Study\Vision\project\Deep-MIML-Network\train.py", line 65, in <module>
    for i, data in enumerate(dataset):
  File "E:\Study\Vision\project\Deep-MIML-Network\data\custom_dataset_data_loader.py", line 43, in __iter__
    for i, data in enumerate(self.dataloader):
  File "D:\Anaconda3\lib\site-packages\torch\utils\data\dataloader.py", line 819, in __iter__
    return _DataLoaderIter(self)
  File "D:\Anaconda3\lib\site-packages\torch\utils\data\dataloader.py", line 560, in __init__
    w.start()
  File "D:\Anaconda3\lib\multiprocessing\process.py", line 105, in start
    self._popen = self._Popen(self)
  File "D:\Anaconda3\lib\multiprocessing\context.py", line 223, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "D:\Anaconda3\lib\multiprocessing\context.py", line 322, in _Popen
    return Popen(process_obj)
  File "D:\Anaconda3\lib\multiprocessing\popen_spawn_win32.py", line 33, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 143, in get_preparation_data
    _check_not_importing_main()
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 136, in _check_not_importing_main
    is not going to be frozen to produce an executable.''')
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.
------------ Options -------------
F: 2401
HDF5FileRoot: e:\Study\Vision\project\Deep-MIML-Network\dataset
K: 4
L: 15
batchSize: 256
checkpoints_dir: checkpoints
continue_train: True
dataset: musicInstruments
decay_factor: 0.94
display_freq: 10
epoch_count: 0
fc_dimension: 1024
gpu_ids: [0]
init_type: normal
isTrain: True
learning_rate: 0.001
learning_rate_decrease_itr: 5
measure_time: False
mode: train
model: MIML
multi_label_threshold: 0.3
nThreads: 1
name: deepMIML
niter: 300
norm: max
num_of_bases: 25
num_of_fc: 1
save_epoch_freq: 5
save_latest_freq: 500
selected_classes: True
using_multi_labels: True
validation_batches: 10
validation_freq: 50
validation_on: False
with_batchnorm: True
with_softmax: False
zeroCenterInput: False
-------------- End ----------------
CustomDatasetDataLoader
dataset [MIMLDataset] was created
MIML dataset initialize suceed
(15,)
(15,)
#training images = 15
MIML
initialization method [normal]
model [MIMLModel] was created
starting from epoch  0
D:\Anaconda3\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
E:\Study\Vision\project\Deep-MIML-Network\models\networks.py:13: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.
  init.normal(m.weight.data, 0.0, 0.02)
E:\Study\Vision\project\Deep-MIML-Network\models\networks.py:17: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.
  init.normal(m.weight.data, 1.0, 0.02)
E:\Study\Vision\project\Deep-MIML-Network\models\networks.py:18: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.
  init.constant(m.bias.data, 0.0)
Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 114, in _main
    prepare(preparation_data)
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 225, in prepare
    _fixup_main_from_path(data['init_main_from_path'])
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 277, in _fixup_main_from_path
    run_name="__mp_main__")
  File "D:\Anaconda3\lib\runpy.py", line 263, in run_path
    pkg_name=pkg_name, script_name=fname)
  File "D:\Anaconda3\lib\runpy.py", line 96, in _run_module_code
    mod_name, mod_spec, pkg_name, script_name)
  File "D:\Anaconda3\lib\runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "E:\Study\Vision\project\Deep-MIML-Network\train.py", line 65, in <module>
    for i, data in enumerate(dataset):
  File "E:\Study\Vision\project\Deep-MIML-Network\data\custom_dataset_data_loader.py", line 43, in __iter__
    for i, data in enumerate(self.dataloader):
  File "D:\Anaconda3\lib\site-packages\torch\utils\data\dataloader.py", line 819, in __iter__
    return _DataLoaderIter(self)
  File "D:\Anaconda3\lib\site-packages\torch\utils\data\dataloader.py", line 560, in __init__
    w.start()
  File "D:\Anaconda3\lib\multiprocessing\process.py", line 105, in start
    self._popen = self._Popen(self)
  File "D:\Anaconda3\lib\multiprocessing\context.py", line 223, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "D:\Anaconda3\lib\multiprocessing\context.py", line 322, in _Popen
    return Popen(process_obj)
  File "D:\Anaconda3\lib\multiprocessing\popen_spawn_win32.py", line 33, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 143, in get_preparation_data
    _check_not_importing_main()
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 136, in _check_not_importing_main
    is not going to be frozen to produce an executable.''')
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.
------------ Options -------------
F: 2401
HDF5FileRoot: e:\Study\Vision\project\Deep-MIML-Network\dataset
K: 4
L: 15
batchSize: 256
checkpoints_dir: checkpoints
continue_train: True
dataset: musicInstruments
decay_factor: 0.94
display_freq: 10
epoch_count: 0
fc_dimension: 1024
gpu_ids: []
init_type: normal
isTrain: True
learning_rate: 0.001
learning_rate_decrease_itr: 5
measure_time: False
mode: train
model: MIML
multi_label_threshold: 0.3
nThreads: 1
name: deepMIML
niter: 300
norm: max
num_of_bases: 25
num_of_fc: 1
save_epoch_freq: 5
save_latest_freq: 500
selected_classes: True
using_multi_labels: True
validation_batches: 10
validation_freq: 50
validation_on: False
with_batchnorm: True
with_softmax: False
zeroCenterInput: False
-------------- End ----------------
CustomDatasetDataLoader
dataset [MIMLDataset] was created
MIML dataset initialize suceed
(15,)
(15,)
#training images = 15
MIML
initialization method [normal]
model [MIMLModel] was created
starting from epoch  0
D:\Anaconda3\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
E:\Study\Vision\project\Deep-MIML-Network\models\networks.py:13: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.
  init.normal(m.weight.data, 0.0, 0.02)
E:\Study\Vision\project\Deep-MIML-Network\models\networks.py:17: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.
  init.normal(m.weight.data, 1.0, 0.02)
E:\Study\Vision\project\Deep-MIML-Network\models\networks.py:18: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.
  init.constant(m.bias.data, 0.0)
Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 114, in _main
    prepare(preparation_data)
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 225, in prepare
    _fixup_main_from_path(data['init_main_from_path'])
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 277, in _fixup_main_from_path
    run_name="__mp_main__")
  File "D:\Anaconda3\lib\runpy.py", line 263, in run_path
    pkg_name=pkg_name, script_name=fname)
  File "D:\Anaconda3\lib\runpy.py", line 96, in _run_module_code
    mod_name, mod_spec, pkg_name, script_name)
  File "D:\Anaconda3\lib\runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "E:\Study\Vision\project\Deep-MIML-Network\train.py", line 65, in <module>
    for i, data in enumerate(dataset):
  File "E:\Study\Vision\project\Deep-MIML-Network\data\custom_dataset_data_loader.py", line 43, in __iter__
    for i, data in enumerate(self.dataloader):
  File "D:\Anaconda3\lib\site-packages\torch\utils\data\dataloader.py", line 819, in __iter__
    return _DataLoaderIter(self)
  File "D:\Anaconda3\lib\site-packages\torch\utils\data\dataloader.py", line 560, in __init__
    w.start()
  File "D:\Anaconda3\lib\multiprocessing\process.py", line 105, in start
    self._popen = self._Popen(self)
  File "D:\Anaconda3\lib\multiprocessing\context.py", line 223, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "D:\Anaconda3\lib\multiprocessing\context.py", line 322, in _Popen
    return Popen(process_obj)
  File "D:\Anaconda3\lib\multiprocessing\popen_spawn_win32.py", line 33, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 143, in get_preparation_data
    _check_not_importing_main()
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 136, in _check_not_importing_main
    is not going to be frozen to produce an executable.''')
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.
------------ Options -------------
F: 2401
HDF5FileRoot: e:\Study\Vision\project\Deep-MIML-Network\dataset
K: 4
L: 15
batchSize: 256
checkpoints_dir: checkpoints
continue_train: True
dataset: musicInstruments
decay_factor: 0.94
display_freq: 10
epoch_count: 0
fc_dimension: 1024
gpu_ids: []
init_type: normal
isTrain: True
learning_rate: 0.001
learning_rate_decrease_itr: 5
measure_time: False
mode: train
model: MIML
multi_label_threshold: 0.3
nThreads: 1
name: deepMIML
niter: 300
norm: max
num_of_bases: 25
num_of_fc: 1
save_epoch_freq: 5
save_latest_freq: 500
selected_classes: True
using_multi_labels: True
validation_batches: 10
validation_freq: 50
validation_on: True
with_batchnorm: True
with_softmax: False
zeroCenterInput: False
-------------- End ----------------
CustomDatasetDataLoader
dataset [MIMLDataset] was created
MIML dataset initialize suceed
(15,)
(15,)
#training images = 15
CustomDatasetDataLoader
dataset [MIMLDataset] was created
MIML dataset initialize suceed
(7,)
(7,)
#validation images = 7
MIML
initialization method [normal]
model [MIMLModel] was created
starting from epoch  0
D:\Anaconda3\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
E:\Study\Vision\project\Deep-MIML-Network\models\networks.py:13: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.
  init.normal(m.weight.data, 0.0, 0.02)
E:\Study\Vision\project\Deep-MIML-Network\models\networks.py:17: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.
  init.normal(m.weight.data, 1.0, 0.02)
E:\Study\Vision\project\Deep-MIML-Network\models\networks.py:18: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.
  init.constant(m.bias.data, 0.0)
Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 114, in _main
    prepare(preparation_data)
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 225, in prepare
    _fixup_main_from_path(data['init_main_from_path'])
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 277, in _fixup_main_from_path
    run_name="__mp_main__")
  File "D:\Anaconda3\lib\runpy.py", line 263, in run_path
    pkg_name=pkg_name, script_name=fname)
  File "D:\Anaconda3\lib\runpy.py", line 96, in _run_module_code
    mod_name, mod_spec, pkg_name, script_name)
  File "D:\Anaconda3\lib\runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "E:\Study\Vision\project\Deep-MIML-Network\train.py", line 65, in <module>
    for i, data in enumerate(dataset):
  File "E:\Study\Vision\project\Deep-MIML-Network\data\custom_dataset_data_loader.py", line 43, in __iter__
    for i, data in enumerate(self.dataloader):
  File "D:\Anaconda3\lib\site-packages\torch\utils\data\dataloader.py", line 819, in __iter__
    return _DataLoaderIter(self)
  File "D:\Anaconda3\lib\site-packages\torch\utils\data\dataloader.py", line 560, in __init__
    w.start()
  File "D:\Anaconda3\lib\multiprocessing\process.py", line 105, in start
    self._popen = self._Popen(self)
  File "D:\Anaconda3\lib\multiprocessing\context.py", line 223, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "D:\Anaconda3\lib\multiprocessing\context.py", line 322, in _Popen
    return Popen(process_obj)
  File "D:\Anaconda3\lib\multiprocessing\popen_spawn_win32.py", line 33, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 143, in get_preparation_data
    _check_not_importing_main()
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 136, in _check_not_importing_main
    is not going to be frozen to produce an executable.''')
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.
------------ Options -------------
F: 2401
HDF5FileRoot: e:\Study\Vision\project\Deep-MIML-Network\dataset
K: 4
L: 15
batchSize: 256
checkpoints_dir: checkpoints
continue_train: True
dataset: musicInstruments
decay_factor: 0.94
display_freq: 10
epoch_count: 0
fc_dimension: 1024
gpu_ids: []
init_type: normal
isTrain: True
learning_rate: 0.001
learning_rate_decrease_itr: 5
measure_time: False
mode: train
model: MIML
multi_label_threshold: 0.3
nThreads: 1
name: deepMIML
niter: 300
norm: max
num_of_bases: 25
num_of_fc: 1
save_epoch_freq: 5
save_latest_freq: 500
selected_classes: True
using_multi_labels: True
validation_batches: 10
validation_freq: 50
validation_on: True
with_batchnorm: True
with_softmax: False
zeroCenterInput: False
-------------- End ----------------
CustomDatasetDataLoader
dataset [MIMLDataset] was created
MIML dataset initialize suceed
(15,)
(15,)
#training images = 15
CustomDatasetDataLoader
dataset [MIMLDataset] was created
MIML dataset initialize suceed
(7,)
(7,)
#validation images = 7
MIML
initialization method [normal]
model [MIMLModel] was created
starting from epoch  0
D:\Anaconda3\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
E:\Study\Vision\project\Deep-MIML-Network\models\networks.py:13: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.
  init.normal(m.weight.data, 0.0, 0.02)
E:\Study\Vision\project\Deep-MIML-Network\models\networks.py:17: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.
  init.normal(m.weight.data, 1.0, 0.02)
E:\Study\Vision\project\Deep-MIML-Network\models\networks.py:18: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.
  init.constant(m.bias.data, 0.0)
Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 114, in _main
    prepare(preparation_data)
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 225, in prepare
    _fixup_main_from_path(data['init_main_from_path'])
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 277, in _fixup_main_from_path
    run_name="__mp_main__")
  File "D:\Anaconda3\lib\runpy.py", line 263, in run_path
    pkg_name=pkg_name, script_name=fname)
  File "D:\Anaconda3\lib\runpy.py", line 96, in _run_module_code
    mod_name, mod_spec, pkg_name, script_name)
  File "D:\Anaconda3\lib\runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "E:\Study\Vision\project\Deep-MIML-Network\train.py", line 65, in <module>
    for i, data in enumerate(dataset):
  File "E:\Study\Vision\project\Deep-MIML-Network\data\custom_dataset_data_loader.py", line 43, in __iter__
    for i, data in enumerate(self.dataloader):
  File "D:\Anaconda3\lib\site-packages\torch\utils\data\dataloader.py", line 819, in __iter__
    return _DataLoaderIter(self)
  File "D:\Anaconda3\lib\site-packages\torch\utils\data\dataloader.py", line 560, in __init__
    w.start()
  File "D:\Anaconda3\lib\multiprocessing\process.py", line 105, in start
    self._popen = self._Popen(self)
  File "D:\Anaconda3\lib\multiprocessing\context.py", line 223, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "D:\Anaconda3\lib\multiprocessing\context.py", line 322, in _Popen
    return Popen(process_obj)
  File "D:\Anaconda3\lib\multiprocessing\popen_spawn_win32.py", line 33, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 143, in get_preparation_data
    _check_not_importing_main()
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 136, in _check_not_importing_main
    is not going to be frozen to produce an executable.''')
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.
------------ Options -------------
F: 2401
HDF5FileRoot: e:\Study\Vision\project\Deep-MIML-Network\dataset
K: 4
L: 15
batchSize: 256
checkpoints_dir: checkpoints
continue_train: True
dataset: musicInstruments
decay_factor: 0.94
display_freq: 10
epoch_count: 0
fc_dimension: 1024
gpu_ids: []
init_type: normal
isTrain: True
learning_rate: 0.001
learning_rate_decrease_itr: 5
measure_time: False
mode: train
model: MIML
multi_label_threshold: 0.3
nThreads: 1
name: deepMIML
niter: 300
norm: max
num_of_bases: 25
num_of_fc: 1
save_epoch_freq: 5
save_latest_freq: 500
selected_classes: True
using_multi_labels: True
validation_batches: 10
validation_freq: 50
validation_on: True
with_batchnorm: True
with_softmax: False
zeroCenterInput: False
-------------- End ----------------
CustomDatasetDataLoader
dataset [MIMLDataset] was created
MIML dataset initialize suceed
(15,)
(15,)
#training images = 15
CustomDatasetDataLoader
dataset [MIMLDataset] was created
MIML dataset initialize suceed
(7,)
(7,)
#validation images = 7
MIML
initialization method [normal]
model [MIMLModel] was created
starting from epoch  0
D:\Anaconda3\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
E:\Study\Vision\project\Deep-MIML-Network\models\networks.py:13: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.
  init.normal(m.weight.data, 0.0, 0.02)
E:\Study\Vision\project\Deep-MIML-Network\models\networks.py:17: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.
  init.normal(m.weight.data, 1.0, 0.02)
E:\Study\Vision\project\Deep-MIML-Network\models\networks.py:18: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.
  init.constant(m.bias.data, 0.0)
Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 114, in _main
    prepare(preparation_data)
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 225, in prepare
    _fixup_main_from_path(data['init_main_from_path'])
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 277, in _fixup_main_from_path
    run_name="__mp_main__")
  File "D:\Anaconda3\lib\runpy.py", line 263, in run_path
    pkg_name=pkg_name, script_name=fname)
  File "D:\Anaconda3\lib\runpy.py", line 96, in _run_module_code
    mod_name, mod_spec, pkg_name, script_name)
  File "D:\Anaconda3\lib\runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "E:\Study\Vision\project\Deep-MIML-Network\train.py", line 65, in <module>
    for i, data in enumerate(dataset):
  File "E:\Study\Vision\project\Deep-MIML-Network\data\custom_dataset_data_loader.py", line 43, in __iter__
    for i, data in enumerate(self.dataloader):
  File "D:\Anaconda3\lib\site-packages\torch\utils\data\dataloader.py", line 819, in __iter__
    return _DataLoaderIter(self)
  File "D:\Anaconda3\lib\site-packages\torch\utils\data\dataloader.py", line 560, in __init__
    w.start()
  File "D:\Anaconda3\lib\multiprocessing\process.py", line 105, in start
    self._popen = self._Popen(self)
  File "D:\Anaconda3\lib\multiprocessing\context.py", line 223, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "D:\Anaconda3\lib\multiprocessing\context.py", line 322, in _Popen
    return Popen(process_obj)
  File "D:\Anaconda3\lib\multiprocessing\popen_spawn_win32.py", line 33, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 143, in get_preparation_data
    _check_not_importing_main()
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 136, in _check_not_importing_main
    is not going to be frozen to produce an executable.''')
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.
------------ Options -------------
F: 2401
HDF5FileRoot: e:\Study\Vision\project\Deep-MIML-Network\dataset
K: 4
L: 15
batchSize: 256
checkpoints_dir: checkpoints
continue_train: True
dataset: musicInstruments
decay_factor: 0.94
display_freq: 10
epoch_count: 0
fc_dimension: 1024
gpu_ids: []
init_type: normal
isTrain: True
learning_rate: 0.001
learning_rate_decrease_itr: 5
measure_time: False
mode: train
model: MIML
multi_label_threshold: 0.3
nThreads: 1
name: deepMIML
niter: 300
norm: max
num_of_bases: 25
num_of_fc: 1
save_epoch_freq: 5
save_latest_freq: 500
selected_classes: True
using_multi_labels: True
validation_batches: 10
validation_freq: 50
validation_on: True
with_batchnorm: True
with_softmax: False
zeroCenterInput: False
-------------- End ----------------
CustomDatasetDataLoader
dataset [MIMLDataset] was created
MIML dataset initialize suceed
(15,)
(15,)
#training images = 15
CustomDatasetDataLoader
dataset [MIMLDataset] was created
MIML dataset initialize suceed
(7,)
(7,)
#validation images = 7
MIML
initialization method [normal]
model [MIMLModel] was created
starting from epoch  0
D:\Anaconda3\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
------------ Options -------------
F: 2401
HDF5FileRoot: e:\Study\Vision\project\Deep-MIML-Network\dataset
K: 4
L: 15
batchSize: 256
checkpoints_dir: checkpoints
continue_train: True
dataset: musicInstruments
decay_factor: 0.94
display_freq: 10
epoch_count: 0
fc_dimension: 1024
gpu_ids: []
init_type: normal
isTrain: True
learning_rate: 0.001
learning_rate_decrease_itr: 5
measure_time: False
mode: train
model: MIML
multi_label_threshold: 0.3
nThreads: 0
name: deepMIML
niter: 300
norm: max
num_of_bases: 25
num_of_fc: 1
save_epoch_freq: 5
save_latest_freq: 500
selected_classes: True
using_multi_labels: True
validation_batches: 10
validation_freq: 50
validation_on: True
with_batchnorm: True
with_softmax: False
zeroCenterInput: False
-------------- End ----------------
CustomDatasetDataLoader
dataset [MIMLDataset] was created
MIML dataset initialize suceed
(15,)
(15,)
#training images = 15
CustomDatasetDataLoader
dataset [MIMLDataset] was created
MIML dataset initialize suceed
(7,)
(7,)
#validation images = 7
MIML
initialization method [normal]
model [MIMLModel] was created
starting from epoch  0
Traceback (most recent call last):
  File "train.py", line 65, in <module>
    for i, data in enumerate(dataset):
  File "E:\Study\Vision\project\Deep-MIML-Network\data\custom_dataset_data_loader.py", line 43, in __iter__
    for i, data in enumerate(self.dataloader):
  File "D:\Anaconda3\lib\site-packages\torch\utils\data\dataloader.py", line 615, in __next__
    batch = self.collate_fn([self.dataset[i] for i in indices])
  File "D:\Anaconda3\lib\site-packages\torch\utils\data\dataloader.py", line 615, in <listcomp>
    batch = self.collate_fn([self.dataset[i] for i in indices])
  File "E:\Study\Vision\project\Deep-MIML-Network\data\MIML_dataset.py", line 100, in __getitem__
    loaded_label = softmax(subsetOfClasses(np.load(self.labels[index].decode("utf-8"))))
AttributeError: 'numpy.int32' object has no attribute 'decode'
D:\Anaconda3\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
------------ Options -------------
F: 2401
HDF5FileRoot: e:\Study\Vision\project\Deep-MIML-Network\dataset
K: 4
L: 15
batchSize: 256
checkpoints_dir: checkpoints
continue_train: True
dataset: musicInstruments
decay_factor: 0.94
display_freq: 10
epoch_count: 0
fc_dimension: 1024
gpu_ids: []
init_type: normal
isTrain: True
learning_rate: 0.001
learning_rate_decrease_itr: 5
measure_time: False
mode: train
model: MIML
multi_label_threshold: 0.3
nThreads: 0
name: deepMIML
niter: 300
norm: max
num_of_bases: 25
num_of_fc: 1
save_epoch_freq: 5
save_latest_freq: 500
selected_classes: True
using_multi_labels: True
validation_batches: 10
validation_freq: 50
validation_on: True
with_batchnorm: True
with_softmax: False
zeroCenterInput: False
-------------- End ----------------
CustomDatasetDataLoader
dataset [MIMLDataset] was created
MIML dataset initialize suceed
(15,)
(15,)
#training images = 15
CustomDatasetDataLoader
dataset [MIMLDataset] was created
MIML dataset initialize suceed
(7,)
(7,)
#validation images = 7
MIML
initialization method [normal]
model [MIMLModel] was created
starting from epoch  0
Traceback (most recent call last):
  File "train.py", line 65, in <module>
    for i, data in enumerate(dataset):
  File "E:\Study\Vision\project\Deep-MIML-Network\data\custom_dataset_data_loader.py", line 43, in __iter__
    for i, data in enumerate(self.dataloader):
  File "D:\Anaconda3\lib\site-packages\torch\utils\data\dataloader.py", line 615, in __next__
    batch = self.collate_fn([self.dataset[i] for i in indices])
  File "D:\Anaconda3\lib\site-packages\torch\utils\data\dataloader.py", line 615, in <listcomp>
    batch = self.collate_fn([self.dataset[i] for i in indices])
  File "E:\Study\Vision\project\Deep-MIML-Network\data\MIML_dataset.py", line 100, in __getitem__
    loaded_label = softmax(subsetOfClasses(np.load(self.labels[index].decode("utf-8"))))
  File "D:\Anaconda3\lib\site-packages\numpy\lib\npyio.py", line 372, in load
    fid = open(file, "rb")
FileNotFoundError: [Errno 2] No such file or directory: '401'
D:\Anaconda3\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
------------ Options -------------
F: 2401
HDF5FileRoot: e:\Study\Vision\project\Deep-MIML-Network\dataset
K: 4
L: 15
batchSize: 256
checkpoints_dir: checkpoints
continue_train: True
dataset: musicInstruments
decay_factor: 0.94
display_freq: 10
epoch_count: 0
fc_dimension: 1024
gpu_ids: []
init_type: normal
isTrain: True
learning_rate: 0.001
learning_rate_decrease_itr: 5
measure_time: False
mode: train
model: MIML
multi_label_threshold: 0.3
nThreads: 0
name: deepMIML
niter: 300
norm: max
num_of_bases: 25
num_of_fc: 1
save_epoch_freq: 5
save_latest_freq: 500
selected_classes: True
using_multi_labels: True
validation_batches: 10
validation_freq: 50
validation_on: True
with_batchnorm: True
with_softmax: False
zeroCenterInput: False
-------------- End ----------------
CustomDatasetDataLoader
dataset [MIMLDataset] was created
MIML dataset initialize suceed
(15,)
(15,)
#training images = 15
CustomDatasetDataLoader
dataset [MIMLDataset] was created
MIML dataset initialize suceed
(7,)
(7,)
#validation images = 7
MIML
initialization method [normal]
model [MIMLModel] was created
starting from epoch  0
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
Traceback (most recent call last):
  File "train.py", line 76, in <module>
    model.optimize_parameters()
  File "E:\Study\Vision\project\Deep-MIML-Network\models\MIML.py", line 213, in optimize_parameters
    self.backward()
  File "E:\Study\Vision\project\Deep-MIML-Network\models\MIML.py", line 139, in backward
    self.batch_loss.append(loss.data[0]) 
IndexError: invalid index of a 0-dim tensor. Use tensor.item() to convert a 0-dim tensor to a Python number
------------ Options -------------
F: 2401
HDF5FileRoot: e:\Study\Vision\project\Deep-MIML-Network\dataset
K: 4
L: 15
batchSize: 256
checkpoints_dir: checkpoints
continue_train: True
dataset: musicInstruments
decay_factor: 0.94
display_freq: 10
epoch_count: 0
fc_dimension: 1024
gpu_ids: []
init_type: normal
isTrain: True
learning_rate: 0.001
learning_rate_decrease_itr: 5
measure_time: False
mode: train
model: MIML
multi_label_threshold: 0.3
nThreads: 0
name: deepMIML
niter: 300
norm: max
num_of_bases: 25
num_of_fc: 1
save_epoch_freq: 5
save_latest_freq: 500
selected_classes: True
using_multi_labels: True
validation_batches: 10
validation_freq: 50
validation_on: True
with_batchnorm: True
with_softmax: False
zeroCenterInput: False
-------------- End ----------------
CustomDatasetDataLoader
dataset [MIMLDataset] was created
MIML dataset initialize suceed
(15,)
(15,)
#training images = 15
CustomDatasetDataLoader
dataset [MIMLDataset] was created
MIML dataset initialize suceed
(7,)
(7,)
#validation images = 7
MIML
initialization method [normal]
model [MIMLModel] was created
starting from epoch  0
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
saving the model at the end of epoch 5, total_steps 1280
current learning rate: 0.00094
decreased learning rate by  0.94
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
Display training progress at (epoch 10, total_steps 2560)
loss: 0.9058656394481659
mAP: 0.1413333333333333
end of display 

saving the model at the end of epoch 10, total_steps 2560
current learning rate: 0.0008836
decreased learning rate by  0.94
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
saving the model at the end of epoch 15, total_steps 3840
current learning rate: 0.0008305839999999999
decreased learning rate by  0.94
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
Display training progress at (epoch 20, total_steps 5120)
loss: 0.8226962924003601
mAP: 0.18488888888888896
end of display 

saving the model at the end of epoch 20, total_steps 5120
current learning rate: 0.0007807489599999998
decreased learning rate by  0.94
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
saving the model at the end of epoch 25, total_steps 6400
current learning rate: 0.0007339040223999998
decreased learning rate by  0.94
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
Display training progress at (epoch 30, total_steps 7680)
loss: 0.8150656282901764
mAP: 0.1911111111111112
end of display 

saving the model at the end of epoch 30, total_steps 7680
current learning rate: 0.0006898697810559998
decreased learning rate by  0.94
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
saving the model at the end of epoch 35, total_steps 8960
current learning rate: 0.0006484775941926397
decreased learning rate by  0.94
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
Display training progress at (epoch 40, total_steps 10240)
loss: 0.81213858127594
mAP: 0.1911111111111111
end of display 

saving the model at the end of epoch 40, total_steps 10240
current learning rate: 0.0006095689385410813
decreased learning rate by  0.94
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
saving the model at the end of epoch 45, total_steps 11520
current learning rate: 0.0005729948022286164
decreased learning rate by  0.94
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
Display training progress at (epoch 50, total_steps 12800)
loss: 0.8106990873813629
mAP: 0.19111111111111115
end of display 

Display validation results at (epoch 50, total_steps 12800)
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]D:\Anaconda3\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
E:\Study\Vision\project\Deep-MIML-Network\models\MIML.py:68: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  basesnet_output = self.BasesNet(Variable(self.bases, requires_grad=False, volatile=volatile)).view(-1, self.opt.L, self.opt.K, self.opt.num_of_bases)

[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
Traceback (most recent call last):
  File "train.py", line 103, in <module>
    model.display_val(writer, total_steps, dataset_val)
  File "E:\Study\Vision\project\Deep-MIML-Network\models\MIML.py", line 166, in display_val
    ap, loss = self.test_multi_label(val_data)
  File "E:\Study\Vision\project\Deep-MIML-Network\models\MIML.py", line 208, in test_multi_label
    loss = self.loss(self.output, label).data.cpu().numpy()[0]
IndexError: too many indices for array
D:\Anaconda3\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
------------ Options -------------
F: 2401
HDF5FileRoot: e:\Study\Vision\project\Deep-MIML-Network\dataset
K: 4
L: 15
batchSize: 256
checkpoints_dir: checkpoints
continue_train: True
dataset: musicInstruments
decay_factor: 0.94
display_freq: 10
epoch_count: 0
fc_dimension: 1024
gpu_ids: []
init_type: normal
isTrain: True
learning_rate: 0.001
learning_rate_decrease_itr: 5
measure_time: False
mode: train
model: MIML
multi_label_threshold: 0.3
nThreads: 0
name: deepMIML
niter: 300
norm: max
num_of_bases: 25
num_of_fc: 1
save_epoch_freq: 5
save_latest_freq: 500
selected_classes: True
using_multi_labels: True
validation_batches: 10
validation_freq: 50
validation_on: True
with_batchnorm: True
with_softmax: False
zeroCenterInput: False
-------------- End ----------------
CustomDatasetDataLoader
dataset [MIMLDataset] was created
MIML dataset initialize suceed
(15,)
(15,)
#training images = 15
CustomDatasetDataLoader
dataset [MIMLDataset] was created
MIML dataset initialize suceed
(7,)
(7,)
#validation images = 7
MIML
initialization method [normal]
model [MIMLModel] was created
starting from epoch  45
current learning rate: 0.0005729948022286164
Display training progress at (epoch 50, total_steps 12800)
loss: 0.8106991112232208
mAP: 0.19111111111111118
end of display 

Display validation results at (epoch 50, total_steps 12800)
tensor([[3.9202, 4.9415, 3.9451, 1.9999, 3.8855, 3.9120, 3.9101, 3.9369, 4.0258,
         3.9421, 3.9322, 3.9430, 3.9518, 3.9231, 3.9765],
        [3.9202, 4.9415, 3.9451, 1.9999, 3.8855, 3.9120, 3.9101, 3.9369, 4.0258,
         3.9421, 3.9322, 3.9430, 3.9518, 3.9231, 3.9765],
        [3.9202, 4.9415, 3.9451, 1.9999, 3.8855, 3.9120, 3.9101, 3.9369, 4.0258,
         3.9421, 3.9322, 3.9430, 3.9518, 3.9231, 3.9765],
        [3.9202, 4.9415, 3.9451, 1.9999, 3.8855, 3.9120, 3.9101, 3.9369, 4.0258,
         3.9421, 3.9322, 3.9430, 3.9518, 3.9231, 3.9765],
        [3.9202, 4.9415, 3.9451, 1.9999, 3.8855, 3.9120, 3.9101, 3.9369, 4.0258,
         3.9421, 3.9322, 3.9430, 3.9518, 3.9231, 3.9765],
        [3.9202, 4.9415, 3.9451, 1.9999, 3.8855, 3.9120, 3.9101, 3.9369, 4.0258,
         3.9421, 3.9322, 3.9430, 3.9518, 3.9231, 3.9765],
        [3.9202, 4.9415, 3.9451, 1.9999, 3.8855, 3.9120, 3.9101, 3.9369, 4.0258,
         3.9421, 3.9322, 3.9430, 3.9518, 3.9231, 3.9765]],
       grad_fn=<ViewBackward>)
torch.Size([7, 15])
tensor([[ 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],
        [ 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],
        [ 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],
        [ 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],
        [ 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],
        [ 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],
        [ 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]])
torch.Size([7, 15])
Traceback (most recent call last):
  File "train.py", line 103, in <module>
    model.display_val(writer, total_steps, dataset_val)
  File "E:\Study\Vision\project\Deep-MIML-Network\models\MIML.py", line 167, in display_val
    ap, loss = self.test_multi_label(val_data)
  File "E:\Study\Vision\project\Deep-MIML-Network\models\MIML.py", line 213, in test_multi_label
    loss = self.loss(self.output, label).data.cpu().numpy()[0]
IndexError: too many indices for array
D:\Anaconda3\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
------------ Options -------------
F: 2401
HDF5FileRoot: e:\Study\Vision\project\Deep-MIML-Network\dataset
K: 4
L: 15
batchSize: 256
checkpoints_dir: checkpoints
continue_train: True
dataset: musicInstruments
decay_factor: 0.94
display_freq: 10
epoch_count: 0
fc_dimension: 1024
gpu_ids: []
init_type: normal
isTrain: True
learning_rate: 0.001
learning_rate_decrease_itr: 5
measure_time: False
mode: train
model: MIML
multi_label_threshold: 0.3
nThreads: 0
name: deepMIML
niter: 300
norm: max
num_of_bases: 25
num_of_fc: 1
save_epoch_freq: 5
save_latest_freq: 500
selected_classes: True
using_multi_labels: True
validation_batches: 10
validation_freq: 50
validation_on: True
with_batchnorm: True
with_softmax: False
zeroCenterInput: False
-------------- End ----------------
CustomDatasetDataLoader
dataset [MIMLDataset] was created
MIML dataset initialize suceed
(15,)
(15,)
#training images = 15
CustomDatasetDataLoader
dataset [MIMLDataset] was created
MIML dataset initialize suceed
(7,)
(7,)
#validation images = 7
MIML
initialization method [normal]
model [MIMLModel] was created
starting from epoch  45
current learning rate: 0.0005729948022286164
Display training progress at (epoch 50, total_steps 12800)
loss: 0.810699075460434
mAP: 0.19111111111111118
end of display 

Display validation results at (epoch 50, total_steps 12800)
tensor([[3.9202, 4.9415, 3.9451, 1.9999, 3.8855, 3.9120, 3.9101, 3.9369, 4.0258,
         3.9421, 3.9322, 3.9430, 3.9518, 3.9231, 3.9765],
        [3.9202, 4.9415, 3.9451, 1.9999, 3.8855, 3.9120, 3.9101, 3.9369, 4.0258,
         3.9421, 3.9322, 3.9430, 3.9518, 3.9231, 3.9765],
        [3.9202, 4.9415, 3.9451, 1.9999, 3.8855, 3.9120, 3.9101, 3.9369, 4.0258,
         3.9421, 3.9322, 3.9430, 3.9518, 3.9231, 3.9765],
        [3.9202, 4.9415, 3.9451, 1.9999, 3.8855, 3.9120, 3.9101, 3.9369, 4.0258,
         3.9421, 3.9322, 3.9430, 3.9518, 3.9231, 3.9765],
        [3.9202, 4.9415, 3.9451, 1.9999, 3.8855, 3.9120, 3.9101, 3.9369, 4.0258,
         3.9421, 3.9322, 3.9430, 3.9518, 3.9231, 3.9765],
        [3.9202, 4.9415, 3.9451, 1.9999, 3.8855, 3.9120, 3.9101, 3.9369, 4.0258,
         3.9421, 3.9322, 3.9430, 3.9518, 3.9231, 3.9765],
        [3.9202, 4.9415, 3.9451, 1.9999, 3.8855, 3.9120, 3.9101, 3.9369, 4.0258,
         3.9421, 3.9322, 3.9430, 3.9518, 3.9231, 3.9765]],
       grad_fn=<ViewBackward>)
torch.Size([7, 15])
tensor([[ 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],
        [ 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],
        [ 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],
        [ 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],
        [ 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],
        [ 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],
        [ 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]])
torch.Size([7, 15])
tensor(0.6855, grad_fn=<MultilabelMarginLossBackward>)
torch.Size([])
tensor(0.6855)
torch.Size([])
tensor(0.6855)
torch.Size([])
0.68552285
()
Traceback (most recent call last):
  File "train.py", line 103, in <module>
    model.display_val(writer, total_steps, dataset_val)
  File "E:\Study\Vision\project\Deep-MIML-Network\models\MIML.py", line 167, in display_val
    ap, loss = self.test_multi_label(val_data)
  File "E:\Study\Vision\project\Deep-MIML-Network\models\MIML.py", line 221, in test_multi_label
    loss = self.loss(self.output, label).data.cpu().numpy()[0]
IndexError: too many indices for array
D:\Anaconda3\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
------------ Options -------------
F: 2401
HDF5FileRoot: e:\Study\Vision\project\Deep-MIML-Network\dataset
K: 4
L: 15
batchSize: 256
checkpoints_dir: checkpoints
continue_train: True
dataset: musicInstruments
decay_factor: 0.94
display_freq: 10
epoch_count: 0
fc_dimension: 1024
gpu_ids: []
init_type: normal
isTrain: True
learning_rate: 0.001
learning_rate_decrease_itr: 5
measure_time: False
mode: train
model: MIML
multi_label_threshold: 0.3
nThreads: 0
name: deepMIML
niter: 300
norm: max
num_of_bases: 25
num_of_fc: 1
save_epoch_freq: 5
save_latest_freq: 500
selected_classes: True
using_multi_labels: True
validation_batches: 10
validation_freq: 50
validation_on: True
with_batchnorm: True
with_softmax: False
zeroCenterInput: False
-------------- End ----------------
CustomDatasetDataLoader
dataset [MIMLDataset] was created
MIML dataset initialize suceed
(15,)
(15,)
#training images = 15
CustomDatasetDataLoader
dataset [MIMLDataset] was created
MIML dataset initialize suceed
(7,)
(7,)
#validation images = 7
MIML
initialization method [normal]
model [MIMLModel] was created
starting from epoch  45
current learning rate: 0.0005729948022286164
Display training progress at (epoch 50, total_steps 12800)
loss: 0.810699075460434
mAP: 0.19111111111111118
end of display 

Display validation results at (epoch 50, total_steps 12800)
tensor([[3.9201, 4.9415, 3.9451, 1.9999, 3.8855, 3.9120, 3.9101, 3.9369, 4.0258,
         3.9421, 3.9322, 3.9430, 3.9518, 3.9231, 3.9765],
        [3.9201, 4.9415, 3.9451, 1.9999, 3.8855, 3.9120, 3.9101, 3.9369, 4.0258,
         3.9421, 3.9322, 3.9430, 3.9518, 3.9231, 3.9765],
        [3.9201, 4.9415, 3.9451, 1.9999, 3.8855, 3.9120, 3.9101, 3.9369, 4.0258,
         3.9421, 3.9322, 3.9430, 3.9518, 3.9231, 3.9765],
        [3.9201, 4.9415, 3.9451, 1.9999, 3.8855, 3.9120, 3.9101, 3.9369, 4.0258,
         3.9421, 3.9322, 3.9430, 3.9518, 3.9231, 3.9765],
        [3.9201, 4.9415, 3.9451, 1.9999, 3.8855, 3.9120, 3.9101, 3.9369, 4.0258,
         3.9421, 3.9322, 3.9430, 3.9518, 3.9231, 3.9765],
        [3.9201, 4.9415, 3.9451, 1.9999, 3.8855, 3.9120, 3.9101, 3.9369, 4.0258,
         3.9421, 3.9322, 3.9430, 3.9518, 3.9231, 3.9765],
        [3.9201, 4.9415, 3.9451, 1.9999, 3.8855, 3.9120, 3.9101, 3.9369, 4.0258,
         3.9421, 3.9322, 3.9430, 3.9518, 3.9231, 3.9765]],
       grad_fn=<ViewBackward>)
torch.Size([7, 15])
tensor([[ 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],
        [ 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],
        [ 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],
        [ 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],
        [ 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],
        [ 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],
        [ 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]])
torch.Size([7, 15])
tensor(0.6855, grad_fn=<MultilabelMarginLossBackward>)
torch.Size([])
tensor(0.6855)
torch.Size([])
tensor(0.6855)
torch.Size([])
numpy
0.6855248
()
Traceback (most recent call last):
  File "train.py", line 103, in <module>
    model.display_val(writer, total_steps, dataset_val)
  File "E:\Study\Vision\project\Deep-MIML-Network\models\MIML.py", line 167, in display_val
    ap, loss = self.test_multi_label(val_data)
  File "E:\Study\Vision\project\Deep-MIML-Network\models\MIML.py", line 222, in test_multi_label
    loss = self.loss(self.output, label).data.cpu().numpy()[0]
IndexError: too many indices for array
D:\Anaconda3\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
------------ Options -------------
F: 2401
HDF5FileRoot: e:\Study\Vision\project\Deep-MIML-Network\dataset
K: 4
L: 15
batchSize: 256
checkpoints_dir: checkpoints
continue_train: True
dataset: musicInstruments
decay_factor: 0.94
display_freq: 10
epoch_count: 0
fc_dimension: 1024
gpu_ids: []
init_type: normal
isTrain: True
learning_rate: 0.001
learning_rate_decrease_itr: 5
measure_time: False
mode: train
model: MIML
multi_label_threshold: 0.3
nThreads: 0
name: deepMIML
niter: 300
norm: max
num_of_bases: 25
num_of_fc: 1
save_epoch_freq: 5
save_latest_freq: 500
selected_classes: True
using_multi_labels: True
validation_batches: 10
validation_freq: 50
validation_on: True
with_batchnorm: True
with_softmax: False
zeroCenterInput: False
-------------- End ----------------
CustomDatasetDataLoader
dataset [MIMLDataset] was created
MIML dataset initialize suceed
(15,)
(15,)
#training images = 15
CustomDatasetDataLoader
dataset [MIMLDataset] was created
MIML dataset initialize suceed
(7,)
(7,)
#validation images = 7
MIML
initialization method [normal]
model [MIMLModel] was created
starting from epoch  60
Traceback (most recent call last):
  File "train.py", line 53, in <module>
    model = torch.load(os.path.join('.', opt.checkpoints_dir, opt.name, str(opt.epoch_count) + '.pth'))
  File "D:\Anaconda3\lib\site-packages\torch\serialization.py", line 367, in load
    return _load(f, map_location, pickle_module)
  File "D:\Anaconda3\lib\site-packages\torch\serialization.py", line 545, in _load
    deserialized_objects[key]._set_from_file(f, offset, f_should_read_directly)
RuntimeError: unexpected EOF, expected 8 more bytes. The file might be corrupted.
D:\Anaconda3\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
------------ Options -------------
F: 2401
HDF5FileRoot: e:\Study\Vision\project\Deep-MIML-Network\dataset
K: 4
L: 15
batchSize: 256
checkpoints_dir: checkpoints
continue_train: True
dataset: musicInstruments
decay_factor: 0.94
display_freq: 10
epoch_count: 0
fc_dimension: 1024
gpu_ids: []
init_type: normal
isTrain: True
learning_rate: 0.001
learning_rate_decrease_itr: 5
measure_time: False
mode: train
model: MIML
multi_label_threshold: 0.3
nThreads: 0
name: deepMIML
niter: 300
norm: max
num_of_bases: 25
num_of_fc: 1
save_epoch_freq: 5
save_latest_freq: 500
selected_classes: True
using_multi_labels: True
validation_batches: 10
validation_freq: 50
validation_on: True
with_batchnorm: True
with_softmax: False
zeroCenterInput: False
-------------- End ----------------
CustomDatasetDataLoader
dataset [MIMLDataset] was created
MIML dataset initialize suceed
(15,)
(15,)
#training images = 15
CustomDatasetDataLoader
dataset [MIMLDataset] was created
MIML dataset initialize suceed
(7,)
(7,)
#validation images = 7
MIML
initialization method [normal]
model [MIMLModel] was created
starting from epoch  60
Traceback (most recent call last):
  File "train.py", line 53, in <module>
    model = torch.load(os.path.join('.', opt.checkpoints_dir, opt.name, str(opt.epoch_count) + '.pth'))
  File "D:\Anaconda3\lib\site-packages\torch\serialization.py", line 367, in load
    return _load(f, map_location, pickle_module)
  File "D:\Anaconda3\lib\site-packages\torch\serialization.py", line 545, in _load
    deserialized_objects[key]._set_from_file(f, offset, f_should_read_directly)
RuntimeError: unexpected EOF, expected 8 more bytes. The file might be corrupted.
------------ Options -------------
F: 2401
HDF5FileRoot: e:\Study\Vision\project\Deep-MIML-Network\dataset
K: 4
L: 15
batchSize: 256
checkpoints_dir: checkpoints
continue_train: True
dataset: musicInstruments
decay_factor: 0.94
display_freq: 10
epoch_count: 0
fc_dimension: 1024
gpu_ids: []
init_type: normal
isTrain: True
learning_rate: 0.001
learning_rate_decrease_itr: 5
measure_time: False
mode: train
model: MIML
multi_label_threshold: 0.3
nThreads: 0
name: deepMIML
niter: 300
norm: max
num_of_bases: 25
num_of_fc: 1
save_epoch_freq: 5
save_latest_freq: 500
selected_classes: True
using_multi_labels: True
validation_batches: 10
validation_freq: 50
validation_on: True
with_batchnorm: True
with_softmax: False
zeroCenterInput: False
-------------- End ----------------
CustomDatasetDataLoader
dataset [MIMLDataset] was created
MIML dataset initialize suceed
(15,)
(15,)
#training images = 15
CustomDatasetDataLoader
dataset [MIMLDataset] was created
MIML dataset initialize suceed
(7,)
(7,)
#validation images = 7
MIML
initialization method [normal]
model [MIMLModel] was created
starting from epoch  0
saving the model at the end of epoch 5, total_steps 1280
current learning rate: 0.00094
decreased learning rate by  0.94
Display training progress at (epoch 10, total_steps 2560)
loss: 0.9070307672023773
mAP: 0.1351111111111111
end of display 

saving the model at the end of epoch 10, total_steps 2560
current learning rate: 0.0008836
decreased learning rate by  0.94
saving the model at the end of epoch 15, total_steps 3840
current learning rate: 0.0008305839999999999
decreased learning rate by  0.94
Display training progress at (epoch 20, total_steps 5120)
loss: 0.8234691381454468
mAP: 0.19111111111111118
end of display 

saving the model at the end of epoch 20, total_steps 5120
current learning rate: 0.0007807489599999998
decreased learning rate by  0.94
saving the model at the end of epoch 25, total_steps 6400
current learning rate: 0.0007339040223999998
decreased learning rate by  0.94
Display training progress at (epoch 30, total_steps 7680)
loss: 0.8161062002182007
mAP: 0.19111111111111118
end of display 

saving the model at the end of epoch 30, total_steps 7680
current learning rate: 0.0006898697810559998
decreased learning rate by  0.94
saving the model at the end of epoch 35, total_steps 8960
current learning rate: 0.0006484775941926397
decreased learning rate by  0.94
Display training progress at (epoch 40, total_steps 10240)
loss: 0.8123255670070648
mAP: 0.1911111111111112
end of display 

saving the model at the end of epoch 40, total_steps 10240
current learning rate: 0.0006095689385410813
decreased learning rate by  0.94
saving the model at the end of epoch 45, total_steps 11520
current learning rate: 0.0005729948022286164
decreased learning rate by  0.94
Display training progress at (epoch 50, total_steps 12800)
loss: 0.8111866593360901
mAP: 0.19111111111111118
end of display 

Display validation results at (epoch 50, total_steps 12800)
validation mAP is: 0.3333333333333333
validation loss is: 0.6703134775161743
end of display 

saving the model at the end of epoch 50, total_steps 12800
current learning rate: 0.0005386151140948994
decreased learning rate by  0.94
saving the model at the end of epoch 55, total_steps 14080
current learning rate: 0.0005062982072492054
decreased learning rate by  0.94
Display training progress at (epoch 60, total_steps 15360)
loss: 0.8098443508148193
mAP: 0.19111111111111118
end of display 

saving the model at the end of epoch 60, total_steps 15360
current learning rate: 0.0004759203148142531
decreased learning rate by  0.94
saving the model at the end of epoch 65, total_steps 16640
current learning rate: 0.00044736509592539786
decreased learning rate by  0.94
Display training progress at (epoch 70, total_steps 17920)
loss: 0.8098910748958588
mAP: 0.19111111111111115
end of display 

saving the model at the end of epoch 70, total_steps 17920
current learning rate: 0.00042052319016987395
decreased learning rate by  0.94
saving the model at the end of epoch 75, total_steps 19200
current learning rate: 0.00039529179875968153
decreased learning rate by  0.94
Display training progress at (epoch 80, total_steps 20480)
loss: 0.8097197115421295
mAP: 0.19111111111111118
end of display 

saving the model at the end of epoch 80, total_steps 20480
current learning rate: 0.0003715742908341006
decreased learning rate by  0.94
saving the model at the end of epoch 85, total_steps 21760
current learning rate: 0.00034927983338405456
decreased learning rate by  0.94
Display training progress at (epoch 90, total_steps 23040)
loss: 0.8096200585365295
mAP: 0.19111111111111118
end of display 

saving the model at the end of epoch 90, total_steps 23040
current learning rate: 0.0003283230433810113
decreased learning rate by  0.94
saving the model at the end of epoch 95, total_steps 24320
current learning rate: 0.00030862366077815057
decreased learning rate by  0.94
Display training progress at (epoch 100, total_steps 25600)
loss: 0.8095887660980224
mAP: 0.19111111111111118
end of display 

Display validation results at (epoch 100, total_steps 25600)
validation mAP is: 0.33333333333333337
validation loss is: 0.6731781959533691
end of display 

saving the model at the end of epoch 100, total_steps 25600
current learning rate: 0.0002901062411314615
decreased learning rate by  0.94
saving the model at the end of epoch 105, total_steps 26880
current learning rate: 0.00027269986666357375
decreased learning rate by  0.94
Display training progress at (epoch 110, total_steps 28160)
loss: 0.809403908252716
mAP: 0.19111111111111115
end of display 

saving the model at the end of epoch 110, total_steps 28160
current learning rate: 0.00025633787466375937
decreased learning rate by  0.94
saving the model at the end of epoch 115, total_steps 29440
current learning rate: 0.00024095760218393377
decreased learning rate by  0.94
Display training progress at (epoch 120, total_steps 30720)
loss: 0.809325760602951
mAP: 0.19111111111111118
end of display 

saving the model at the end of epoch 120, total_steps 30720
current learning rate: 0.00022650014605289773
decreased learning rate by  0.94
saving the model at the end of epoch 125, total_steps 32000
current learning rate: 0.00021291013728972385
decreased learning rate by  0.94
Display training progress at (epoch 130, total_steps 33280)
loss: 0.809350997209549
mAP: 0.19111111111111118
end of display 

saving the model at the end of epoch 130, total_steps 33280
current learning rate: 0.00020013552905234042
decreased learning rate by  0.94
saving the model at the end of epoch 135, total_steps 34560
current learning rate: 0.00018812739730919998
decreased learning rate by  0.94
Display training progress at (epoch 140, total_steps 35840)
loss: 0.8092908978462219
mAP: 0.19111111111111115
end of display 

saving the model at the end of epoch 140, total_steps 35840
current learning rate: 0.000176839753470648
decreased learning rate by  0.94
saving the model at the end of epoch 145, total_steps 37120
current learning rate: 0.0001662293682624091
decreased learning rate by  0.94
Display training progress at (epoch 150, total_steps 38400)
loss: 0.8092693150043487
mAP: 0.19111111111111118
end of display 

Display validation results at (epoch 150, total_steps 38400)
validation mAP is: 0.3333333333333334
validation loss is: 0.6676363945007324
end of display 

saving the model at the end of epoch 150, total_steps 38400
current learning rate: 0.00015625560616666453
decreased learning rate by  0.94
saving the model at the end of epoch 155, total_steps 39680
current learning rate: 0.00014688026979666467
decreased learning rate by  0.94
Display training progress at (epoch 160, total_steps 40960)
loss: 0.8093260228633881
mAP: 0.19111111111111118
end of display 

saving the model at the end of epoch 160, total_steps 40960
current learning rate: 0.00013806745360886476
decreased learning rate by D:\Anaconda3\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
 0.94
saving the model at the end of epoch 165, total_steps 42240
current learning rate: 0.00012978340639233289
decreased learning rate by  0.94
Display training progress at (epoch 170, total_steps 43520)
loss: 0.8091722369194031
mAP: 0.19111111111111118
end of display 

saving the model at the end of epoch 170, total_steps 43520
current learning rate: 0.00012199640200879289
decreased learning rate by  0.94
saving the model at the end of epoch 175, total_steps 44800
current learning rate: 0.00011467661788826532
decreased learning rate by  0.94
Display training progress at (epoch 180, total_steps 46080)
loss: 0.8091711282730103
mAP: 0.19111111111111118
end of display 

saving the model at the end of epoch 180, total_steps 46080
current learning rate: 0.00010779602081496939
decreased learning rate by  0.94
saving the model at the end of epoch 185, total_steps 47360
current learning rate: 0.00010132825956607122
decreased learning rate by  0.94
Display training progress at (epoch 190, total_steps 48640)
loss: 0.8091501951217651
mAP: 0.19111111111111118
end of display 

saving the model at the end of epoch 190, total_steps 48640
current learning rate: 9.524856399210693e-05
decreased learning rate by  0.94
saving the model at the end of epoch 195, total_steps 49920
current learning rate: 8.95336501525805e-05
decreased learning rate by  0.94
Display training progress at (epoch 200, total_steps 51200)
loss: 0.809169453382492
mAP: 0.19111111111111118
end of display 

Display validation results at (epoch 200, total_steps 51200)
validation mAP is: 0.3333333333333333
validation loss is: 0.6664620041847229
end of display 

saving the model at the end of epoch 200, total_steps 51200
current learning rate: 8.416163114342567e-05
decreased learning rate by  0.94
saving the model at the end of epoch 205, total_steps 52480
current learning rate: 7.911193327482013e-05
decreased learning rate by  0.94
Display training progress at (epoch 210, total_steps 53760)
loss: 0.8090904355049133
mAP: 0.19111111111111118
end of display 

saving the model at the end of epoch 210, total_steps 53760
current learning rate: 7.436521727833093e-05
decreased learning rate by  0.94
saving the model at the end of epoch 215, total_steps 55040
current learning rate: 6.990330424163106e-05
decreased learning rate by  0.94
Display training progress at (epoch 220, total_steps 56320)
loss: 0.8090845763683319
mAP: 0.1911111111111111
end of display 

saving the model at the end of epoch 220, total_steps 56320
current learning rate: 6.57091059871332e-05
decreased learning rate by  0.94
saving the model at the end of epoch 225, total_steps 57600
current learning rate: 6.17665596279052e-05
decreased learning rate by  0.94
Display training progress at (epoch 230, total_steps 58880)
loss: 0.8090736091136932
mAP: 0.19111111111111118
end of display 

saving the model at the end of epoch 230, total_steps 58880
current learning rate: 5.806056605023088e-05
decreased learning rate by  0.94
saving the model at the end of epoch 235, total_steps 60160
current learning rate: 5.457693208721703e-05
decreased learning rate by  0.94
Display training progress at (epoch 240, total_steps 61440)
loss: 0.8090323567390442
mAP: 0.19111111111111118
end of display 

saving the model at the end of epoch 240, total_steps 61440
current learning rate: 5.1302316161984e-05
decreased learning rate by  0.94
saving the model at the end of epoch 245, total_steps 62720
current learning rate: 4.822417719226496e-05
decreased learning rate by  0.94
Display training progress at (epoch 250, total_steps 64000)
loss: 0.8090392887592316
mAP: 0.19111111111111118
end of display 

Display validation results at (epoch 250, total_steps 64000)
validation mAP is: 0.3333333333333334
validation loss is: 0.6667874455451965
end of display 

saving the model at the end of epoch 250, total_steps 64000
current learning rate: 4.533072656072906e-05
decreased learning rate by  0.94
saving the model at the end of epoch 255, total_steps 65280
current learning rate: 4.261088296708532e-05
decreased learning rate by  0.94
Display training progress at (epoch 260, total_steps 66560)
loss: 0.8090284705162049
mAP: 0.1911111111111112
end of display 

saving the model at the end of epoch 260, total_steps 66560
current learning rate: 4.005422998906019e-05
decreased learning rate by  0.94
saving the model at the end of epoch 265, total_steps 67840
current learning rate: 3.765097618971658e-05
decreased learning rate by  0.94
Display training progress at (epoch 270, total_steps 69120)
loss: 0.8089883208274842
mAP: 0.19111111111111118
end of display 

saving the model at the end of epoch 270, total_steps 69120
current learning rate: 3.539191761833358e-05
decreased learning rate by  0.94
saving the model at the end of epoch 275, total_steps 70400
current learning rate: 3.326840256123357e-05
decreased learning rate by  0.94
Display training progress at (epoch 280, total_steps 71680)
loss: 0.8089946806430817
mAP: 0.19111111111111118
end of display 

saving the model at the end of epoch 280, total_steps 71680
current learning rate: 3.1272298407559555e-05
decreased learning rate by  0.94
saving the model at the end of epoch 285, total_steps 72960
current learning rate: 2.9395960503105978e-05
decreased learning rate by  0.94
Display training progress at (epoch 290, total_steps 74240)
loss: 0.8089582562446594
mAP: 0.19111111111111118
end of display 

saving the model at the end of epoch 290, total_steps 74240
current learning rate: 2.7632202872919617e-05
decreased learning rate by  0.94
saving the model at the end of epoch 295, total_steps 75520
current learning rate: 2.597427070054444e-05
decreased learning rate by  0.94
Display training progress at (epoch 300, total_steps 76800)
loss: 0.8089740872383118
mAP: 0.19111111111111115
end of display 

Display validation results at (epoch 300, total_steps 76800)
validation mAP is: 0.3333333333333334
validation loss is: 0.6670138239860535
end of display 

saving the model at the end of epoch 300, total_steps 76800
current learning rate: 2.441581445851177e-05
decreased learning rate by  0.94
D:\Anaconda3\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
------------ Options -------------
F: 2401
HDF5FileRoot: e:\Study\Vision\project\Deep-MIML-Network\dataset
K: 4
L: 15
batchSize: 256
checkpoints_dir: checkpoints
dataset: musicInstruments
fc_dimension: 1024
gpu_ids: []
how_many: 50
isTrain: False
mode: test
model: MIML
multi_label_threshold: 0.3
nThreads: 0
name: deepMIML
norm: max
num_of_bases: 25
num_of_fc: 1
selected_classes: True
using_multi_labels: True
which_epoch: latest
with_batchnorm: True
zeroCenterInput: False
-------------- End ----------------
CustomDatasetDataLoader
dataset [MIMLDataset] was created
MIML dataset initialize suceed
(3,)
(3,)
#testing images = 3
Traceback (most recent call last):
  File "test.py", line 17, in <module>
    model = torch.load(os.path.join('.', opt.checkpoints_dir, opt.name, str(opt.which_epoch) + '.pth'))
NameError: name 'torch' is not defined
Traceback (most recent call last):
  File "test.py", line 8, in <module>
    import pytorch as torch
ModuleNotFoundError: No module named 'pytorch'
D:\Anaconda3\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
------------ Options -------------
F: 2401
HDF5FileRoot: e:\Study\Vision\project\Deep-MIML-Network\dataset
K: 4
L: 15
batchSize: 256
checkpoints_dir: checkpoints
dataset: musicInstruments
fc_dimension: 1024
gpu_ids: []
how_many: 50
isTrain: False
mode: test
model: MIML
multi_label_threshold: 0.3
nThreads: 0
name: deepMIML
norm: max
num_of_bases: 25
num_of_fc: 1
selected_classes: True
using_multi_labels: True
which_epoch: latest
with_batchnorm: True
zeroCenterInput: False
-------------- End ----------------
CustomDatasetDataLoader
dataset [MIMLDataset] was created
MIML dataset initialize suceed
(3,)
(3,)
#testing images = 3
Traceback (most recent call last):
  File "test.py", line 18, in <module>
    model = torch.load(os.path.join('.', opt.checkpoints_dir, opt.name, str(opt.which_epoch) + '.pth'))
NameError: name 'os' is not defined
D:\Anaconda3\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
------------ Options -------------
F: 2401
HDF5FileRoot: e:\Study\Vision\project\Deep-MIML-Network\dataset
K: 4
L: 15
batchSize: 256
checkpoints_dir: checkpoints
dataset: musicInstruments
fc_dimension: 1024
gpu_ids: []
how_many: 50
isTrain: False
mode: test
model: MIML
multi_label_threshold: 0.3
nThreads: 0
name: deepMIML
norm: max
num_of_bases: 25
num_of_fc: 1
selected_classes: True
using_multi_labels: True
which_epoch: latest
with_batchnorm: True
zeroCenterInput: False
-------------- End ----------------
CustomDatasetDataLoader
dataset [MIMLDataset] was created
MIML dataset initialize suceed
(3,)
(3,)
#testing images = 3
0
Traceback (most recent call last):
  File "test.py", line 29, in <module>
    accuracy, loss = model.test(data)
  File "E:\Study\Vision\project\Deep-MIML-Network\models\MIML.py", line 191, in test
    loss = self.loss(self.output, label).data.cpu().numpy()[0]
IndexError: too many indices for array
D:\Anaconda3\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
------------ Options -------------
F: 2401
HDF5FileRoot: e:\Study\Vision\project\Deep-MIML-Network\dataset
K: 4
L: 15
batchSize: 256
checkpoints_dir: checkpoints
dataset: musicInstruments
fc_dimension: 1024
gpu_ids: []
how_many: 50
isTrain: False
mode: test
model: MIML
multi_label_threshold: 0.3
nThreads: 0
name: deepMIML
norm: max
num_of_bases: 25
num_of_fc: 1
selected_classes: True
using_multi_labels: True
which_epoch: latest
with_batchnorm: True
zeroCenterInput: False
-------------- End ----------------
CustomDatasetDataLoader
dataset [MIMLDataset] was created
MIML dataset initialize suceed
(3,)
(3,)
#testing images = 3
0
1
2
test accuracy is: tensor(0)
test loss is: 0.9337942401568095
