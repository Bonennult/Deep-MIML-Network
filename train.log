Traceback (most recent call last):
  File "train.py", line 6, in <module>
    from options.train_options import TrainOptions
  File "E:\Study\视听导\project\Deep-MIML-Network\options\train_options.py", line 4, in <module>
    from .base_options import BaseOptions
  File "E:\Study\视听导\project\Deep-MIML-Network\options\base_options.py", line 7, in <module>
    import torch
ModuleNotFoundError: No module named 'torch'
Traceback (most recent call last):
  File "train.py", line 6, in <module>
    from options.train_options import TrainOptions
  File "E:\Study\视听导\project\Deep-MIML-Network\options\train_options.py", line 4, in <module>
    from .base_options import BaseOptions
  File "E:\Study\视听导\project\Deep-MIML-Network\options\base_options.py", line 7, in <module>
    import torch
ModuleNotFoundError: No module named 'torch'
Traceback (most recent call last):
  File "train.py", line 9, in <module>
    from tensorboardX import SummaryWriter
ModuleNotFoundError: No module named 'tensorboardX'
usage: train.py [-h] [--K K] [--L L] [--F F] [--num_of_bases NUM_OF_BASES]
                [--num_of_fc NUM_OF_FC] [--with_batchnorm]
                [--fc_dimension FC_DIMENSION] --HDF5FileRoot HDF5FILEROOT
                [--gpu_ids GPU_IDS] [--name NAME]
                [--checkpoints_dir CHECKPOINTS_DIR]
                [--model {MIML,KL_divergence}] [--batchSize BATCHSIZE]
                [--nThreads NTHREADS] [--norm {l1,l2,max,none}]
                [--dataset {musicInstruments,animals,vehicles,all}]
                [--using_multi_labels]
                [--multi_label_threshold MULTI_LABEL_THRESHOLD]
                [--selected_classes] [--zeroCenterInput]
                [--display_freq DISPLAY_FREQ]
                [--save_epoch_freq SAVE_EPOCH_FREQ]
                [--save_latest_freq SAVE_LATEST_FREQ] [--continue_train]
                [--epoch_count EPOCH_COUNT] [--learning_rate LEARNING_RATE]
                [--learning_rate_decrease_itr LEARNING_RATE_DECREASE_ITR]
                [--decay_factor DECAY_FACTOR] [--niter NITER]
                [--init_type {normal,xavier,kaiming,orthogonal}]
                [--measure_time] [--validation_on]
                [--validation_freq VALIDATION_FREQ]
                [--validation_batches VALIDATION_BATCHES] [--with_softmax]
train.py: error: unrecognized arguments: --using_multilabels
D:\Anaconda3\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
------------ Options -------------
F: 2401
HDF5FileRoot: D:/Git/datasets/
K: 4
L: 15
batchSize: 256
checkpoints_dir: checkpoints
continue_train: True
dataset: musicInstruments
decay_factor: 0.94
display_freq: 10
epoch_count: 0
fc_dimension: 1024
gpu_ids: []
init_type: normal
isTrain: True
learning_rate: 0.001
learning_rate_decrease_itr: 5
measure_time: False
mode: train
model: MIML
multi_label_threshold: 0.3
nThreads: 2
name: deepMIML
niter: 300
norm: max
num_of_bases: 25
num_of_fc: 1
save_epoch_freq: 5
save_latest_freq: 500
selected_classes: True
using_multi_labels: True
validation_batches: 10
validation_freq: 50
validation_on: True
with_batchnorm: True
with_softmax: False
zeroCenterInput: False
-------------- End ----------------
CustomDatasetDataLoader
dataset [MIMLDataset] was created
Traceback (most recent call last):
  File "train.py", line 13, in <module>
    data_loader = CreateDataLoader(opt)
  File "E:\Study\视听导\project\Deep-MIML-Network\data\data_loader.py", line 8, in CreateDataLoader
    data_loader.initialize(opt)
  File "E:\Study\视听导\project\Deep-MIML-Network\data\custom_dataset_data_loader.py", line 29, in initialize
    self.dataset = CreateDataset(opt)
  File "E:\Study\视听导\project\Deep-MIML-Network\data\custom_dataset_data_loader.py", line 19, in CreateDataset
    dataset.initialize(opt)
  File "E:\Study\视听导\project\Deep-MIML-Network\data\MIML_dataset.py", line 89, in initialize
    h5f = h5py.File(h5f_path, 'r')
  File "D:\Anaconda3\lib\site-packages\h5py\_hl\files.py", line 269, in __init__
    fid = make_fid(name, mode, userblock_size, fapl, swmr=swmr)
  File "D:\Anaconda3\lib\site-packages\h5py\_hl\files.py", line 99, in make_fid
    fid = h5f.open(name, flags, fapl=fapl)
  File "h5py\_objects.pyx", line 54, in h5py._objects.with_phil.wrapper
  File "h5py\_objects.pyx", line 55, in h5py._objects.with_phil.wrapper
  File "h5py\h5f.pyx", line 78, in h5py.h5f.open
OSError: Unable to open file (unable to open file: name = 'D:/Git/datasets/train.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)
D:\Anaconda3\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
------------ Options -------------
F: 2401
HDF5FileRoot: D:/Git/datasets/
K: 4
L: 15
batchSize: 256
checkpoints_dir: checkpoints
continue_train: True
dataset: musicInstruments
decay_factor: 0.94
display_freq: 10
epoch_count: 0
fc_dimension: 1024
gpu_ids: []
init_type: normal
isTrain: True
learning_rate: 0.001
learning_rate_decrease_itr: 5
measure_time: False
mode: train
model: MIML
multi_label_threshold: 0.3
nThreads: 2
name: deepMIML
niter: 300
norm: max
num_of_bases: 25
num_of_fc: 1
save_epoch_freq: 5
save_latest_freq: 500
selected_classes: True
using_multi_labels: True
validation_batches: 10
validation_freq: 50
validation_on: True
with_batchnorm: True
with_softmax: False
zeroCenterInput: False
-------------- End ----------------
CustomDatasetDataLoader
dataset [MIMLDataset] was created
Traceback (most recent call last):
  File "train.py", line 13, in <module>
    data_loader = CreateDataLoader(opt)
  File "E:\Study\Vision\project\Deep-MIML-Network\data\data_loader.py", line 8, in CreateDataLoader
    data_loader.initialize(opt)
  File "E:\Study\Vision\project\Deep-MIML-Network\data\custom_dataset_data_loader.py", line 29, in initialize
    self.dataset = CreateDataset(opt)     #根据opt.model创建一个数据集类“MIMLDataset/KLdataset”，并初始化（包括读取h5文件，放入self.bases,self.labels）
  File "E:\Study\Vision\project\Deep-MIML-Network\data\custom_dataset_data_loader.py", line 19, in CreateDataset
    dataset.initialize(opt)
  File "E:\Study\Vision\project\Deep-MIML-Network\data\MIML_dataset.py", line 89, in initialize
    h5f = h5py.File(h5f_path, 'r')
  File "D:\Anaconda3\lib\site-packages\h5py\_hl\files.py", line 269, in __init__
    fid = make_fid(name, mode, userblock_size, fapl, swmr=swmr)
  File "D:\Anaconda3\lib\site-packages\h5py\_hl\files.py", line 99, in make_fid
    fid = h5f.open(name, flags, fapl=fapl)
  File "h5py\_objects.pyx", line 54, in h5py._objects.with_phil.wrapper
  File "h5py\_objects.pyx", line 55, in h5py._objects.with_phil.wrapper
  File "h5py\h5f.pyx", line 78, in h5py.h5f.open
OSError: Unable to open file (unable to open file: name = 'D:/Git/datasets/train.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)
D:\Anaconda3\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
------------ Options -------------
F: 2401
HDF5FileRoot: e:StudyVisionprojectDeep-MIML-Networkdataset
K: 4
L: 15
batchSize: 256
checkpoints_dir: checkpoints
continue_train: True
dataset: musicInstruments
decay_factor: 0.94
display_freq: 10
epoch_count: 0
fc_dimension: 1024
gpu_ids: []
init_type: normal
isTrain: True
learning_rate: 0.001
learning_rate_decrease_itr: 5
measure_time: False
mode: train
model: MIML
multi_label_threshold: 0.3
nThreads: 2
name: deepMIML
niter: 300
norm: max
num_of_bases: 25
num_of_fc: 1
save_epoch_freq: 5
save_latest_freq: 500
selected_classes: True
using_multi_labels: True
validation_batches: 10
validation_freq: 50
validation_on: True
with_batchnorm: True
with_softmax: False
zeroCenterInput: False
-------------- End ----------------
CustomDatasetDataLoader
dataset [MIMLDataset] was created
Traceback (most recent call last):
  File "train.py", line 13, in <module>
    data_loader = CreateDataLoader(opt)
  File "E:\Study\Vision\project\Deep-MIML-Network\data\data_loader.py", line 8, in CreateDataLoader
    data_loader.initialize(opt)
  File "E:\Study\Vision\project\Deep-MIML-Network\data\custom_dataset_data_loader.py", line 29, in initialize
    self.dataset = CreateDataset(opt)     #根据opt.model创建一个数据集类“MIMLDataset/KLdataset”，并初始化（包括读取h5文件，放入self.bases,self.labels）
  File "E:\Study\Vision\project\Deep-MIML-Network\data\custom_dataset_data_loader.py", line 19, in CreateDataset
    dataset.initialize(opt)
  File "E:\Study\Vision\project\Deep-MIML-Network\data\MIML_dataset.py", line 89, in initialize
    h5f = h5py.File(h5f_path, 'r')
  File "D:\Anaconda3\lib\site-packages\h5py\_hl\files.py", line 269, in __init__
    fid = make_fid(name, mode, userblock_size, fapl, swmr=swmr)
  File "D:\Anaconda3\lib\site-packages\h5py\_hl\files.py", line 99, in make_fid
    fid = h5f.open(name, flags, fapl=fapl)
  File "h5py\_objects.pyx", line 54, in h5py._objects.with_phil.wrapper
  File "h5py\_objects.pyx", line 55, in h5py._objects.with_phil.wrapper
  File "h5py\h5f.pyx", line 78, in h5py.h5f.open
OSError: Unable to open file (unable to open file: name = 'e:StudyVisionprojectDeep-MIML-Networkdataset\train.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)
D:\Anaconda3\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
------------ Options -------------
F: 2401
HDF5FileRoot: e:\Study\Vision\project\Deep-MIML-Network\dataset
K: 4
L: 15
batchSize: 256
checkpoints_dir: checkpoints
continue_train: True
dataset: musicInstruments
decay_factor: 0.94
display_freq: 10
epoch_count: 0
fc_dimension: 1024
gpu_ids: []
init_type: normal
isTrain: True
learning_rate: 0.001
learning_rate_decrease_itr: 5
measure_time: False
mode: train
model: MIML
multi_label_threshold: 0.3
nThreads: 2
name: deepMIML
niter: 300
norm: max
num_of_bases: 25
num_of_fc: 1
save_epoch_freq: 5
save_latest_freq: 500
selected_classes: True
using_multi_labels: True
validation_batches: 10
validation_freq: 50
validation_on: True
with_batchnorm: True
with_softmax: False
zeroCenterInput: False
-------------- End ----------------
CustomDatasetDataLoader
dataset [MIMLDataset] was created
#training images = 15
CustomDatasetDataLoader
dataset [MIMLDataset] was created
Traceback (most recent call last):
  File "train.py", line 22, in <module>
    data_loader_val = CreateDataLoader(opt)
  File "E:\Study\Vision\project\Deep-MIML-Network\data\data_loader.py", line 8, in CreateDataLoader
    data_loader.initialize(opt)
  File "E:\Study\Vision\project\Deep-MIML-Network\data\custom_dataset_data_loader.py", line 29, in initialize
    self.dataset = CreateDataset(opt)     #根据opt.model创建一个数据集类“MIMLDataset/KLdataset”，并初始化（包括读取h5文件，放入self.bases,self.labels）
  File "E:\Study\Vision\project\Deep-MIML-Network\data\custom_dataset_data_loader.py", line 19, in CreateDataset
    dataset.initialize(opt)
  File "E:\Study\Vision\project\Deep-MIML-Network\data\MIML_dataset.py", line 89, in initialize
    h5f = h5py.File(h5f_path, 'r')
  File "D:\Anaconda3\lib\site-packages\h5py\_hl\files.py", line 269, in __init__
    fid = make_fid(name, mode, userblock_size, fapl, swmr=swmr)
  File "D:\Anaconda3\lib\site-packages\h5py\_hl\files.py", line 99, in make_fid
    fid = h5f.open(name, flags, fapl=fapl)
  File "h5py\_objects.pyx", line 54, in h5py._objects.with_phil.wrapper
  File "h5py\_objects.pyx", line 55, in h5py._objects.with_phil.wrapper
  File "h5py\h5f.pyx", line 78, in h5py.h5f.open
OSError: Unable to open file (unable to open file: name = 'e:\Study\Vision\project\Deep-MIML-Network\dataset\val.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)
D:\Anaconda3\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
E:\Study\Vision\project\Deep-MIML-Network\models\networks.py:13: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.
  init.normal(m.weight.data, 0.0, 0.02)
E:\Study\Vision\project\Deep-MIML-Network\models\networks.py:17: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.
  init.normal(m.weight.data, 1.0, 0.02)
E:\Study\Vision\project\Deep-MIML-Network\models\networks.py:18: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.
  init.constant(m.bias.data, 0.0)
Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 114, in _main
    prepare(preparation_data)
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 225, in prepare
    _fixup_main_from_path(data['init_main_from_path'])
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 277, in _fixup_main_from_path
    run_name="__mp_main__")
  File "D:\Anaconda3\lib\runpy.py", line 263, in run_path
    pkg_name=pkg_name, script_name=fname)
  File "D:\Anaconda3\lib\runpy.py", line 96, in _run_module_code
    mod_name, mod_spec, pkg_name, script_name)
  File "D:\Anaconda3\lib\runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "E:\Study\Vision\project\Deep-MIML-Network\train.py", line 65, in <module>
    for i, data in enumerate(dataset):
  File "E:\Study\Vision\project\Deep-MIML-Network\data\custom_dataset_data_loader.py", line 43, in __iter__
    for i, data in enumerate(self.dataloader):
  File "D:\Anaconda3\lib\site-packages\torch\utils\data\dataloader.py", line 819, in __iter__
    return _DataLoaderIter(self)
  File "D:\Anaconda3\lib\site-packages\torch\utils\data\dataloader.py", line 560, in __init__
    w.start()
  File "D:\Anaconda3\lib\multiprocessing\process.py", line 105, in start
    self._popen = self._Popen(self)
  File "D:\Anaconda3\lib\multiprocessing\context.py", line 223, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "D:\Anaconda3\lib\multiprocessing\context.py", line 322, in _Popen
    return Popen(process_obj)
  File "D:\Anaconda3\lib\multiprocessing\popen_spawn_win32.py", line 33, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 143, in get_preparation_data
    _check_not_importing_main()
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 136, in _check_not_importing_main
    is not going to be frozen to produce an executable.''')
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.
------------ Options -------------
F: 2401
HDF5FileRoot: e:\Study\Vision\project\Deep-MIML-Network\dataset
K: 4
L: 15
batchSize: 256
checkpoints_dir: checkpoints
continue_train: True
dataset: musicInstruments
decay_factor: 0.94
display_freq: 10
epoch_count: 0
fc_dimension: 1024
gpu_ids: []
init_type: normal
isTrain: True
learning_rate: 0.001
learning_rate_decrease_itr: 5
measure_time: False
mode: train
model: MIML
multi_label_threshold: 0.3
nThreads: 2
name: deepMIML
niter: 300
norm: max
num_of_bases: 25
num_of_fc: 1
save_epoch_freq: 5
save_latest_freq: 500
selected_classes: True
using_multi_labels: True
validation_batches: 10
validation_freq: 50
validation_on: False
with_batchnorm: True
with_softmax: False
zeroCenterInput: False
-------------- End ----------------
CustomDatasetDataLoader
dataset [MIMLDataset] was created
#training images = 15
MIML
initialization method [normal]
model [MIMLModel] was created
starting from epoch  0
D:\Anaconda3\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
E:\Study\Vision\project\Deep-MIML-Network\models\networks.py:13: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.
  init.normal(m.weight.data, 0.0, 0.02)
E:\Study\Vision\project\Deep-MIML-Network\models\networks.py:17: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.
  init.normal(m.weight.data, 1.0, 0.02)
E:\Study\Vision\project\Deep-MIML-Network\models\networks.py:18: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.
  init.constant(m.bias.data, 0.0)
Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 114, in _main
    prepare(preparation_data)
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 225, in prepare
    _fixup_main_from_path(data['init_main_from_path'])
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 277, in _fixup_main_from_path
    run_name="__mp_main__")
  File "D:\Anaconda3\lib\runpy.py", line 263, in run_path
    pkg_name=pkg_name, script_name=fname)
  File "D:\Anaconda3\lib\runpy.py", line 96, in _run_module_code
    mod_name, mod_spec, pkg_name, script_name)
  File "D:\Anaconda3\lib\runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "E:\Study\Vision\project\Deep-MIML-Network\train.py", line 65, in <module>
    for i, data in enumerate(dataset):
  File "E:\Study\Vision\project\Deep-MIML-Network\data\custom_dataset_data_loader.py", line 43, in __iter__
    for i, data in enumerate(self.dataloader):
  File "D:\Anaconda3\lib\site-packages\torch\utils\data\dataloader.py", line 819, in __iter__
    return _DataLoaderIter(self)
  File "D:\Anaconda3\lib\site-packages\torch\utils\data\dataloader.py", line 560, in __init__
    w.start()
  File "D:\Anaconda3\lib\multiprocessing\process.py", line 105, in start
    self._popen = self._Popen(self)
  File "D:\Anaconda3\lib\multiprocessing\context.py", line 223, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "D:\Anaconda3\lib\multiprocessing\context.py", line 322, in _Popen
    return Popen(process_obj)
  File "D:\Anaconda3\lib\multiprocessing\popen_spawn_win32.py", line 33, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 143, in get_preparation_data
    _check_not_importing_main()
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 136, in _check_not_importing_main
    is not going to be frozen to produce an executable.''')
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.
------------ Options -------------
F: 2401
HDF5FileRoot: e:\Study\Vision\project\Deep-MIML-Network\dataset
K: 4
L: 15
batchSize: 256
checkpoints_dir: checkpoints
continue_train: True
dataset: musicInstruments
decay_factor: 0.94
display_freq: 10
epoch_count: 0
fc_dimension: 1024
gpu_ids: []
init_type: normal
isTrain: True
learning_rate: 0.001
learning_rate_decrease_itr: 5
measure_time: False
mode: train
model: MIML
multi_label_threshold: 0.3
nThreads: 2
name: deepMIML
niter: 300
norm: max
num_of_bases: 25
num_of_fc: 1
save_epoch_freq: 5
save_latest_freq: 500
selected_classes: True
using_multi_labels: True
validation_batches: 10
validation_freq: 50
validation_on: False
with_batchnorm: True
with_softmax: False
zeroCenterInput: False
-------------- End ----------------
CustomDatasetDataLoader
dataset [MIMLDataset] was created
#training images = 15
MIML
initialization method [normal]
model [MIMLModel] was created
starting from epoch  0
D:\Anaconda3\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
------------ Options -------------
F: 2401
HDF5FileRoot: e:\Study\Vision\project\Deep-MIML-Network\dataset
K: 4
L: 15
batchSize: 256
checkpoints_dir: checkpoints
continue_train: True
dataset: musicInstruments
decay_factor: 0.94
display_freq: 10
epoch_count: 0
fc_dimension: 1024
gpu_ids: []
init_type: normal
isTrain: True
learning_rate: 0.001
learning_rate_decrease_itr: 5
measure_time: False
mode: train
model: KL_divergence
multi_label_threshold: 0.3
nThreads: 2
name: deepMIML
niter: 300
norm: max
num_of_bases: 25
num_of_fc: 1
save_epoch_freq: 5
save_latest_freq: 500
selected_classes: True
using_multi_labels: True
validation_batches: 10
validation_freq: 50
validation_on: False
with_batchnorm: True
with_softmax: False
zeroCenterInput: False
-------------- End ----------------
CustomDatasetDataLoader
dataset [KLDataset] was created
#training images = 15
KL_divergence
loading epoch 0 model to continue training!
Traceback (most recent call last):
  File "train.py", line 35, in <module>
    model = create_model(opt)
  File "E:\Study\Vision\project\Deep-MIML-Network\models\models.py", line 15, in create_model
    model.initialize(opt)
  File "E:\Study\Vision\project\Deep-MIML-Network\models\KL_divergence.py", line 41, in initialize
    self.load_network(self.BasesNet, opt.epoch_count)
AttributeError: 'KLModel' object has no attribute 'load_network'
D:\Anaconda3\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
E:\Study\Vision\project\Deep-MIML-Network\models\networks.py:13: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.
  init.normal(m.weight.data, 0.0, 0.02)
E:\Study\Vision\project\Deep-MIML-Network\models\networks.py:17: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.
  init.normal(m.weight.data, 1.0, 0.02)
E:\Study\Vision\project\Deep-MIML-Network\models\networks.py:18: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.
  init.constant(m.bias.data, 0.0)
Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 114, in _main
    prepare(preparation_data)
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 225, in prepare
    _fixup_main_from_path(data['init_main_from_path'])
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 277, in _fixup_main_from_path
    run_name="__mp_main__")
  File "D:\Anaconda3\lib\runpy.py", line 263, in run_path
    pkg_name=pkg_name, script_name=fname)
  File "D:\Anaconda3\lib\runpy.py", line 96, in _run_module_code
    mod_name, mod_spec, pkg_name, script_name)
  File "D:\Anaconda3\lib\runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "E:\Study\Vision\project\Deep-MIML-Network\train.py", line 65, in <module>
    for i, data in enumerate(dataset):
  File "E:\Study\Vision\project\Deep-MIML-Network\data\custom_dataset_data_loader.py", line 43, in __iter__
    for i, data in enumerate(self.dataloader):
  File "D:\Anaconda3\lib\site-packages\torch\utils\data\dataloader.py", line 819, in __iter__
    return _DataLoaderIter(self)
  File "D:\Anaconda3\lib\site-packages\torch\utils\data\dataloader.py", line 560, in __init__
    w.start()
  File "D:\Anaconda3\lib\multiprocessing\process.py", line 105, in start
    self._popen = self._Popen(self)
  File "D:\Anaconda3\lib\multiprocessing\context.py", line 223, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "D:\Anaconda3\lib\multiprocessing\context.py", line 322, in _Popen
    return Popen(process_obj)
  File "D:\Anaconda3\lib\multiprocessing\popen_spawn_win32.py", line 33, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 143, in get_preparation_data
    _check_not_importing_main()
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 136, in _check_not_importing_main
    is not going to be frozen to produce an executable.''')
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.
------------ Options -------------
F: 2401
HDF5FileRoot: e:\Study\Vision\project\Deep-MIML-Network\dataset
K: 4
L: 15
batchSize: 256
checkpoints_dir: checkpoints
continue_train: True
dataset: musicInstruments
decay_factor: 0.94
display_freq: 10
epoch_count: 0
fc_dimension: 1024
gpu_ids: []
init_type: normal
isTrain: True
learning_rate: 0.001
learning_rate_decrease_itr: 5
measure_time: False
mode: train
model: MIML
multi_label_threshold: 0.3
nThreads: 2
name: deepMIML
niter: 300
norm: max
num_of_bases: 25
num_of_fc: 1
save_epoch_freq: 5
save_latest_freq: 500
selected_classes: True
using_multi_labels: True
validation_batches: 10
validation_freq: 50
validation_on: False
with_batchnorm: True
with_softmax: False
zeroCenterInput: False
-------------- End ----------------
CustomDatasetDataLoader
dataset [MIMLDataset] was created
#training images = 15
MIML
initialization method [normal]
model [MIMLModel] was created
starting from epoch  0
D:\Anaconda3\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
E:\Study\Vision\project\Deep-MIML-Network\models\networks.py:13: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.
  init.normal(m.weight.data, 0.0, 0.02)
E:\Study\Vision\project\Deep-MIML-Network\models\networks.py:17: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.
  init.normal(m.weight.data, 1.0, 0.02)
E:\Study\Vision\project\Deep-MIML-Network\models\networks.py:18: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.
  init.constant(m.bias.data, 0.0)
Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 114, in _main
    prepare(preparation_data)
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 225, in prepare
    _fixup_main_from_path(data['init_main_from_path'])
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 277, in _fixup_main_from_path
    run_name="__mp_main__")
  File "D:\Anaconda3\lib\runpy.py", line 263, in run_path
    pkg_name=pkg_name, script_name=fname)
  File "D:\Anaconda3\lib\runpy.py", line 96, in _run_module_code
    mod_name, mod_spec, pkg_name, script_name)
  File "D:\Anaconda3\lib\runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "E:\Study\Vision\project\Deep-MIML-Network\train.py", line 65, in <module>
    for i, data in enumerate(dataset):
  File "E:\Study\Vision\project\Deep-MIML-Network\data\custom_dataset_data_loader.py", line 43, in __iter__
    for i, data in enumerate(self.dataloader):
  File "D:\Anaconda3\lib\site-packages\torch\utils\data\dataloader.py", line 819, in __iter__
    return _DataLoaderIter(self)
  File "D:\Anaconda3\lib\site-packages\torch\utils\data\dataloader.py", line 560, in __init__
    w.start()
  File "D:\Anaconda3\lib\multiprocessing\process.py", line 105, in start
    self._popen = self._Popen(self)
  File "D:\Anaconda3\lib\multiprocessing\context.py", line 223, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "D:\Anaconda3\lib\multiprocessing\context.py", line 322, in _Popen
    return Popen(process_obj)
  File "D:\Anaconda3\lib\multiprocessing\popen_spawn_win32.py", line 33, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 143, in get_preparation_data
    _check_not_importing_main()
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 136, in _check_not_importing_main
    is not going to be frozen to produce an executable.''')
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.
------------ Options -------------
F: 2401
HDF5FileRoot: e:\Study\Vision\project\Deep-MIML-Network\dataset
K: 4
L: 15
batchSize: 256
checkpoints_dir: checkpoints
continue_train: True
dataset: musicInstruments
decay_factor: 0.94
display_freq: 10
epoch_count: 0
fc_dimension: 1024
gpu_ids: []
init_type: normal
isTrain: True
learning_rate: 0.001
learning_rate_decrease_itr: 5
measure_time: False
mode: train
model: MIML
multi_label_threshold: 0.3
nThreads: 2
name: deepMIML
niter: 300
norm: max
num_of_bases: 25
num_of_fc: 1
save_epoch_freq: 5
save_latest_freq: 500
selected_classes: True
using_multi_labels: True
validation_batches: 10
validation_freq: 50
validation_on: False
with_batchnorm: True
with_softmax: False
zeroCenterInput: False
-------------- End ----------------
CustomDatasetDataLoader
dataset [MIMLDataset] was created
#training images = 15
MIML
initialization method [normal]
model [MIMLModel] was created
starting from epoch  0
D:\Anaconda3\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
E:\Study\Vision\project\Deep-MIML-Network\models\networks.py:13: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.
  init.normal(m.weight.data, 0.0, 0.02)
E:\Study\Vision\project\Deep-MIML-Network\models\networks.py:17: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.
  init.normal(m.weight.data, 1.0, 0.02)
E:\Study\Vision\project\Deep-MIML-Network\models\networks.py:18: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.
  init.constant(m.bias.data, 0.0)
Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 114, in _main
    prepare(preparation_data)
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 225, in prepare
    _fixup_main_from_path(data['init_main_from_path'])
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 277, in _fixup_main_from_path
    run_name="__mp_main__")
  File "D:\Anaconda3\lib\runpy.py", line 263, in run_path
    pkg_name=pkg_name, script_name=fname)
  File "D:\Anaconda3\lib\runpy.py", line 96, in _run_module_code
    mod_name, mod_spec, pkg_name, script_name)
  File "D:\Anaconda3\lib\runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "E:\Study\Vision\project\Deep-MIML-Network\train.py", line 65, in <module>
    for i, data in enumerate(dataset):
  File "E:\Study\Vision\project\Deep-MIML-Network\data\custom_dataset_data_loader.py", line 43, in __iter__
    for i, data in enumerate(self.dataloader):
  File "D:\Anaconda3\lib\site-packages\torch\utils\data\dataloader.py", line 819, in __iter__
    return _DataLoaderIter(self)
  File "D:\Anaconda3\lib\site-packages\torch\utils\data\dataloader.py", line 560, in __init__
    w.start()
  File "D:\Anaconda3\lib\multiprocessing\process.py", line 105, in start
    self._popen = self._Popen(self)
  File "D:\Anaconda3\lib\multiprocessing\context.py", line 223, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "D:\Anaconda3\lib\multiprocessing\context.py", line 322, in _Popen
    return Popen(process_obj)
  File "D:\Anaconda3\lib\multiprocessing\popen_spawn_win32.py", line 33, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 143, in get_preparation_data
    _check_not_importing_main()
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 136, in _check_not_importing_main
    is not going to be frozen to produce an executable.''')
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.
------------ Options -------------
F: 2401
HDF5FileRoot: e:\Study\Vision\project\Deep-MIML-Network\dataset
K: 4
L: 15
batchSize: 256
checkpoints_dir: checkpoints
continue_train: True
dataset: musicInstruments
decay_factor: 0.94
display_freq: 10
epoch_count: 0
fc_dimension: 1024
gpu_ids: [0]
init_type: normal
isTrain: True
learning_rate: 0.001
learning_rate_decrease_itr: 5
measure_time: False
mode: train
model: MIML
multi_label_threshold: 0.3
nThreads: 1
name: deepMIML
niter: 300
norm: max
num_of_bases: 25
num_of_fc: 1
save_epoch_freq: 5
save_latest_freq: 500
selected_classes: True
using_multi_labels: True
validation_batches: 10
validation_freq: 50
validation_on: False
with_batchnorm: True
with_softmax: False
zeroCenterInput: False
-------------- End ----------------
CustomDatasetDataLoader
dataset [MIMLDataset] was created
#training images = 15
MIML
initialization method [normal]
model [MIMLModel] was created
starting from epoch  0
D:\Anaconda3\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
E:\Study\Vision\project\Deep-MIML-Network\models\networks.py:13: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.
  init.normal(m.weight.data, 0.0, 0.02)
E:\Study\Vision\project\Deep-MIML-Network\models\networks.py:17: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.
  init.normal(m.weight.data, 1.0, 0.02)
E:\Study\Vision\project\Deep-MIML-Network\models\networks.py:18: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.
  init.constant(m.bias.data, 0.0)
Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 114, in _main
    prepare(preparation_data)
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 225, in prepare
    _fixup_main_from_path(data['init_main_from_path'])
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 277, in _fixup_main_from_path
    run_name="__mp_main__")
  File "D:\Anaconda3\lib\runpy.py", line 263, in run_path
    pkg_name=pkg_name, script_name=fname)
  File "D:\Anaconda3\lib\runpy.py", line 96, in _run_module_code
    mod_name, mod_spec, pkg_name, script_name)
  File "D:\Anaconda3\lib\runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "E:\Study\Vision\project\Deep-MIML-Network\train.py", line 65, in <module>
    for i, data in enumerate(dataset):
  File "E:\Study\Vision\project\Deep-MIML-Network\data\custom_dataset_data_loader.py", line 43, in __iter__
    for i, data in enumerate(self.dataloader):
  File "D:\Anaconda3\lib\site-packages\torch\utils\data\dataloader.py", line 819, in __iter__
    return _DataLoaderIter(self)
  File "D:\Anaconda3\lib\site-packages\torch\utils\data\dataloader.py", line 560, in __init__
    w.start()
  File "D:\Anaconda3\lib\multiprocessing\process.py", line 105, in start
    self._popen = self._Popen(self)
  File "D:\Anaconda3\lib\multiprocessing\context.py", line 223, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "D:\Anaconda3\lib\multiprocessing\context.py", line 322, in _Popen
    return Popen(process_obj)
  File "D:\Anaconda3\lib\multiprocessing\popen_spawn_win32.py", line 33, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 143, in get_preparation_data
    _check_not_importing_main()
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 136, in _check_not_importing_main
    is not going to be frozen to produce an executable.''')
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.
------------ Options -------------
F: 2401
HDF5FileRoot: e:\Study\Vision\project\Deep-MIML-Network\dataset
K: 4
L: 15
batchSize: 256
checkpoints_dir: checkpoints
continue_train: True
dataset: musicInstruments
decay_factor: 0.94
display_freq: 10
epoch_count: 0
fc_dimension: 1024
gpu_ids: [0]
init_type: normal
isTrain: True
learning_rate: 0.001
learning_rate_decrease_itr: 5
measure_time: False
mode: train
model: MIML
multi_label_threshold: 0.3
nThreads: 1
name: deepMIML
niter: 300
norm: max
num_of_bases: 25
num_of_fc: 1
save_epoch_freq: 5
save_latest_freq: 500
selected_classes: True
using_multi_labels: True
validation_batches: 10
validation_freq: 50
validation_on: False
with_batchnorm: True
with_softmax: False
zeroCenterInput: False
-------------- End ----------------
CustomDatasetDataLoader
dataset [MIMLDataset] was created
#training images = 15
MIML
initialization method [normal]
model [MIMLModel] was created
starting from epoch  0
------------ Options -------------
F: 2401
HDF5FileRoot: e:\Study\Vision\project\Deep-MIML-Network\dataset
K: 4
L: 15
batchSize: 256
checkpoints_dir: checkpoints
continue_train: True
dataset: musicInstruments
decay_factor: 0.94
display_freq: 10
epoch_count: 0
fc_dimension: 1024
gpu_ids: [0]
init_type: normal
isTrain: True
learning_rate: 0.001
learning_rate_decrease_itr: 5
measure_time: False
mode: train
model: MIML
multi_label_threshold: 0.3
nThreads: 1
name: deepMIML
niter: 300
norm: max
num_of_bases: 25
num_of_fc: 1
save_epoch_freq: 5
save_latest_freq: 500
selected_classes: True
using_multi_labels: True
validation_batches: 10
validation_freq: 50
validation_on: False
with_batchnorm: True
with_softmax: False
zeroCenterInput: False
-------------- End ----------------
CustomDatasetDataLoader
Traceback (most recent call last):
  File "train.py", line 13, in <module>
    data_loader = CreateDataLoader(opt)
  File "E:\Study\Vision\project\Deep-MIML-Network\data\data_loader.py", line 8, in CreateDataLoader
    data_loader.initialize(opt)
  File "E:\Study\Vision\project\Deep-MIML-Network\data\custom_dataset_data_loader.py", line 29, in initialize
    self.dataset = CreateDataset(opt)     #根据opt.model创建一个数据集类“MIMLDataset/KLdataset”，并初始化（包括读取h5文件，放入self.bases,self.labels）
  File "E:\Study\Vision\project\Deep-MIML-Network\data\custom_dataset_data_loader.py", line 10, in CreateDataset
    from data.MIML_dataset import MIMLDataset
  File "E:\Study\Vision\project\Deep-MIML-Network\data\MIML_dataset.py", line 92
    print('MIML dataset initialize suceed')
                                          ^
TabError: inconsistent use of tabs and spaces in indentation
D:\Anaconda3\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
E:\Study\Vision\project\Deep-MIML-Network\models\networks.py:13: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.
  init.normal(m.weight.data, 0.0, 0.02)
E:\Study\Vision\project\Deep-MIML-Network\models\networks.py:17: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.
  init.normal(m.weight.data, 1.0, 0.02)
E:\Study\Vision\project\Deep-MIML-Network\models\networks.py:18: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.
  init.constant(m.bias.data, 0.0)
Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 114, in _main
    prepare(preparation_data)
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 225, in prepare
    _fixup_main_from_path(data['init_main_from_path'])
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 277, in _fixup_main_from_path
    run_name="__mp_main__")
  File "D:\Anaconda3\lib\runpy.py", line 263, in run_path
    pkg_name=pkg_name, script_name=fname)
  File "D:\Anaconda3\lib\runpy.py", line 96, in _run_module_code
    mod_name, mod_spec, pkg_name, script_name)
  File "D:\Anaconda3\lib\runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "E:\Study\Vision\project\Deep-MIML-Network\train.py", line 65, in <module>
    for i, data in enumerate(dataset):
  File "E:\Study\Vision\project\Deep-MIML-Network\data\custom_dataset_data_loader.py", line 43, in __iter__
    for i, data in enumerate(self.dataloader):
  File "D:\Anaconda3\lib\site-packages\torch\utils\data\dataloader.py", line 819, in __iter__
    return _DataLoaderIter(self)
  File "D:\Anaconda3\lib\site-packages\torch\utils\data\dataloader.py", line 560, in __init__
    w.start()
  File "D:\Anaconda3\lib\multiprocessing\process.py", line 105, in start
    self._popen = self._Popen(self)
  File "D:\Anaconda3\lib\multiprocessing\context.py", line 223, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "D:\Anaconda3\lib\multiprocessing\context.py", line 322, in _Popen
    return Popen(process_obj)
  File "D:\Anaconda3\lib\multiprocessing\popen_spawn_win32.py", line 33, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 143, in get_preparation_data
    _check_not_importing_main()
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 136, in _check_not_importing_main
    is not going to be frozen to produce an executable.''')
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.
------------ Options -------------
F: 2401
HDF5FileRoot: e:\Study\Vision\project\Deep-MIML-Network\dataset
K: 4
L: 15
batchSize: 256
checkpoints_dir: checkpoints
continue_train: True
dataset: musicInstruments
decay_factor: 0.94
display_freq: 10
epoch_count: 0
fc_dimension: 1024
gpu_ids: [0]
init_type: normal
isTrain: True
learning_rate: 0.001
learning_rate_decrease_itr: 5
measure_time: False
mode: train
model: MIML
multi_label_threshold: 0.3
nThreads: 1
name: deepMIML
niter: 300
norm: max
num_of_bases: 25
num_of_fc: 1
save_epoch_freq: 5
save_latest_freq: 500
selected_classes: True
using_multi_labels: True
validation_batches: 10
validation_freq: 50
validation_on: False
with_batchnorm: True
with_softmax: False
zeroCenterInput: False
-------------- End ----------------
CustomDatasetDataLoader
dataset [MIMLDataset] was created
MIML dataset initialize suceed
#training images = 15
MIML
initialization method [normal]
model [MIMLModel] was created
starting from epoch  0
D:\Anaconda3\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
E:\Study\Vision\project\Deep-MIML-Network\models\networks.py:13: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.
  init.normal(m.weight.data, 0.0, 0.02)
E:\Study\Vision\project\Deep-MIML-Network\models\networks.py:17: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.
  init.normal(m.weight.data, 1.0, 0.02)
E:\Study\Vision\project\Deep-MIML-Network\models\networks.py:18: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.
  init.constant(m.bias.data, 0.0)
Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 114, in _main
    prepare(preparation_data)
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 225, in prepare
    _fixup_main_from_path(data['init_main_from_path'])
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 277, in _fixup_main_from_path
    run_name="__mp_main__")
  File "D:\Anaconda3\lib\runpy.py", line 263, in run_path
    pkg_name=pkg_name, script_name=fname)
  File "D:\Anaconda3\lib\runpy.py", line 96, in _run_module_code
    mod_name, mod_spec, pkg_name, script_name)
  File "D:\Anaconda3\lib\runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "E:\Study\Vision\project\Deep-MIML-Network\train.py", line 65, in <module>
    for i, data in enumerate(dataset):
  File "E:\Study\Vision\project\Deep-MIML-Network\data\custom_dataset_data_loader.py", line 43, in __iter__
    for i, data in enumerate(self.dataloader):
  File "D:\Anaconda3\lib\site-packages\torch\utils\data\dataloader.py", line 819, in __iter__
    return _DataLoaderIter(self)
  File "D:\Anaconda3\lib\site-packages\torch\utils\data\dataloader.py", line 560, in __init__
    w.start()
  File "D:\Anaconda3\lib\multiprocessing\process.py", line 105, in start
    self._popen = self._Popen(self)
  File "D:\Anaconda3\lib\multiprocessing\context.py", line 223, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "D:\Anaconda3\lib\multiprocessing\context.py", line 322, in _Popen
    return Popen(process_obj)
  File "D:\Anaconda3\lib\multiprocessing\popen_spawn_win32.py", line 33, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 143, in get_preparation_data
    _check_not_importing_main()
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 136, in _check_not_importing_main
    is not going to be frozen to produce an executable.''')
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.
------------ Options -------------
F: 2401
HDF5FileRoot: e:\Study\Vision\project\Deep-MIML-Network\dataset
K: 4
L: 15
batchSize: 256
checkpoints_dir: checkpoints
continue_train: True
dataset: musicInstruments
decay_factor: 0.94
display_freq: 10
epoch_count: 0
fc_dimension: 1024
gpu_ids: [0]
init_type: normal
isTrain: True
learning_rate: 0.001
learning_rate_decrease_itr: 5
measure_time: False
mode: train
model: MIML
multi_label_threshold: 0.3
nThreads: 1
name: deepMIML
niter: 300
norm: max
num_of_bases: 25
num_of_fc: 1
save_epoch_freq: 5
save_latest_freq: 500
selected_classes: True
using_multi_labels: True
validation_batches: 10
validation_freq: 50
validation_on: False
with_batchnorm: True
with_softmax: False
zeroCenterInput: False
-------------- End ----------------
CustomDatasetDataLoader
dataset [MIMLDataset] was created
MIML dataset initialize suceed
(15,)
(15,)
#training images = 15
MIML
initialization method [normal]
model [MIMLModel] was created
starting from epoch  0
D:\Anaconda3\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
E:\Study\Vision\project\Deep-MIML-Network\models\networks.py:13: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.
  init.normal(m.weight.data, 0.0, 0.02)
E:\Study\Vision\project\Deep-MIML-Network\models\networks.py:17: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.
  init.normal(m.weight.data, 1.0, 0.02)
E:\Study\Vision\project\Deep-MIML-Network\models\networks.py:18: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.
  init.constant(m.bias.data, 0.0)
Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 114, in _main
    prepare(preparation_data)
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 225, in prepare
    _fixup_main_from_path(data['init_main_from_path'])
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 277, in _fixup_main_from_path
    run_name="__mp_main__")
  File "D:\Anaconda3\lib\runpy.py", line 263, in run_path
    pkg_name=pkg_name, script_name=fname)
  File "D:\Anaconda3\lib\runpy.py", line 96, in _run_module_code
    mod_name, mod_spec, pkg_name, script_name)
  File "D:\Anaconda3\lib\runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "E:\Study\Vision\project\Deep-MIML-Network\train.py", line 65, in <module>
    for i, data in enumerate(dataset):
  File "E:\Study\Vision\project\Deep-MIML-Network\data\custom_dataset_data_loader.py", line 43, in __iter__
    for i, data in enumerate(self.dataloader):
  File "D:\Anaconda3\lib\site-packages\torch\utils\data\dataloader.py", line 819, in __iter__
    return _DataLoaderIter(self)
  File "D:\Anaconda3\lib\site-packages\torch\utils\data\dataloader.py", line 560, in __init__
    w.start()
  File "D:\Anaconda3\lib\multiprocessing\process.py", line 105, in start
    self._popen = self._Popen(self)
  File "D:\Anaconda3\lib\multiprocessing\context.py", line 223, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "D:\Anaconda3\lib\multiprocessing\context.py", line 322, in _Popen
    return Popen(process_obj)
  File "D:\Anaconda3\lib\multiprocessing\popen_spawn_win32.py", line 33, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 143, in get_preparation_data
    _check_not_importing_main()
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 136, in _check_not_importing_main
    is not going to be frozen to produce an executable.''')
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.
------------ Options -------------
F: 2401
HDF5FileRoot: e:\Study\Vision\project\Deep-MIML-Network\dataset
K: 4
L: 15
batchSize: 256
checkpoints_dir: checkpoints
continue_train: True
dataset: musicInstruments
decay_factor: 0.94
display_freq: 10
epoch_count: 0
fc_dimension: 1024
gpu_ids: []
init_type: normal
isTrain: True
learning_rate: 0.001
learning_rate_decrease_itr: 5
measure_time: False
mode: train
model: MIML
multi_label_threshold: 0.3
nThreads: 1
name: deepMIML
niter: 300
norm: max
num_of_bases: 25
num_of_fc: 1
save_epoch_freq: 5
save_latest_freq: 500
selected_classes: True
using_multi_labels: True
validation_batches: 10
validation_freq: 50
validation_on: False
with_batchnorm: True
with_softmax: False
zeroCenterInput: False
-------------- End ----------------
CustomDatasetDataLoader
dataset [MIMLDataset] was created
MIML dataset initialize suceed
(15,)
(15,)
#training images = 15
MIML
initialization method [normal]
model [MIMLModel] was created
starting from epoch  0
D:\Anaconda3\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
E:\Study\Vision\project\Deep-MIML-Network\models\networks.py:13: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.
  init.normal(m.weight.data, 0.0, 0.02)
E:\Study\Vision\project\Deep-MIML-Network\models\networks.py:17: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.
  init.normal(m.weight.data, 1.0, 0.02)
E:\Study\Vision\project\Deep-MIML-Network\models\networks.py:18: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.
  init.constant(m.bias.data, 0.0)
Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 114, in _main
    prepare(preparation_data)
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 225, in prepare
    _fixup_main_from_path(data['init_main_from_path'])
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 277, in _fixup_main_from_path
    run_name="__mp_main__")
  File "D:\Anaconda3\lib\runpy.py", line 263, in run_path
    pkg_name=pkg_name, script_name=fname)
  File "D:\Anaconda3\lib\runpy.py", line 96, in _run_module_code
    mod_name, mod_spec, pkg_name, script_name)
  File "D:\Anaconda3\lib\runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "E:\Study\Vision\project\Deep-MIML-Network\train.py", line 65, in <module>
    for i, data in enumerate(dataset):
  File "E:\Study\Vision\project\Deep-MIML-Network\data\custom_dataset_data_loader.py", line 43, in __iter__
    for i, data in enumerate(self.dataloader):
  File "D:\Anaconda3\lib\site-packages\torch\utils\data\dataloader.py", line 819, in __iter__
    return _DataLoaderIter(self)
  File "D:\Anaconda3\lib\site-packages\torch\utils\data\dataloader.py", line 560, in __init__
    w.start()
  File "D:\Anaconda3\lib\multiprocessing\process.py", line 105, in start
    self._popen = self._Popen(self)
  File "D:\Anaconda3\lib\multiprocessing\context.py", line 223, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "D:\Anaconda3\lib\multiprocessing\context.py", line 322, in _Popen
    return Popen(process_obj)
  File "D:\Anaconda3\lib\multiprocessing\popen_spawn_win32.py", line 33, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 143, in get_preparation_data
    _check_not_importing_main()
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 136, in _check_not_importing_main
    is not going to be frozen to produce an executable.''')
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.
------------ Options -------------
F: 2401
HDF5FileRoot: e:\Study\Vision\project\Deep-MIML-Network\dataset
K: 4
L: 15
batchSize: 256
checkpoints_dir: checkpoints
continue_train: True
dataset: musicInstruments
decay_factor: 0.94
display_freq: 10
epoch_count: 0
fc_dimension: 1024
gpu_ids: []
init_type: normal
isTrain: True
learning_rate: 0.001
learning_rate_decrease_itr: 5
measure_time: False
mode: train
model: MIML
multi_label_threshold: 0.3
nThreads: 1
name: deepMIML
niter: 300
norm: max
num_of_bases: 25
num_of_fc: 1
save_epoch_freq: 5
save_latest_freq: 500
selected_classes: True
using_multi_labels: True
validation_batches: 10
validation_freq: 50
validation_on: True
with_batchnorm: True
with_softmax: False
zeroCenterInput: False
-------------- End ----------------
CustomDatasetDataLoader
dataset [MIMLDataset] was created
MIML dataset initialize suceed
(15,)
(15,)
#training images = 15
CustomDatasetDataLoader
dataset [MIMLDataset] was created
MIML dataset initialize suceed
(7,)
(7,)
#validation images = 7
MIML
initialization method [normal]
model [MIMLModel] was created
starting from epoch  0
D:\Anaconda3\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
E:\Study\Vision\project\Deep-MIML-Network\models\networks.py:13: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.
  init.normal(m.weight.data, 0.0, 0.02)
E:\Study\Vision\project\Deep-MIML-Network\models\networks.py:17: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.
  init.normal(m.weight.data, 1.0, 0.02)
E:\Study\Vision\project\Deep-MIML-Network\models\networks.py:18: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.
  init.constant(m.bias.data, 0.0)
Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 114, in _main
    prepare(preparation_data)
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 225, in prepare
    _fixup_main_from_path(data['init_main_from_path'])
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 277, in _fixup_main_from_path
    run_name="__mp_main__")
  File "D:\Anaconda3\lib\runpy.py", line 263, in run_path
    pkg_name=pkg_name, script_name=fname)
  File "D:\Anaconda3\lib\runpy.py", line 96, in _run_module_code
    mod_name, mod_spec, pkg_name, script_name)
  File "D:\Anaconda3\lib\runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "E:\Study\Vision\project\Deep-MIML-Network\train.py", line 65, in <module>
    for i, data in enumerate(dataset):
  File "E:\Study\Vision\project\Deep-MIML-Network\data\custom_dataset_data_loader.py", line 43, in __iter__
    for i, data in enumerate(self.dataloader):
  File "D:\Anaconda3\lib\site-packages\torch\utils\data\dataloader.py", line 819, in __iter__
    return _DataLoaderIter(self)
  File "D:\Anaconda3\lib\site-packages\torch\utils\data\dataloader.py", line 560, in __init__
    w.start()
  File "D:\Anaconda3\lib\multiprocessing\process.py", line 105, in start
    self._popen = self._Popen(self)
  File "D:\Anaconda3\lib\multiprocessing\context.py", line 223, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "D:\Anaconda3\lib\multiprocessing\context.py", line 322, in _Popen
    return Popen(process_obj)
  File "D:\Anaconda3\lib\multiprocessing\popen_spawn_win32.py", line 33, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 143, in get_preparation_data
    _check_not_importing_main()
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 136, in _check_not_importing_main
    is not going to be frozen to produce an executable.''')
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.
------------ Options -------------
F: 2401
HDF5FileRoot: e:\Study\Vision\project\Deep-MIML-Network\dataset
K: 4
L: 15
batchSize: 256
checkpoints_dir: checkpoints
continue_train: True
dataset: musicInstruments
decay_factor: 0.94
display_freq: 10
epoch_count: 0
fc_dimension: 1024
gpu_ids: []
init_type: normal
isTrain: True
learning_rate: 0.001
learning_rate_decrease_itr: 5
measure_time: False
mode: train
model: MIML
multi_label_threshold: 0.3
nThreads: 1
name: deepMIML
niter: 300
norm: max
num_of_bases: 25
num_of_fc: 1
save_epoch_freq: 5
save_latest_freq: 500
selected_classes: True
using_multi_labels: True
validation_batches: 10
validation_freq: 50
validation_on: True
with_batchnorm: True
with_softmax: False
zeroCenterInput: False
-------------- End ----------------
CustomDatasetDataLoader
dataset [MIMLDataset] was created
MIML dataset initialize suceed
(15,)
(15,)
#training images = 15
CustomDatasetDataLoader
dataset [MIMLDataset] was created
MIML dataset initialize suceed
(7,)
(7,)
#validation images = 7
MIML
initialization method [normal]
model [MIMLModel] was created
starting from epoch  0
D:\Anaconda3\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
E:\Study\Vision\project\Deep-MIML-Network\models\networks.py:13: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.
  init.normal(m.weight.data, 0.0, 0.02)
E:\Study\Vision\project\Deep-MIML-Network\models\networks.py:17: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.
  init.normal(m.weight.data, 1.0, 0.02)
E:\Study\Vision\project\Deep-MIML-Network\models\networks.py:18: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.
  init.constant(m.bias.data, 0.0)
Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 114, in _main
    prepare(preparation_data)
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 225, in prepare
    _fixup_main_from_path(data['init_main_from_path'])
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 277, in _fixup_main_from_path
    run_name="__mp_main__")
  File "D:\Anaconda3\lib\runpy.py", line 263, in run_path
    pkg_name=pkg_name, script_name=fname)
  File "D:\Anaconda3\lib\runpy.py", line 96, in _run_module_code
    mod_name, mod_spec, pkg_name, script_name)
  File "D:\Anaconda3\lib\runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "E:\Study\Vision\project\Deep-MIML-Network\train.py", line 65, in <module>
    for i, data in enumerate(dataset):
  File "E:\Study\Vision\project\Deep-MIML-Network\data\custom_dataset_data_loader.py", line 43, in __iter__
    for i, data in enumerate(self.dataloader):
  File "D:\Anaconda3\lib\site-packages\torch\utils\data\dataloader.py", line 819, in __iter__
    return _DataLoaderIter(self)
  File "D:\Anaconda3\lib\site-packages\torch\utils\data\dataloader.py", line 560, in __init__
    w.start()
  File "D:\Anaconda3\lib\multiprocessing\process.py", line 105, in start
    self._popen = self._Popen(self)
  File "D:\Anaconda3\lib\multiprocessing\context.py", line 223, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "D:\Anaconda3\lib\multiprocessing\context.py", line 322, in _Popen
    return Popen(process_obj)
  File "D:\Anaconda3\lib\multiprocessing\popen_spawn_win32.py", line 33, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 143, in get_preparation_data
    _check_not_importing_main()
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 136, in _check_not_importing_main
    is not going to be frozen to produce an executable.''')
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.
------------ Options -------------
F: 2401
HDF5FileRoot: e:\Study\Vision\project\Deep-MIML-Network\dataset
K: 4
L: 15
batchSize: 256
checkpoints_dir: checkpoints
continue_train: True
dataset: musicInstruments
decay_factor: 0.94
display_freq: 10
epoch_count: 0
fc_dimension: 1024
gpu_ids: []
init_type: normal
isTrain: True
learning_rate: 0.001
learning_rate_decrease_itr: 5
measure_time: False
mode: train
model: MIML
multi_label_threshold: 0.3
nThreads: 1
name: deepMIML
niter: 300
norm: max
num_of_bases: 25
num_of_fc: 1
save_epoch_freq: 5
save_latest_freq: 500
selected_classes: True
using_multi_labels: True
validation_batches: 10
validation_freq: 50
validation_on: True
with_batchnorm: True
with_softmax: False
zeroCenterInput: False
-------------- End ----------------
CustomDatasetDataLoader
dataset [MIMLDataset] was created
MIML dataset initialize suceed
(15,)
(15,)
#training images = 15
CustomDatasetDataLoader
dataset [MIMLDataset] was created
MIML dataset initialize suceed
(7,)
(7,)
#validation images = 7
MIML
initialization method [normal]
model [MIMLModel] was created
starting from epoch  0
D:\Anaconda3\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
E:\Study\Vision\project\Deep-MIML-Network\models\networks.py:13: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.
  init.normal(m.weight.data, 0.0, 0.02)
E:\Study\Vision\project\Deep-MIML-Network\models\networks.py:17: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.
  init.normal(m.weight.data, 1.0, 0.02)
E:\Study\Vision\project\Deep-MIML-Network\models\networks.py:18: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.
  init.constant(m.bias.data, 0.0)
Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 114, in _main
    prepare(preparation_data)
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 225, in prepare
    _fixup_main_from_path(data['init_main_from_path'])
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 277, in _fixup_main_from_path
    run_name="__mp_main__")
  File "D:\Anaconda3\lib\runpy.py", line 263, in run_path
    pkg_name=pkg_name, script_name=fname)
  File "D:\Anaconda3\lib\runpy.py", line 96, in _run_module_code
    mod_name, mod_spec, pkg_name, script_name)
  File "D:\Anaconda3\lib\runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "E:\Study\Vision\project\Deep-MIML-Network\train.py", line 65, in <module>
    for i, data in enumerate(dataset):
  File "E:\Study\Vision\project\Deep-MIML-Network\data\custom_dataset_data_loader.py", line 43, in __iter__
    for i, data in enumerate(self.dataloader):
  File "D:\Anaconda3\lib\site-packages\torch\utils\data\dataloader.py", line 819, in __iter__
    return _DataLoaderIter(self)
  File "D:\Anaconda3\lib\site-packages\torch\utils\data\dataloader.py", line 560, in __init__
    w.start()
  File "D:\Anaconda3\lib\multiprocessing\process.py", line 105, in start
    self._popen = self._Popen(self)
  File "D:\Anaconda3\lib\multiprocessing\context.py", line 223, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "D:\Anaconda3\lib\multiprocessing\context.py", line 322, in _Popen
    return Popen(process_obj)
  File "D:\Anaconda3\lib\multiprocessing\popen_spawn_win32.py", line 33, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 143, in get_preparation_data
    _check_not_importing_main()
  File "D:\Anaconda3\lib\multiprocessing\spawn.py", line 136, in _check_not_importing_main
    is not going to be frozen to produce an executable.''')
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.
------------ Options -------------
F: 2401
HDF5FileRoot: e:\Study\Vision\project\Deep-MIML-Network\dataset
K: 4
L: 15
batchSize: 256
checkpoints_dir: checkpoints
continue_train: True
dataset: musicInstruments
decay_factor: 0.94
display_freq: 10
epoch_count: 0
fc_dimension: 1024
gpu_ids: []
init_type: normal
isTrain: True
learning_rate: 0.001
learning_rate_decrease_itr: 5
measure_time: False
mode: train
model: MIML
multi_label_threshold: 0.3
nThreads: 1
name: deepMIML
niter: 300
norm: max
num_of_bases: 25
num_of_fc: 1
save_epoch_freq: 5
save_latest_freq: 500
selected_classes: True
using_multi_labels: True
validation_batches: 10
validation_freq: 50
validation_on: True
with_batchnorm: True
with_softmax: False
zeroCenterInput: False
-------------- End ----------------
CustomDatasetDataLoader
dataset [MIMLDataset] was created
MIML dataset initialize suceed
(15,)
(15,)
#training images = 15
CustomDatasetDataLoader
dataset [MIMLDataset] was created
MIML dataset initialize suceed
(7,)
(7,)
#validation images = 7
MIML
initialization method [normal]
model [MIMLModel] was created
starting from epoch  0
D:\Anaconda3\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
------------ Options -------------
F: 2401
HDF5FileRoot: e:\Study\Vision\project\Deep-MIML-Network\dataset
K: 4
L: 15
batchSize: 256
checkpoints_dir: checkpoints
continue_train: True
dataset: musicInstruments
decay_factor: 0.94
display_freq: 10
epoch_count: 0
fc_dimension: 1024
gpu_ids: []
init_type: normal
isTrain: True
learning_rate: 0.001
learning_rate_decrease_itr: 5
measure_time: False
mode: train
model: MIML
multi_label_threshold: 0.3
nThreads: 0
name: deepMIML
niter: 300
norm: max
num_of_bases: 25
num_of_fc: 1
save_epoch_freq: 5
save_latest_freq: 500
selected_classes: True
using_multi_labels: True
validation_batches: 10
validation_freq: 50
validation_on: True
with_batchnorm: True
with_softmax: False
zeroCenterInput: False
-------------- End ----------------
CustomDatasetDataLoader
dataset [MIMLDataset] was created
MIML dataset initialize suceed
(15,)
(15,)
#training images = 15
CustomDatasetDataLoader
dataset [MIMLDataset] was created
MIML dataset initialize suceed
(7,)
(7,)
#validation images = 7
MIML
initialization method [normal]
model [MIMLModel] was created
starting from epoch  0
Traceback (most recent call last):
  File "train.py", line 65, in <module>
    for i, data in enumerate(dataset):
  File "E:\Study\Vision\project\Deep-MIML-Network\data\custom_dataset_data_loader.py", line 43, in __iter__
    for i, data in enumerate(self.dataloader):
  File "D:\Anaconda3\lib\site-packages\torch\utils\data\dataloader.py", line 615, in __next__
    batch = self.collate_fn([self.dataset[i] for i in indices])
  File "D:\Anaconda3\lib\site-packages\torch\utils\data\dataloader.py", line 615, in <listcomp>
    batch = self.collate_fn([self.dataset[i] for i in indices])
  File "E:\Study\Vision\project\Deep-MIML-Network\data\MIML_dataset.py", line 100, in __getitem__
    loaded_label = softmax(subsetOfClasses(np.load(self.labels[index].decode("utf-8"))))
AttributeError: 'numpy.int32' object has no attribute 'decode'
D:\Anaconda3\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
------------ Options -------------
F: 2401
HDF5FileRoot: e:\Study\Vision\project\Deep-MIML-Network\dataset
K: 4
L: 15
batchSize: 256
checkpoints_dir: checkpoints
continue_train: True
dataset: musicInstruments
decay_factor: 0.94
display_freq: 10
epoch_count: 0
fc_dimension: 1024
gpu_ids: []
init_type: normal
isTrain: True
learning_rate: 0.001
learning_rate_decrease_itr: 5
measure_time: False
mode: train
model: MIML
multi_label_threshold: 0.3
nThreads: 0
name: deepMIML
niter: 300
norm: max
num_of_bases: 25
num_of_fc: 1
save_epoch_freq: 5
save_latest_freq: 500
selected_classes: True
using_multi_labels: True
validation_batches: 10
validation_freq: 50
validation_on: True
with_batchnorm: True
with_softmax: False
zeroCenterInput: False
-------------- End ----------------
CustomDatasetDataLoader
dataset [MIMLDataset] was created
MIML dataset initialize suceed
(15,)
(15,)
#training images = 15
CustomDatasetDataLoader
dataset [MIMLDataset] was created
MIML dataset initialize suceed
(7,)
(7,)
#validation images = 7
MIML
initialization method [normal]
model [MIMLModel] was created
starting from epoch  0
Traceback (most recent call last):
  File "train.py", line 65, in <module>
    for i, data in enumerate(dataset):
  File "E:\Study\Vision\project\Deep-MIML-Network\data\custom_dataset_data_loader.py", line 43, in __iter__
    for i, data in enumerate(self.dataloader):
  File "D:\Anaconda3\lib\site-packages\torch\utils\data\dataloader.py", line 615, in __next__
    batch = self.collate_fn([self.dataset[i] for i in indices])
  File "D:\Anaconda3\lib\site-packages\torch\utils\data\dataloader.py", line 615, in <listcomp>
    batch = self.collate_fn([self.dataset[i] for i in indices])
  File "E:\Study\Vision\project\Deep-MIML-Network\data\MIML_dataset.py", line 100, in __getitem__
    loaded_label = softmax(subsetOfClasses(np.load(self.labels[index].decode("utf-8"))))
  File "D:\Anaconda3\lib\site-packages\numpy\lib\npyio.py", line 372, in load
    fid = open(file, "rb")
FileNotFoundError: [Errno 2] No such file or directory: '401'
D:\Anaconda3\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
------------ Options -------------
F: 2401
HDF5FileRoot: e:\Study\Vision\project\Deep-MIML-Network\dataset
K: 4
L: 15
batchSize: 256
checkpoints_dir: checkpoints
continue_train: True
dataset: musicInstruments
decay_factor: 0.94
display_freq: 10
epoch_count: 0
fc_dimension: 1024
gpu_ids: []
init_type: normal
isTrain: True
learning_rate: 0.001
learning_rate_decrease_itr: 5
measure_time: False
mode: train
model: MIML
multi_label_threshold: 0.3
nThreads: 0
name: deepMIML
niter: 300
norm: max
num_of_bases: 25
num_of_fc: 1
save_epoch_freq: 5
save_latest_freq: 500
selected_classes: True
using_multi_labels: True
validation_batches: 10
validation_freq: 50
validation_on: True
with_batchnorm: True
with_softmax: False
zeroCenterInput: False
-------------- End ----------------
CustomDatasetDataLoader
dataset [MIMLDataset] was created
MIML dataset initialize suceed
(15,)
(15,)
#training images = 15
CustomDatasetDataLoader
dataset [MIMLDataset] was created
MIML dataset initialize suceed
(7,)
(7,)
#validation images = 7
MIML
initialization method [normal]
model [MIMLModel] was created
starting from epoch  0
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
Traceback (most recent call last):
  File "train.py", line 76, in <module>
    model.optimize_parameters()
  File "E:\Study\Vision\project\Deep-MIML-Network\models\MIML.py", line 213, in optimize_parameters
    self.backward()
  File "E:\Study\Vision\project\Deep-MIML-Network\models\MIML.py", line 139, in backward
    self.batch_loss.append(loss.data[0]) 
IndexError: invalid index of a 0-dim tensor. Use tensor.item() to convert a 0-dim tensor to a Python number
------------ Options -------------
F: 2401
HDF5FileRoot: e:\Study\Vision\project\Deep-MIML-Network\dataset
K: 4
L: 15
batchSize: 256
checkpoints_dir: checkpoints
continue_train: True
dataset: musicInstruments
decay_factor: 0.94
display_freq: 10
epoch_count: 0
fc_dimension: 1024
gpu_ids: []
init_type: normal
isTrain: True
learning_rate: 0.001
learning_rate_decrease_itr: 5
measure_time: False
mode: train
model: MIML
multi_label_threshold: 0.3
nThreads: 0
name: deepMIML
niter: 300
norm: max
num_of_bases: 25
num_of_fc: 1
save_epoch_freq: 5
save_latest_freq: 500
selected_classes: True
using_multi_labels: True
validation_batches: 10
validation_freq: 50
validation_on: True
with_batchnorm: True
with_softmax: False
zeroCenterInput: False
-------------- End ----------------
CustomDatasetDataLoader
dataset [MIMLDataset] was created
MIML dataset initialize suceed
(15,)
(15,)
#training images = 15
CustomDatasetDataLoader
dataset [MIMLDataset] was created
MIML dataset initialize suceed
(7,)
(7,)
#validation images = 7
MIML
initialization method [normal]
model [MIMLModel] was created
starting from epoch  0
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
saving the model at the end of epoch 5, total_steps 1280
current learning rate: 0.00094
decreased learning rate by  0.94
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
Display training progress at (epoch 10, total_steps 2560)
loss: 0.9058656394481659
mAP: 0.1413333333333333
end of display 

saving the model at the end of epoch 10, total_steps 2560
current learning rate: 0.0008836
decreased learning rate by  0.94
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
saving the model at the end of epoch 15, total_steps 3840
current learning rate: 0.0008305839999999999
decreased learning rate by  0.94
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
Display training progress at (epoch 20, total_steps 5120)
loss: 0.8226962924003601
mAP: 0.18488888888888896
end of display 

saving the model at the end of epoch 20, total_steps 5120
current learning rate: 0.0007807489599999998
decreased learning rate by  0.94
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
saving the model at the end of epoch 25, total_steps 6400
current learning rate: 0.0007339040223999998
decreased learning rate by  0.94
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
Display training progress at (epoch 30, total_steps 7680)
loss: 0.8150656282901764
mAP: 0.1911111111111112
end of display 

saving the model at the end of epoch 30, total_steps 7680
current learning rate: 0.0006898697810559998
decreased learning rate by  0.94
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
saving the model at the end of epoch 35, total_steps 8960
current learning rate: 0.0006484775941926397
decreased learning rate by  0.94
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
Display training progress at (epoch 40, total_steps 10240)
loss: 0.81213858127594
mAP: 0.1911111111111111
end of display 

saving the model at the end of epoch 40, total_steps 10240
current learning rate: 0.0006095689385410813
decreased learning rate by  0.94
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
saving the model at the end of epoch 45, total_steps 11520
current learning rate: 0.0005729948022286164
decreased learning rate by  0.94
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.03608665 0.03608661 0.03608645 0.03608642 0.03608641 0.03608632
 0.03614472 0.03608667 0.49472854 0.03608735 0.0360863  0.03608657
 0.03608643 0.03608797 0.0360866 ]
[0.06561828 0.06533993 0.06529471 0.06585314 0.06523466 0.06517247
 0.0652717  0.06513066 0.06531232 0.06587175 0.06543563 0.06524995
 0.06561531 0.06514152 0.08445796]
[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.05535819 0.05528427 0.05530684 0.05525797 0.05568332 0.05529468
 0.05965132 0.06205788 0.05609375 0.05529477 0.0553161  0.05525215
 0.05828707 0.21054567 0.055316  ]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.02933841 0.02933841 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.58926224 0.02933841 0.02933841 0.02933841 0.02933841
 0.02933841 0.02933848 0.02933841]
[0.05801456 0.0578337  0.05783235 0.05807303 0.05782411 0.05782837
 0.05783179 0.05785142 0.05783659 0.05791555 0.05784721 0.18911204
 0.0582593  0.05784937 0.0580906 ]
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]
[0.03332005 0.03332212 0.03331744 0.0333168  0.03332714 0.03332873
 0.03332608 0.03337453 0.03331665 0.03332241 0.03331904 0.03331624
 0.53170452 0.03507181 0.03331642]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
[0.02933796 0.02933796 0.02933796 0.02933796 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796 0.58926861 0.02933796 0.02933796
 0.02933796 0.02933796 0.02933796]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
Display training progress at (epoch 50, total_steps 12800)
loss: 0.8106990873813629
mAP: 0.19111111111111115
end of display 

Display validation results at (epoch 50, total_steps 12800)
[0.58884392 0.02936734 0.02936749 0.02936723 0.02936751 0.02936938
 0.02936969 0.02936726 0.02936769 0.02936763 0.02936974 0.029368
 0.02936854 0.02936919 0.02936937]D:\Anaconda3\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
E:\Study\Vision\project\Deep-MIML-Network\models\MIML.py:68: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  basesnet_output = self.BasesNet(Variable(self.bases, requires_grad=False, volatile=volatile)).view(-1, self.opt.L, self.opt.K, self.opt.num_of_bases)

[0.02936291 0.0293677  0.58890522 0.02936187 0.02937104 0.0293742
 0.02936185 0.02936186 0.02936198 0.02936196 0.02936188 0.0293618
 0.02936187 0.02936196 0.02936189]
[0.05215463 0.05174588 0.05184741 0.05167479 0.2744672  0.05189723
 0.05167826 0.05167422 0.05169602 0.05164912 0.05259415 0.05173999
 0.05171766 0.05178716 0.05167628]
[0.0396543  0.42655656 0.04030313 0.03963234 0.03958612 0.05316231
 0.03959617 0.03959749 0.04409782 0.03961323 0.03958588 0.03958372
 0.03972982 0.03959637 0.03970472]
[0.06294768 0.09323083 0.06597184 0.08893726 0.06888398 0.07121911
 0.05935879 0.05896712 0.0632305  0.06918319 0.05943605 0.05916935
 0.05925156 0.05911854 0.06109418]
[0.02961681 0.02997251 0.02965507 0.02961633 0.02961632 0.58497427
 0.02961632 0.02961632 0.0296166  0.02961638 0.02961635 0.02961631
 0.02961691 0.02961633 0.02961717]
[0.05396237 0.05395795 0.05396637 0.05396022 0.05395317 0.05395564
 0.244232   0.05408576 0.05406871 0.05396045 0.05395319 0.05395185
 0.0539748  0.0540613  0.05395621]
Traceback (most recent call last):
  File "train.py", line 103, in <module>
    model.display_val(writer, total_steps, dataset_val)
  File "E:\Study\Vision\project\Deep-MIML-Network\models\MIML.py", line 166, in display_val
    ap, loss = self.test_multi_label(val_data)
  File "E:\Study\Vision\project\Deep-MIML-Network\models\MIML.py", line 208, in test_multi_label
    loss = self.loss(self.output, label).data.cpu().numpy()[0]
IndexError: too many indices for array
D:\Anaconda3\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
------------ Options -------------
F: 2401
HDF5FileRoot: e:\Study\Vision\project\Deep-MIML-Network\dataset
K: 4
L: 15
batchSize: 256
checkpoints_dir: checkpoints
continue_train: True
dataset: musicInstruments
decay_factor: 0.94
display_freq: 10
epoch_count: 0
fc_dimension: 1024
gpu_ids: []
init_type: normal
isTrain: True
learning_rate: 0.001
learning_rate_decrease_itr: 5
measure_time: False
mode: train
model: MIML
multi_label_threshold: 0.3
nThreads: 0
name: deepMIML
niter: 300
norm: max
num_of_bases: 25
num_of_fc: 1
save_epoch_freq: 5
save_latest_freq: 500
selected_classes: True
using_multi_labels: True
validation_batches: 10
validation_freq: 50
validation_on: True
with_batchnorm: True
with_softmax: False
zeroCenterInput: False
-------------- End ----------------
CustomDatasetDataLoader
dataset [MIMLDataset] was created
MIML dataset initialize suceed
(15,)
(15,)
#training images = 15
CustomDatasetDataLoader
dataset [MIMLDataset] was created
MIML dataset initialize suceed
(7,)
(7,)
#validation images = 7
MIML
initialization method [normal]
model [MIMLModel] was created
starting from epoch  45
current learning rate: 0.0005729948022286164
Display training progress at (epoch 50, total_steps 12800)
loss: 0.8106991112232208
mAP: 0.19111111111111118
end of display 

Display validation results at (epoch 50, total_steps 12800)
tensor([[3.9202, 4.9415, 3.9451, 1.9999, 3.8855, 3.9120, 3.9101, 3.9369, 4.0258,
         3.9421, 3.9322, 3.9430, 3.9518, 3.9231, 3.9765],
        [3.9202, 4.9415, 3.9451, 1.9999, 3.8855, 3.9120, 3.9101, 3.9369, 4.0258,
         3.9421, 3.9322, 3.9430, 3.9518, 3.9231, 3.9765],
        [3.9202, 4.9415, 3.9451, 1.9999, 3.8855, 3.9120, 3.9101, 3.9369, 4.0258,
         3.9421, 3.9322, 3.9430, 3.9518, 3.9231, 3.9765],
        [3.9202, 4.9415, 3.9451, 1.9999, 3.8855, 3.9120, 3.9101, 3.9369, 4.0258,
         3.9421, 3.9322, 3.9430, 3.9518, 3.9231, 3.9765],
        [3.9202, 4.9415, 3.9451, 1.9999, 3.8855, 3.9120, 3.9101, 3.9369, 4.0258,
         3.9421, 3.9322, 3.9430, 3.9518, 3.9231, 3.9765],
        [3.9202, 4.9415, 3.9451, 1.9999, 3.8855, 3.9120, 3.9101, 3.9369, 4.0258,
         3.9421, 3.9322, 3.9430, 3.9518, 3.9231, 3.9765],
        [3.9202, 4.9415, 3.9451, 1.9999, 3.8855, 3.9120, 3.9101, 3.9369, 4.0258,
         3.9421, 3.9322, 3.9430, 3.9518, 3.9231, 3.9765]],
       grad_fn=<ViewBackward>)
torch.Size([7, 15])
tensor([[ 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],
        [ 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],
        [ 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],
        [ 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],
        [ 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],
        [ 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],
        [ 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]])
torch.Size([7, 15])
Traceback (most recent call last):
  File "train.py", line 103, in <module>
    model.display_val(writer, total_steps, dataset_val)
  File "E:\Study\Vision\project\Deep-MIML-Network\models\MIML.py", line 167, in display_val
    ap, loss = self.test_multi_label(val_data)
  File "E:\Study\Vision\project\Deep-MIML-Network\models\MIML.py", line 213, in test_multi_label
    loss = self.loss(self.output, label).data.cpu().numpy()[0]
IndexError: too many indices for array
D:\Anaconda3\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
------------ Options -------------
F: 2401
HDF5FileRoot: e:\Study\Vision\project\Deep-MIML-Network\dataset
K: 4
L: 15
batchSize: 256
checkpoints_dir: checkpoints
continue_train: True
dataset: musicInstruments
decay_factor: 0.94
display_freq: 10
epoch_count: 0
fc_dimension: 1024
gpu_ids: []
init_type: normal
isTrain: True
learning_rate: 0.001
learning_rate_decrease_itr: 5
measure_time: False
mode: train
model: MIML
multi_label_threshold: 0.3
nThreads: 0
name: deepMIML
niter: 300
norm: max
num_of_bases: 25
num_of_fc: 1
save_epoch_freq: 5
save_latest_freq: 500
selected_classes: True
using_multi_labels: True
validation_batches: 10
validation_freq: 50
validation_on: True
with_batchnorm: True
with_softmax: False
zeroCenterInput: False
-------------- End ----------------
CustomDatasetDataLoader
dataset [MIMLDataset] was created
MIML dataset initialize suceed
(15,)
(15,)
#training images = 15
CustomDatasetDataLoader
dataset [MIMLDataset] was created
MIML dataset initialize suceed
(7,)
(7,)
#validation images = 7
MIML
initialization method [normal]
model [MIMLModel] was created
starting from epoch  45
current learning rate: 0.0005729948022286164
Display training progress at (epoch 50, total_steps 12800)
loss: 0.810699075460434
mAP: 0.19111111111111118
end of display 

Display validation results at (epoch 50, total_steps 12800)
tensor([[3.9202, 4.9415, 3.9451, 1.9999, 3.8855, 3.9120, 3.9101, 3.9369, 4.0258,
         3.9421, 3.9322, 3.9430, 3.9518, 3.9231, 3.9765],
        [3.9202, 4.9415, 3.9451, 1.9999, 3.8855, 3.9120, 3.9101, 3.9369, 4.0258,
         3.9421, 3.9322, 3.9430, 3.9518, 3.9231, 3.9765],
        [3.9202, 4.9415, 3.9451, 1.9999, 3.8855, 3.9120, 3.9101, 3.9369, 4.0258,
         3.9421, 3.9322, 3.9430, 3.9518, 3.9231, 3.9765],
        [3.9202, 4.9415, 3.9451, 1.9999, 3.8855, 3.9120, 3.9101, 3.9369, 4.0258,
         3.9421, 3.9322, 3.9430, 3.9518, 3.9231, 3.9765],
        [3.9202, 4.9415, 3.9451, 1.9999, 3.8855, 3.9120, 3.9101, 3.9369, 4.0258,
         3.9421, 3.9322, 3.9430, 3.9518, 3.9231, 3.9765],
        [3.9202, 4.9415, 3.9451, 1.9999, 3.8855, 3.9120, 3.9101, 3.9369, 4.0258,
         3.9421, 3.9322, 3.9430, 3.9518, 3.9231, 3.9765],
        [3.9202, 4.9415, 3.9451, 1.9999, 3.8855, 3.9120, 3.9101, 3.9369, 4.0258,
         3.9421, 3.9322, 3.9430, 3.9518, 3.9231, 3.9765]],
       grad_fn=<ViewBackward>)
torch.Size([7, 15])
tensor([[ 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],
        [ 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],
        [ 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],
        [ 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],
        [ 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],
        [ 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],
        [ 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]])
torch.Size([7, 15])
tensor(0.6855, grad_fn=<MultilabelMarginLossBackward>)
torch.Size([])
tensor(0.6855)
torch.Size([])
tensor(0.6855)
torch.Size([])
0.68552285
()
Traceback (most recent call last):
  File "train.py", line 103, in <module>
    model.display_val(writer, total_steps, dataset_val)
  File "E:\Study\Vision\project\Deep-MIML-Network\models\MIML.py", line 167, in display_val
    ap, loss = self.test_multi_label(val_data)
  File "E:\Study\Vision\project\Deep-MIML-Network\models\MIML.py", line 221, in test_multi_label
    loss = self.loss(self.output, label).data.cpu().numpy()[0]
IndexError: too many indices for array
D:\Anaconda3\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
------------ Options -------------
F: 2401
HDF5FileRoot: e:\Study\Vision\project\Deep-MIML-Network\dataset
K: 4
L: 15
batchSize: 256
checkpoints_dir: checkpoints
continue_train: True
dataset: musicInstruments
decay_factor: 0.94
display_freq: 10
epoch_count: 0
fc_dimension: 1024
gpu_ids: []
init_type: normal
isTrain: True
learning_rate: 0.001
learning_rate_decrease_itr: 5
measure_time: False
mode: train
model: MIML
multi_label_threshold: 0.3
nThreads: 0
name: deepMIML
niter: 300
norm: max
num_of_bases: 25
num_of_fc: 1
save_epoch_freq: 5
save_latest_freq: 500
selected_classes: True
using_multi_labels: True
validation_batches: 10
validation_freq: 50
validation_on: True
with_batchnorm: True
with_softmax: False
zeroCenterInput: False
-------------- End ----------------
CustomDatasetDataLoader
dataset [MIMLDataset] was created
MIML dataset initialize suceed
(15,)
(15,)
#training images = 15
CustomDatasetDataLoader
dataset [MIMLDataset] was created
MIML dataset initialize suceed
(7,)
(7,)
#validation images = 7
MIML
initialization method [normal]
model [MIMLModel] was created
starting from epoch  45
current learning rate: 0.0005729948022286164
Display training progress at (epoch 50, total_steps 12800)
loss: 0.810699075460434
mAP: 0.19111111111111118
end of display 

Display validation results at (epoch 50, total_steps 12800)
tensor([[3.9201, 4.9415, 3.9451, 1.9999, 3.8855, 3.9120, 3.9101, 3.9369, 4.0258,
         3.9421, 3.9322, 3.9430, 3.9518, 3.9231, 3.9765],
        [3.9201, 4.9415, 3.9451, 1.9999, 3.8855, 3.9120, 3.9101, 3.9369, 4.0258,
         3.9421, 3.9322, 3.9430, 3.9518, 3.9231, 3.9765],
        [3.9201, 4.9415, 3.9451, 1.9999, 3.8855, 3.9120, 3.9101, 3.9369, 4.0258,
         3.9421, 3.9322, 3.9430, 3.9518, 3.9231, 3.9765],
        [3.9201, 4.9415, 3.9451, 1.9999, 3.8855, 3.9120, 3.9101, 3.9369, 4.0258,
         3.9421, 3.9322, 3.9430, 3.9518, 3.9231, 3.9765],
        [3.9201, 4.9415, 3.9451, 1.9999, 3.8855, 3.9120, 3.9101, 3.9369, 4.0258,
         3.9421, 3.9322, 3.9430, 3.9518, 3.9231, 3.9765],
        [3.9201, 4.9415, 3.9451, 1.9999, 3.8855, 3.9120, 3.9101, 3.9369, 4.0258,
         3.9421, 3.9322, 3.9430, 3.9518, 3.9231, 3.9765],
        [3.9201, 4.9415, 3.9451, 1.9999, 3.8855, 3.9120, 3.9101, 3.9369, 4.0258,
         3.9421, 3.9322, 3.9430, 3.9518, 3.9231, 3.9765]],
       grad_fn=<ViewBackward>)
torch.Size([7, 15])
tensor([[ 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],
        [ 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],
        [ 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],
        [ 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],
        [ 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],
        [ 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],
        [ 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]])
torch.Size([7, 15])
tensor(0.6855, grad_fn=<MultilabelMarginLossBackward>)
torch.Size([])
tensor(0.6855)
torch.Size([])
tensor(0.6855)
torch.Size([])
numpy
0.6855248
()
Traceback (most recent call last):
  File "train.py", line 103, in <module>
    model.display_val(writer, total_steps, dataset_val)
  File "E:\Study\Vision\project\Deep-MIML-Network\models\MIML.py", line 167, in display_val
    ap, loss = self.test_multi_label(val_data)
  File "E:\Study\Vision\project\Deep-MIML-Network\models\MIML.py", line 222, in test_multi_label
    loss = self.loss(self.output, label).data.cpu().numpy()[0]
IndexError: too many indices for array
D:\Anaconda3\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
------------ Options -------------
F: 2401
HDF5FileRoot: e:\Study\Vision\project\Deep-MIML-Network\dataset
K: 4
L: 15
batchSize: 256
checkpoints_dir: checkpoints
continue_train: True
dataset: musicInstruments
decay_factor: 0.94
display_freq: 10
epoch_count: 0
fc_dimension: 1024
gpu_ids: []
init_type: normal
isTrain: True
learning_rate: 0.001
learning_rate_decrease_itr: 5
measure_time: False
mode: train
model: MIML
multi_label_threshold: 0.3
nThreads: 0
name: deepMIML
niter: 300
norm: max
num_of_bases: 25
num_of_fc: 1
save_epoch_freq: 5
save_latest_freq: 500
selected_classes: True
using_multi_labels: True
validation_batches: 10
validation_freq: 50
validation_on: True
with_batchnorm: True
with_softmax: False
zeroCenterInput: False
-------------- End ----------------
CustomDatasetDataLoader
dataset [MIMLDataset] was created
MIML dataset initialize suceed
(15,)
(15,)
#training images = 15
CustomDatasetDataLoader
dataset [MIMLDataset] was created
MIML dataset initialize suceed
(7,)
(7,)
#validation images = 7
MIML
initialization method [normal]
model [MIMLModel] was created
starting from epoch  60
Traceback (most recent call last):
  File "train.py", line 53, in <module>
    model = torch.load(os.path.join('.', opt.checkpoints_dir, opt.name, str(opt.epoch_count) + '.pth'))
  File "D:\Anaconda3\lib\site-packages\torch\serialization.py", line 367, in load
    return _load(f, map_location, pickle_module)
  File "D:\Anaconda3\lib\site-packages\torch\serialization.py", line 545, in _load
    deserialized_objects[key]._set_from_file(f, offset, f_should_read_directly)
RuntimeError: unexpected EOF, expected 8 more bytes. The file might be corrupted.
D:\Anaconda3\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
------------ Options -------------
F: 2401
HDF5FileRoot: e:\Study\Vision\project\Deep-MIML-Network\dataset
K: 4
L: 15
batchSize: 256
checkpoints_dir: checkpoints
continue_train: True
dataset: musicInstruments
decay_factor: 0.94
display_freq: 10
epoch_count: 0
fc_dimension: 1024
gpu_ids: []
init_type: normal
isTrain: True
learning_rate: 0.001
learning_rate_decrease_itr: 5
measure_time: False
mode: train
model: MIML
multi_label_threshold: 0.3
nThreads: 0
name: deepMIML
niter: 300
norm: max
num_of_bases: 25
num_of_fc: 1
save_epoch_freq: 5
save_latest_freq: 500
selected_classes: True
using_multi_labels: True
validation_batches: 10
validation_freq: 50
validation_on: True
with_batchnorm: True
with_softmax: False
zeroCenterInput: False
-------------- End ----------------
CustomDatasetDataLoader
dataset [MIMLDataset] was created
MIML dataset initialize suceed
(15,)
(15,)
#training images = 15
CustomDatasetDataLoader
dataset [MIMLDataset] was created
MIML dataset initialize suceed
(7,)
(7,)
#validation images = 7
MIML
initialization method [normal]
model [MIMLModel] was created
starting from epoch  60
Traceback (most recent call last):
  File "train.py", line 53, in <module>
    model = torch.load(os.path.join('.', opt.checkpoints_dir, opt.name, str(opt.epoch_count) + '.pth'))
  File "D:\Anaconda3\lib\site-packages\torch\serialization.py", line 367, in load
    return _load(f, map_location, pickle_module)
  File "D:\Anaconda3\lib\site-packages\torch\serialization.py", line 545, in _load
    deserialized_objects[key]._set_from_file(f, offset, f_should_read_directly)
RuntimeError: unexpected EOF, expected 8 more bytes. The file might be corrupted.
------------ Options -------------
F: 2401
HDF5FileRoot: e:\Study\Vision\project\Deep-MIML-Network\dataset
K: 4
L: 15
batchSize: 256
checkpoints_dir: checkpoints
continue_train: True
dataset: musicInstruments
decay_factor: 0.94
display_freq: 10
epoch_count: 0
fc_dimension: 1024
gpu_ids: []
init_type: normal
isTrain: True
learning_rate: 0.001
learning_rate_decrease_itr: 5
measure_time: False
mode: train
model: MIML
multi_label_threshold: 0.3
nThreads: 0
name: deepMIML
niter: 300
norm: max
num_of_bases: 25
num_of_fc: 1
save_epoch_freq: 5
save_latest_freq: 500
selected_classes: True
using_multi_labels: True
validation_batches: 10
validation_freq: 50
validation_on: True
with_batchnorm: True
with_softmax: False
zeroCenterInput: False
-------------- End ----------------
CustomDatasetDataLoader
dataset [MIMLDataset] was created
MIML dataset initialize suceed
(15,)
(15,)
#training images = 15
CustomDatasetDataLoader
dataset [MIMLDataset] was created
MIML dataset initialize suceed
(7,)
(7,)
#validation images = 7
MIML
initialization method [normal]
model [MIMLModel] was created
starting from epoch  0
saving the model at the end of epoch 5, total_steps 1280
current learning rate: 0.00094
decreased learning rate by  0.94
Display training progress at (epoch 10, total_steps 2560)
loss: 0.9070307672023773
mAP: 0.1351111111111111
end of display 

saving the model at the end of epoch 10, total_steps 2560
current learning rate: 0.0008836
decreased learning rate by  0.94
saving the model at the end of epoch 15, total_steps 3840
current learning rate: 0.0008305839999999999
decreased learning rate by  0.94
Display training progress at (epoch 20, total_steps 5120)
loss: 0.8234691381454468
mAP: 0.19111111111111118
end of display 

saving the model at the end of epoch 20, total_steps 5120
current learning rate: 0.0007807489599999998
decreased learning rate by  0.94
saving the model at the end of epoch 25, total_steps 6400
current learning rate: 0.0007339040223999998
decreased learning rate by  0.94
Display training progress at (epoch 30, total_steps 7680)
loss: 0.8161062002182007
mAP: 0.19111111111111118
end of display 

saving the model at the end of epoch 30, total_steps 7680
current learning rate: 0.0006898697810559998
decreased learning rate by  0.94
saving the model at the end of epoch 35, total_steps 8960
current learning rate: 0.0006484775941926397
decreased learning rate by  0.94
Display training progress at (epoch 40, total_steps 10240)
loss: 0.8123255670070648
mAP: 0.1911111111111112
end of display 

saving the model at the end of epoch 40, total_steps 10240
current learning rate: 0.0006095689385410813
decreased learning rate by  0.94
saving the model at the end of epoch 45, total_steps 11520
current learning rate: 0.0005729948022286164
decreased learning rate by  0.94
Display training progress at (epoch 50, total_steps 12800)
loss: 0.8111866593360901
mAP: 0.19111111111111118
end of display 

Display validation results at (epoch 50, total_steps 12800)
validation mAP is: 0.3333333333333333
validation loss is: 0.6703134775161743
end of display 

saving the model at the end of epoch 50, total_steps 12800
current learning rate: 0.0005386151140948994
decreased learning rate by  0.94
saving the model at the end of epoch 55, total_steps 14080
current learning rate: 0.0005062982072492054
decreased learning rate by  0.94
Display training progress at (epoch 60, total_steps 15360)
loss: 0.8098443508148193
mAP: 0.19111111111111118
end of display 

saving the model at the end of epoch 60, total_steps 15360
current learning rate: 0.0004759203148142531
decreased learning rate by  0.94
saving the model at the end of epoch 65, total_steps 16640
current learning rate: 0.00044736509592539786
decreased learning rate by  0.94
Display training progress at (epoch 70, total_steps 17920)
loss: 0.8098910748958588
mAP: 0.19111111111111115
end of display 

saving the model at the end of epoch 70, total_steps 17920
current learning rate: 0.00042052319016987395
decreased learning rate by  0.94
saving the model at the end of epoch 75, total_steps 19200
current learning rate: 0.00039529179875968153
decreased learning rate by  0.94
Display training progress at (epoch 80, total_steps 20480)
loss: 0.8097197115421295
mAP: 0.19111111111111118
end of display 

saving the model at the end of epoch 80, total_steps 20480
current learning rate: 0.0003715742908341006
decreased learning rate by  0.94
saving the model at the end of epoch 85, total_steps 21760
current learning rate: 0.00034927983338405456
decreased learning rate by  0.94
Display training progress at (epoch 90, total_steps 23040)
loss: 0.8096200585365295
mAP: 0.19111111111111118
end of display 

saving the model at the end of epoch 90, total_steps 23040
current learning rate: 0.0003283230433810113
decreased learning rate by  0.94
saving the model at the end of epoch 95, total_steps 24320
current learning rate: 0.00030862366077815057
decreased learning rate by  0.94
Display training progress at (epoch 100, total_steps 25600)
loss: 0.8095887660980224
mAP: 0.19111111111111118
end of display 

Display validation results at (epoch 100, total_steps 25600)
validation mAP is: 0.33333333333333337
validation loss is: 0.6731781959533691
end of display 

saving the model at the end of epoch 100, total_steps 25600
current learning rate: 0.0002901062411314615
decreased learning rate by  0.94
saving the model at the end of epoch 105, total_steps 26880
current learning rate: 0.00027269986666357375
decreased learning rate by  0.94
Display training progress at (epoch 110, total_steps 28160)
loss: 0.809403908252716
mAP: 0.19111111111111115
end of display 

saving the model at the end of epoch 110, total_steps 28160
current learning rate: 0.00025633787466375937
decreased learning rate by  0.94
saving the model at the end of epoch 115, total_steps 29440
current learning rate: 0.00024095760218393377
decreased learning rate by  0.94
Display training progress at (epoch 120, total_steps 30720)
loss: 0.809325760602951
mAP: 0.19111111111111118
end of display 

saving the model at the end of epoch 120, total_steps 30720
current learning rate: 0.00022650014605289773
decreased learning rate by  0.94
saving the model at the end of epoch 125, total_steps 32000
current learning rate: 0.00021291013728972385
decreased learning rate by  0.94
Display training progress at (epoch 130, total_steps 33280)
loss: 0.809350997209549
mAP: 0.19111111111111118
end of display 

saving the model at the end of epoch 130, total_steps 33280
current learning rate: 0.00020013552905234042
decreased learning rate by  0.94
saving the model at the end of epoch 135, total_steps 34560
current learning rate: 0.00018812739730919998
decreased learning rate by  0.94
Display training progress at (epoch 140, total_steps 35840)
loss: 0.8092908978462219
mAP: 0.19111111111111115
end of display 

saving the model at the end of epoch 140, total_steps 35840
current learning rate: 0.000176839753470648
decreased learning rate by  0.94
saving the model at the end of epoch 145, total_steps 37120
current learning rate: 0.0001662293682624091
decreased learning rate by  0.94
Display training progress at (epoch 150, total_steps 38400)
loss: 0.8092693150043487
mAP: 0.19111111111111118
end of display 

Display validation results at (epoch 150, total_steps 38400)
validation mAP is: 0.3333333333333334
validation loss is: 0.6676363945007324
end of display 

saving the model at the end of epoch 150, total_steps 38400
current learning rate: 0.00015625560616666453
decreased learning rate by  0.94
saving the model at the end of epoch 155, total_steps 39680
current learning rate: 0.00014688026979666467
decreased learning rate by  0.94
Display training progress at (epoch 160, total_steps 40960)
loss: 0.8093260228633881
mAP: 0.19111111111111118
end of display 

saving the model at the end of epoch 160, total_steps 40960
current learning rate: 0.00013806745360886476
decreased learning rate by D:\Anaconda3\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
 0.94
saving the model at the end of epoch 165, total_steps 42240
current learning rate: 0.00012978340639233289
decreased learning rate by  0.94
Display training progress at (epoch 170, total_steps 43520)
loss: 0.8091722369194031
mAP: 0.19111111111111118
end of display 

saving the model at the end of epoch 170, total_steps 43520
current learning rate: 0.00012199640200879289
decreased learning rate by  0.94
saving the model at the end of epoch 175, total_steps 44800
current learning rate: 0.00011467661788826532
decreased learning rate by  0.94
Display training progress at (epoch 180, total_steps 46080)
loss: 0.8091711282730103
mAP: 0.19111111111111118
end of display 

saving the model at the end of epoch 180, total_steps 46080
current learning rate: 0.00010779602081496939
decreased learning rate by  0.94
saving the model at the end of epoch 185, total_steps 47360
current learning rate: 0.00010132825956607122
decreased learning rate by  0.94
Display training progress at (epoch 190, total_steps 48640)
loss: 0.8091501951217651
mAP: 0.19111111111111118
end of display 

saving the model at the end of epoch 190, total_steps 48640
current learning rate: 9.524856399210693e-05
decreased learning rate by  0.94
saving the model at the end of epoch 195, total_steps 49920
current learning rate: 8.95336501525805e-05
decreased learning rate by  0.94
Display training progress at (epoch 200, total_steps 51200)
loss: 0.809169453382492
mAP: 0.19111111111111118
end of display 

Display validation results at (epoch 200, total_steps 51200)
validation mAP is: 0.3333333333333333
validation loss is: 0.6664620041847229
end of display 

saving the model at the end of epoch 200, total_steps 51200
current learning rate: 8.416163114342567e-05
decreased learning rate by  0.94
saving the model at the end of epoch 205, total_steps 52480
current learning rate: 7.911193327482013e-05
decreased learning rate by  0.94
Display training progress at (epoch 210, total_steps 53760)
loss: 0.8090904355049133
mAP: 0.19111111111111118
end of display 

saving the model at the end of epoch 210, total_steps 53760
current learning rate: 7.436521727833093e-05
decreased learning rate by  0.94
saving the model at the end of epoch 215, total_steps 55040
current learning rate: 6.990330424163106e-05
decreased learning rate by  0.94
Display training progress at (epoch 220, total_steps 56320)
loss: 0.8090845763683319
mAP: 0.1911111111111111
end of display 

saving the model at the end of epoch 220, total_steps 56320
current learning rate: 6.57091059871332e-05
decreased learning rate by  0.94
saving the model at the end of epoch 225, total_steps 57600
current learning rate: 6.17665596279052e-05
decreased learning rate by  0.94
Display training progress at (epoch 230, total_steps 58880)
loss: 0.8090736091136932
mAP: 0.19111111111111118
end of display 

saving the model at the end of epoch 230, total_steps 58880
current learning rate: 5.806056605023088e-05
decreased learning rate by  0.94
saving the model at the end of epoch 235, total_steps 60160
current learning rate: 5.457693208721703e-05
decreased learning rate by  0.94
Display training progress at (epoch 240, total_steps 61440)
loss: 0.8090323567390442
mAP: 0.19111111111111118
end of display 

saving the model at the end of epoch 240, total_steps 61440
current learning rate: 5.1302316161984e-05
decreased learning rate by  0.94
saving the model at the end of epoch 245, total_steps 62720
current learning rate: 4.822417719226496e-05
decreased learning rate by  0.94
Display training progress at (epoch 250, total_steps 64000)
loss: 0.8090392887592316
mAP: 0.19111111111111118
end of display 

Display validation results at (epoch 250, total_steps 64000)
validation mAP is: 0.3333333333333334
validation loss is: 0.6667874455451965
end of display 

saving the model at the end of epoch 250, total_steps 64000
current learning rate: 4.533072656072906e-05
decreased learning rate by  0.94
saving the model at the end of epoch 255, total_steps 65280
current learning rate: 4.261088296708532e-05
decreased learning rate by  0.94
Display training progress at (epoch 260, total_steps 66560)
loss: 0.8090284705162049
mAP: 0.1911111111111112
end of display 

saving the model at the end of epoch 260, total_steps 66560
current learning rate: 4.005422998906019e-05
decreased learning rate by  0.94
saving the model at the end of epoch 265, total_steps 67840
current learning rate: 3.765097618971658e-05
decreased learning rate by  0.94
Display training progress at (epoch 270, total_steps 69120)
loss: 0.8089883208274842
mAP: 0.19111111111111118
end of display 

saving the model at the end of epoch 270, total_steps 69120
current learning rate: 3.539191761833358e-05
decreased learning rate by  0.94
saving the model at the end of epoch 275, total_steps 70400
current learning rate: 3.326840256123357e-05
decreased learning rate by  0.94
Display training progress at (epoch 280, total_steps 71680)
loss: 0.8089946806430817
mAP: 0.19111111111111118
end of display 

saving the model at the end of epoch 280, total_steps 71680
current learning rate: 3.1272298407559555e-05
decreased learning rate by  0.94
saving the model at the end of epoch 285, total_steps 72960
current learning rate: 2.9395960503105978e-05
decreased learning rate by  0.94
Display training progress at (epoch 290, total_steps 74240)
loss: 0.8089582562446594
mAP: 0.19111111111111118
end of display 

saving the model at the end of epoch 290, total_steps 74240
current learning rate: 2.7632202872919617e-05
decreased learning rate by  0.94
saving the model at the end of epoch 295, total_steps 75520
current learning rate: 2.597427070054444e-05
decreased learning rate by  0.94
Display training progress at (epoch 300, total_steps 76800)
loss: 0.8089740872383118
mAP: 0.19111111111111115
end of display 

Display validation results at (epoch 300, total_steps 76800)
validation mAP is: 0.3333333333333334
validation loss is: 0.6670138239860535
end of display 

saving the model at the end of epoch 300, total_steps 76800
current learning rate: 2.441581445851177e-05
decreased learning rate by  0.94
D:\Anaconda3\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
------------ Options -------------
F: 2401
HDF5FileRoot: e:\Study\Vision\project\Deep-MIML-Network\dataset
K: 4
L: 15
batchSize: 256
checkpoints_dir: checkpoints
dataset: musicInstruments
fc_dimension: 1024
gpu_ids: []
how_many: 50
isTrain: False
mode: test
model: MIML
multi_label_threshold: 0.3
nThreads: 0
name: deepMIML
norm: max
num_of_bases: 25
num_of_fc: 1
selected_classes: True
using_multi_labels: True
which_epoch: latest
with_batchnorm: True
zeroCenterInput: False
-------------- End ----------------
CustomDatasetDataLoader
dataset [MIMLDataset] was created
MIML dataset initialize suceed
(3,)
(3,)
#testing images = 3
Traceback (most recent call last):
  File "test.py", line 17, in <module>
    model = torch.load(os.path.join('.', opt.checkpoints_dir, opt.name, str(opt.which_epoch) + '.pth'))
NameError: name 'torch' is not defined
Traceback (most recent call last):
  File "test.py", line 8, in <module>
    import pytorch as torch
ModuleNotFoundError: No module named 'pytorch'
D:\Anaconda3\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
------------ Options -------------
F: 2401
HDF5FileRoot: e:\Study\Vision\project\Deep-MIML-Network\dataset
K: 4
L: 15
batchSize: 256
checkpoints_dir: checkpoints
dataset: musicInstruments
fc_dimension: 1024
gpu_ids: []
how_many: 50
isTrain: False
mode: test
model: MIML
multi_label_threshold: 0.3
nThreads: 0
name: deepMIML
norm: max
num_of_bases: 25
num_of_fc: 1
selected_classes: True
using_multi_labels: True
which_epoch: latest
with_batchnorm: True
zeroCenterInput: False
-------------- End ----------------
CustomDatasetDataLoader
dataset [MIMLDataset] was created
MIML dataset initialize suceed
(3,)
(3,)
#testing images = 3
Traceback (most recent call last):
  File "test.py", line 18, in <module>
    model = torch.load(os.path.join('.', opt.checkpoints_dir, opt.name, str(opt.which_epoch) + '.pth'))
NameError: name 'os' is not defined
D:\Anaconda3\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
------------ Options -------------
F: 2401
HDF5FileRoot: e:\Study\Vision\project\Deep-MIML-Network\dataset
K: 4
L: 15
batchSize: 256
checkpoints_dir: checkpoints
dataset: musicInstruments
fc_dimension: 1024
gpu_ids: []
how_many: 50
isTrain: False
mode: test
model: MIML
multi_label_threshold: 0.3
nThreads: 0
name: deepMIML
norm: max
num_of_bases: 25
num_of_fc: 1
selected_classes: True
using_multi_labels: True
which_epoch: latest
with_batchnorm: True
zeroCenterInput: False
-------------- End ----------------
CustomDatasetDataLoader
dataset [MIMLDataset] was created
MIML dataset initialize suceed
(3,)
(3,)
#testing images = 3
0
Traceback (most recent call last):
  File "test.py", line 29, in <module>
    accuracy, loss = model.test(data)
  File "E:\Study\Vision\project\Deep-MIML-Network\models\MIML.py", line 191, in test
    loss = self.loss(self.output, label).data.cpu().numpy()[0]
IndexError: too many indices for array
D:\Anaconda3\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
------------ Options -------------
F: 2401
HDF5FileRoot: e:\Study\Vision\project\Deep-MIML-Network\dataset
K: 4
L: 15
batchSize: 256
checkpoints_dir: checkpoints
dataset: musicInstruments
fc_dimension: 1024
gpu_ids: []
how_many: 50
isTrain: False
mode: test
model: MIML
multi_label_threshold: 0.3
nThreads: 0
name: deepMIML
norm: max
num_of_bases: 25
num_of_fc: 1
selected_classes: True
using_multi_labels: True
which_epoch: latest
with_batchnorm: True
zeroCenterInput: False
-------------- End ----------------
CustomDatasetDataLoader
dataset [MIMLDataset] was created
MIML dataset initialize suceed
(3,)
(3,)
#testing images = 3
0
1
2
test accuracy is: tensor(0)
test loss is: 0.9337942401568095
D:\Anaconda3\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
------------ Options -------------
F: 2401
HDF5FileRoot: e:\Study\Vision\project\Deep-MIML-Network\dataset
K: 4
L: 15
batchSize: 256
checkpoints_dir: checkpoints
continue_train: True
dataset: musicInstruments
decay_factor: 0.94
display_freq: 10
epoch_count: 0
fc_dimension: 1024
gpu_ids: []
init_type: normal
isTrain: True
learning_rate: 0.001
learning_rate_decrease_itr: 5
measure_time: False
mode: train
model: MIML
multi_label_threshold: 0.3
nThreads: 0
name: deepMIML
niter: 300
norm: max
num_of_bases: 25
num_of_fc: 1
save_epoch_freq: 5
save_latest_freq: 500
selected_classes: True
using_multi_labels: True
validation_batches: 10
validation_freq: 50
validation_on: True
with_batchnorm: True
with_softmax: True
zeroCenterInput: False
-------------- End ----------------
CustomDatasetDataLoader
dataset [MIMLDataset] was created
MIML dataset initialize suceed
(15,)
(15,)
#training images = 15
CustomDatasetDataLoader
dataset [MIMLDataset] was created
MIML dataset initialize suceed
(7,)
(7,)
#validation images = 7
MIML
initialization method [normal]
model [MIMLModel] was created
starting from epoch  300
current learning rate: 2.441581445851177e-05
------------ Options -------------
F: 2401
HDF5FileRoot: e:\Study\Vision\project\Deep-MIML-Network\dataset
K: 4
L: 15
batchSize: 256
checkpoints_dir: checkpoints
continue_train: True
dataset: musicInstruments
decay_factor: 0.94
display_freq: 10
epoch_count: 0
fc_dimension: 1024
gpu_ids: []
init_type: normal
isTrain: True
learning_rate: 0.001
learning_rate_decrease_itr: 5
measure_time: False
mode: train
model: MIML
multi_label_threshold: 0.3
nThreads: 0
name: deepMIML
niter: 300
norm: max
num_of_bases: 25
num_of_fc: 1
save_epoch_freq: 5
save_latest_freq: 500
selected_classes: True
using_multi_labels: True
validation_batches: 10
validation_freq: 50
validation_on: True
with_batchnorm: True
with_softmax: True
zeroCenterInput: False
-------------- End ----------------
CustomDatasetDataLoader
dataset [MIMLDataset] was created
MIML dataset initialize suceed
(15,)
(15,)
#training images = 15
CustomDatasetDataLoader
dataset [MIMLDataset] was created
MIML dataset initialize suceed
(7,)
(7,)
#validation images = 7
MIML
initialization method [normal]
model [MIMLModel] was created
starting from epoch  0
saving the model at the end of epoch 5, total_steps 1280
current learning rate: 0.00094
decreased learning rate by  0.94
Display training progress at (epoch 10, total_steps 2560)
loss: 0.8923505902290344
mAP: 0.18488888888888894
end of display 

saving the model at the end of epoch 10, total_steps 2560
current learning rate: 0.0008836
decreased learning rate by  0.94
saving the model at the end of epoch 15, total_steps 3840
current learning rate: 0.0008305839999999999
decreased learning rate by  0.94
Display training progress at (epoch 20, total_steps 5120)
loss: 0.8782714426517486
mAP: 0.19111111111111115
end of display 

saving the model at the end of epoch 20, total_steps 5120
current learning rate: 0.0007807489599999998
decreased learning rate by  0.94
saving the model at the end of epoch 25, total_steps 6400
current learning rate: 0.0007339040223999998
decreased learning rate by  0.94
Display training progress at (epoch 30, total_steps 7680)
loss: 0.8758477985858917
mAP: 0.19111111111111118
end of display 

saving the model at the end of epoch 30, total_steps 7680
current learning rate: 0.0006898697810559998
decreased learning rate by  0.94
saving the model at the end of epoch 35, total_steps 8960
current learning rate: 0.0006484775941926397
decreased learning rate by  0.94
Display training progress at (epoch 40, total_steps 10240)
loss: 0.8741873502731323
mAP: 0.19111111111111118
end of display 

saving the model at the end of epoch 40, total_steps 10240
current learning rate: 0.0006095689385410813
decreased learning rate by  0.94
saving the model at the end of epoch 45, total_steps 11520
current learning rate: 0.0005729948022286164
decreased learning rate by  0.94
Display training progress at (epoch 50, total_steps 12800)
loss: 0.8733251512050628
mAP: 0.19111111111111118
end of display 

Display validation results at (epoch 50, total_steps 12800)
validation mAP is: 0.33333333333333337
validation loss is: 0.8104938864707947
end of display 

saving the model at the end of epoch 50, total_steps 12800
current learning rate: 0.0005386151140948994
decreased learning rate by  0.94
saving the model at the end of epoch 55, total_steps 14080
current learning rate: 0.0005062982072492054
decreased learning rate by  0.94
Display training progress at (epoch 60, total_steps 15360)
loss: 0.8728763461112976
mAP: 0.19111111111111118
end of display 

saving the model at the end of epoch 60, total_steps 15360
current learning rate: 0.0004759203148142531
decreased learning rate by  0.94
saving the model at the end of epoch 65, total_steps 16640
current learning rate: 0.00044736509592539786
decreased learning rate by  0.94
Display training progress at (epoch 70, total_steps 17920)
loss: 0.8726294159889221
mAP: 0.19111111111111118
end of display 

saving the model at the end of epoch 70, total_steps 17920
current learning rate: 0.00042052319016987395
decreased learning rate by  0.94
saving the model at the end of epoch 75, total_steps 19200
current learning rate: 0.00039529179875968153
decreased learning rate by  0.94
Display training progress at (epoch 80, total_steps 20480)
loss: 0.87242893576622
mAP: 0.19111111111111118
end of display 

saving the model at the end of epoch 80, total_steps 20480
current learning rate: 0.0003715742908341006
decreased learning rate by  0.94
saving the model at the end of epoch 85, total_steps 21760
current learning rate: 0.00034927983338405456
decreased learning rate by  0.94
Display training progress at (epoch 90, total_steps 23040)
loss: 0.8722337424755097
mAP: 0.19111111111111118
end of display 

saving the model at the end of epoch 90, total_steps 23040
current learning rate: 0.0003283230433810113
decreased learning rate by  0.94
saving the model at the end of epoch 95, total_steps 24320
current learning rate: 0.00030862366077815057
decreased learning rate by  0.94
Display training progress at (epoch 100, total_steps 25600)
loss: 0.8721000611782074
mAP: 0.1911111111111112
end of display 

Display validation results at (epoch 100, total_steps 25600)
validation mAP is: 0.3333333333333334
validation loss is: 0.8194959759712219
end of display 

saving the model at the end of epoch 100, total_steps 25600
current learning rate: 0.0002901062411314615
decreased learning rate by  0.94
saving the model at the end of epoch 105, total_steps 26880
current learning rate: 0.00027269986666357375
decreased learning rate by  0.94
Display training progress at (epoch 110, total_steps 28160)
loss: 0.8719759941101074
mAP: 0.19111111111111118
end of display 

saving the model at the end of epoch 110, total_steps 28160
current learning rate: 0.00025633787466375937
decreased learning rate by  0.94
saving the model at the end of epoch 115, total_steps 29440
current learning rate: 0.00024095760218393377
decreased learning rate by  0.94
Display training progress at (epoch 120, total_steps 30720)
loss: 0.8718801617622376
mAP: 0.19111111111111118
end of display 

saving the model at the end of epoch 120, total_steps 30720
current learning rate: 0.00022650014605289773
decreased learning rate by  0.94
saving the model at the end of epoch 125, total_steps 32000
current learning rate: 0.00021291013728972385
decreased learning rate by  0.94
Display training progress at (epoch 130, total_steps 33280)
loss: 0.8717708110809326
mAP: 0.19111111111111118
end of display 

saving the model at the end of epoch 130, total_steps 33280
current learning rate: 0.00020013552905234042
decreased learning rate by  0.94
saving the model at the end of epoch 135, total_steps 34560
current learning rate: 0.00018812739730919998
decreased learning rate by  0.94
Display training progress at (epoch 140, total_steps 35840)
loss: 0.8716904640197753
mAP: 0.19111111111111118
end of display 

saving the model at the end of epoch 140, total_steps 35840
current learning rate: 0.000176839753470648
decreased learning rate by  0.94
saving the model at the end of epoch 145, total_steps 37120
current learning rate: 0.0001662293682624091
decreased learning rate by  0.94
Display training progress at (epoch 150, total_steps 38400)
loss: 0.87161283493042
mAP: 0.19111111111111118
end of display 

Display validation results at (epoch 150, total_steps 38400)
validation mAP is: 0.33333333333333337
validation loss is: 0.831455409526825
end of display 

saving the model at the end of epoch 150, total_steps 38400
current learning rate: 0.00015625560616666453
decreased learning rate by  0.94
saving the model at the end of epoch 155, total_steps 39680
current learning rate: 0.00014688026979666467
decreased learning rate by  0.94
Display training progress at (epoch 160, total_steps 40960)
loss: 0.8715543448925018
mAP: 0.19111111111111115
end of display 

saving the model at the end of epoch 160, total_steps 40960
current learning rate: 0.00013806745360886476
decreased learning rate by D:\Anaconda3\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
 0.94
saving the model at the end of epoch 165, total_steps 42240
current learning rate: 0.00012978340639233289
decreased learning rate by  0.94
Display training progress at (epoch 170, total_steps 43520)
loss: 0.8715073764324188
mAP: 0.19111111111111118
end of display 

saving the model at the end of epoch 170, total_steps 43520
current learning rate: 0.00012199640200879289
decreased learning rate by  0.94
saving the model at the end of epoch 175, total_steps 44800
current learning rate: 0.00011467661788826532
decreased learning rate by  0.94
Display training progress at (epoch 180, total_steps 46080)
loss: 0.8714593887329102
mAP: 0.19111111111111115
end of display 

saving the model at the end of epoch 180, total_steps 46080
current learning rate: 0.00010779602081496939
decreased learning rate by  0.94
saving the model at the end of epoch 185, total_steps 47360
current learning rate: 0.00010132825956607122
decreased learning rate by  0.94
Display training progress at (epoch 190, total_steps 48640)
loss: 0.8714199364185333
mAP: 0.19111111111111118
end of display 

saving the model at the end of epoch 190, total_steps 48640
current learning rate: 9.524856399210693e-05
decreased learning rate by  0.94
saving the model at the end of epoch 195, total_steps 49920
current learning rate: 8.95336501525805e-05
decreased learning rate by  0.94
Display training progress at (epoch 200, total_steps 51200)
loss: 0.8713805794715881
mAP: 0.19111111111111118
end of display 

Display validation results at (epoch 200, total_steps 51200)
validation mAP is: 0.33333333333333337
validation loss is: 0.8337851166725159
end of display 

saving the model at the end of epoch 200, total_steps 51200
current learning rate: 8.416163114342567e-05
decreased learning rate by  0.94
saving the model at the end of epoch 205, total_steps 52480
current learning rate: 7.911193327482013e-05
decreased learning rate by  0.94
Display training progress at (epoch 210, total_steps 53760)
loss: 0.8713482260704041
mAP: 0.1911111111111112
end of display 

saving the model at the end of epoch 210, total_steps 53760
current learning rate: 7.436521727833093e-05
decreased learning rate by  0.94
saving the model at the end of epoch 215, total_steps 55040
current learning rate: 6.990330424163106e-05
decreased learning rate by  0.94
Display training progress at (epoch 220, total_steps 56320)
loss: 0.8713173508644104
mAP: 0.19111111111111118
end of display 

saving the model at the end of epoch 220, total_steps 56320
current learning rate: 6.57091059871332e-05
decreased learning rate by  0.94
saving the model at the end of epoch 225, total_steps 57600
current learning rate: 6.17665596279052e-05
decreased learning rate by  0.94
Display training progress at (epoch 230, total_steps 58880)
loss: 0.8712938725948334
mAP: 0.19111111111111118
end of display 

saving the model at the end of epoch 230, total_steps 58880
current learning rate: 5.806056605023088e-05
decreased learning rate by  0.94
saving the model at the end of epoch 235, total_steps 60160
current learning rate: 5.457693208721703e-05
decreased learning rate by  0.94
Display training progress at (epoch 240, total_steps 61440)
loss: 0.8712729632854461
mAP: 0.19111111111111115
end of display 

saving the model at the end of epoch 240, total_steps 61440
current learning rate: 5.1302316161984e-05
decreased learning rate by  0.94
saving the model at the end of epoch 245, total_steps 62720
current learning rate: 4.822417719226496e-05
decreased learning rate by  0.94
Display training progress at (epoch 250, total_steps 64000)
loss: 0.8712519764900207
mAP: 0.19111111111111118
end of display 

Display validation results at (epoch 250, total_steps 64000)
validation mAP is: 0.3333333333333333
validation loss is: 0.831790566444397
end of display 

saving the model at the end of epoch 250, total_steps 64000
current learning rate: 4.533072656072906e-05
decreased learning rate by  0.94
saving the model at the end of epoch 255, total_steps 65280
current learning rate: 4.261088296708532e-05
decreased learning rate by  0.94
Display training progress at (epoch 260, total_steps 66560)
loss: 0.871234679222107
mAP: 0.19111111111111118
end of display 

saving the model at the end of epoch 260, total_steps 66560
current learning rate: 4.005422998906019e-05
decreased learning rate by  0.94
saving the model at the end of epoch 265, total_steps 67840
current learning rate: 3.765097618971658e-05
decreased learning rate by  0.94
Display training progress at (epoch 270, total_steps 69120)
loss: 0.8712196111679077
mAP: 0.19111111111111118
end of display 

saving the model at the end of epoch 270, total_steps 69120
current learning rate: 3.539191761833358e-05
decreased learning rate by  0.94
saving the model at the end of epoch 275, total_steps 70400
current learning rate: 3.326840256123357e-05
decreased learning rate by  0.94
Display training progress at (epoch 280, total_steps 71680)
loss: 0.8712062954902648
mAP: 0.19111111111111118
end of display 

saving the model at the end of epoch 280, total_steps 71680
current learning rate: 3.1272298407559555e-05
decreased learning rate by  0.94
saving the model at the end of epoch 285, total_steps 72960
current learning rate: 2.9395960503105978e-05
decreased learning rate by  0.94
Display training progress at (epoch 290, total_steps 74240)
loss: 0.871195787191391
mAP: 0.19111111111111118
end of display 

saving the model at the end of epoch 290, total_steps 74240
current learning rate: 2.7632202872919617e-05
decreased learning rate by  0.94
saving the model at the end of epoch 295, total_steps 75520
current learning rate: 2.597427070054444e-05
decreased learning rate by  0.94
Display training progress at (epoch 300, total_steps 76800)
loss: 0.8711823284626007
mAP: 0.19111111111111115
end of display 

Display validation results at (epoch 300, total_steps 76800)
validation mAP is: 0.3333333333333334
validation loss is: 0.8299155831336975
end of display 

saving the model at the end of epoch 300, total_steps 76800
current learning rate: 2.441581445851177e-05
decreased learning rate by  0.94
usage: train.py [-h] [--K K] [--L L] [--F F] [--num_of_bases NUM_OF_BASES]
                [--num_of_fc NUM_OF_FC] [--with_batchnorm]
                [--fc_dimension FC_DIMENSION] --HDF5FileRoot HDF5FILEROOT
                [--gpu_ids GPU_IDS] [--name NAME]
                [--checkpoints_dir CHECKPOINTS_DIR]
                [--model {MIML,KL_divergence}] [--batchSize BATCHSIZE]
                [--nThreads NTHREADS] [--norm {l1,l2,max,none}]
                [--dataset {musicInstruments,animals,vehicles,all}]
                [--using_multi_labels]
                [--multi_label_threshold MULTI_LABEL_THRESHOLD]
                [--selected_classes] [--zeroCenterInput]
                [--display_freq DISPLAY_FREQ]
                [--save_epoch_freq SAVE_EPOCH_FREQ]
                [--save_latest_freq SAVE_LATEST_FREQ] [--continue_train]
                [--epoch_count EPOCH_COUNT] [--learning_rate LEARNING_RATE]
                [--learning_rate_decrease_itr LEARNING_RATE_DECREASE_ITR]
                [--decay_factor DECAY_FACTOR] [--niter NITER]
                [--init_type {normal,xavier,kaiming,orthogonal}]
                [--measure_time] [--validation_on]
                [--validation_freq VALIDATION_FREQ]
                [--validation_batches VALIDATION_BATCHES] [--with_softmax]
train.py: error: unrecognized arguments: -- K 1 --num_of_bases 16 --num_of_fc 1 --learning_rate 0.001 --learning_rate_decrease_itr 5 --decay_factor 0.94 --display_freq 20 --save_epoch_freq 20 --save_latest_freq 500 --gpu_ids -1 --nThreads 0 --with_batchnorm --with_softmax --continue_train --niter 300 --validation_on --validation_freq 50 --validation_batches 10 --measure_time --selected_classes --using_multi_labels
D:\Anaconda3\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
------------ Options -------------
F: 2401
HDF5FileRoot: e:\Study\Vision\project\Deep-MIML-Network\dataset
K: 1
L: 8
batchSize: 64
checkpoints_dir: checkpoints
continue_train: True
dataset: musicInstruments
decay_factor: 0.94
display_freq: 20
epoch_count: 0
fc_dimension: 1024
gpu_ids: []
init_type: normal
isTrain: True
learning_rate: 0.001
learning_rate_decrease_itr: 5
measure_time: True
mode: train
model: MIML
multi_label_threshold: 0.3
nThreads: 0
name: deepMIML
niter: 300
norm: max
num_of_bases: 16
num_of_fc: 1
save_epoch_freq: 20
save_latest_freq: 500
selected_classes: True
using_multi_labels: True
validation_batches: 10
validation_freq: 50
validation_on: True
with_batchnorm: True
with_softmax: True
zeroCenterInput: False
-------------- End ----------------
CustomDatasetDataLoader
dataset [MIMLDataset] was created
MIML dataset initialize suceed
bases num:  (309,)
labels num:  (309,)
#training images = 309
CustomDatasetDataLoader
dataset [MIMLDataset] was created
MIML dataset initialize suceed
bases num:  (133,)
labels num:  (133,)
#validation images = 133
MIML
initialization method [normal]
model [MIMLModel] was created
starting from epoch  0
Display training progress at (epoch 4, total_steps 1280)
loss: 0.4246387183666229
mAP: 0.7775095813679245
average data loading time: 0.2299911618232727
average forward time: 0.18596576452255248
average backward time: 0.3279459476470947
end of display 

current learning rate: 0.00094
decreased learning rate by  0.94
Display training progress at (epoch 8, total_steps 2560)
loss: 0.3445870652794838
mAP: 0.43913067511792453
average data loading time: 0.1380144476890564
average forward time: 0.17073572874069215
average backward time: 0.30281226634979247
end of display 

Display validation results at (epoch 10, total_steps 3200)
validation mAP is: 0.43671875
validation loss is: 0.31913867592811584
end of display 

current learning rate: 0.0008836
decreased learning rate by  0.94
Display training progress at (epoch 12, total_steps 3840)
loss: 0.3286358579993248
mAP: 0.31703825176886796
average data loading time: 0.12850112915039064
average forward time: 0.15149955749511718
average backward time: 0.26283214092254636
end of display 

current learning rate: 0.0008305839999999999
decreased learning rate by  0.94
Display training progress at (epoch 16, total_steps 5120)
loss: 0.3183804348111153
mAP: 0.3171801297169811
average data loading time: 0.14243351221084594
average forward time: 0.1830195188522339
average backward time: 0.3088840365409851
end of display 

Display training progress at (epoch 20, total_steps 6400)
loss: 0.3111312761902809
mAP: 0.31732200766509433
average data loading time: 0.14761518239974974
average forward time: 0.17287348508834838
average backward time: 0.3115360617637634
end of display 

Display validation results at (epoch 20, total_steps 6400)
validation mAP is: 0.43671875
validation loss is: 0.30726127823193866
end of display 

saving the model at the end of epoch 20, total_steps 6400
Traceback (most recent call last):
  File "train.py", line 115, in <module>
    os.remove(os.path.join('.', opt.checkpoints_dir, opt.name, str(epoch-save_epoch_freq) + '.pth'))  # 保存新的模型时就删去旧的
NameError: name 'save_epoch_freq' is not defined
D:\Anaconda3\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
------------ Options -------------
F: 2401
HDF5FileRoot: e:\Study\Vision\project\Deep-MIML-Network\dataset
K: 1
L: 8
batchSize: 64
checkpoints_dir: checkpoints
continue_train: True
dataset: musicInstruments
decay_factor: 0.94
display_freq: 20
epoch_count: 0
fc_dimension: 1024
gpu_ids: []
init_type: normal
isTrain: True
learning_rate: 0.001
learning_rate_decrease_itr: 5
measure_time: True
mode: train
model: MIML
multi_label_threshold: 0.3
nThreads: 0
name: deepMIML
niter: 300
norm: max
num_of_bases: 16
num_of_fc: 1
save_epoch_freq: 20
save_latest_freq: 500
selected_classes: True
using_multi_labels: True
validation_batches: 10
validation_freq: 50
validation_on: True
with_batchnorm: True
with_softmax: True
zeroCenterInput: False
-------------- End ----------------
CustomDatasetDataLoader
dataset [MIMLDataset] was created
MIML dataset initialize suceed
bases num:  (309,)
labels num:  (309,)
#training images = 309
CustomDatasetDataLoader
dataset [MIMLDataset] was created
MIML dataset initialize suceed
bases num:  (133,)
labels num:  (133,)
#validation images = 133
MIML
initialization method [normal]
model [MIMLModel] was created
starting from epoch  0
Display training progress at (epoch 4, total_steps 1280)
loss: 0.45196854174137113
mAP: 0.39091796874999996
average data loading time: 0.12550045251846315
average forward time: 0.15454812049865724
average backward time: 0.2812024474143982
end of display 

current learning rate: 0.00094
decreased learning rate by  0.94
Display training progress at (epoch 8, total_steps 2560)
loss: 0.3501884162425995
mAP: 0.31718012971698106
average data loading time: 0.13507899045944213
average forward time: 0.15689095258712768
average backward time: 0.2771921634674072
end of display 

Display validation results at (epoch 10, total_steps 3200)
validation mAP is: 0.43671875
validation loss is: 0.3216607669989268
end of display 

current learning rate: 0.0008836
decreased learning rate by  0.94
Display training progress at (epoch 12, total_steps 3840)
loss: 0.3326344728469849
mAP: 0.31803139740566044
average data loading time: 0.13377465009689332
average forward time: 0.16607023477554322
average backward time: 0.28049317598342893
end of display 

current learning rate: 0.0008305839999999999
decreased learning rate by  0.94
Display training progress at (epoch 16, total_steps 5120)
loss: 0.3206221267580986
mAP: 0.7074476709905662
average data loading time: 0.12483772039413452
average forward time: 0.15527791976928712
average backward time: 0.26914019584655763
end of display 

Display training progress at (epoch 20, total_steps 6400)
loss: 0.31106694489717485
mAP: 0.8078198702830189
average data loading time: 0.1281592845916748
average forward time: 0.15414075851440429
average backward time: 0.2618436098098755
end of display 

Display validation results at (epoch 20, total_steps 6400)
validation mAP is: 0.849609375
validation loss is: 0.3079139490922292
end of display 

saving the model at the end of epoch 20, total_steps 6400
Traceback (most recent call last):
  File "train.py", line 115, in <module>
    os.remove(os.path.join('.', opt.checkpoints_dir, opt.name, str(epoch-opt.save_epoch_freq) + '.pth'))  # 保存新的模型时就删去旧的
FileNotFoundError: [WinError 2] 系统找不到指定的文件。: '.\\checkpoints\\deepMIML\\0.pth'
------------ Options -------------
F: 2401
HDF5FileRoot: e:\Study\Vision\project\Deep-MIML-Network\dataset
K: 1
L: 8
batchSize: 64
checkpoints_dir: checkpoints
continue_train: True
dataset: musicInstruments
decay_factor: 0.94
display_freq: 20
epoch_count: 0
fc_dimension: 1024
gpu_ids: []
init_type: normal
isTrain: True
learning_rate: 0.001
learning_rate_decrease_itr: 5
measure_time: True
mode: train
model: MIML
multi_label_threshold: 0.3
nThreads: 0
name: deepMIML
niter: 300
norm: max
num_of_bases: 16
num_of_fc: 1
save_epoch_freq: 20
save_latest_freq: 500
selected_classes: True
using_multi_labels: True
validation_batches: 10
validation_freq: 50
validation_on: True
with_batchnorm: True
with_softmax: True
zeroCenterInput: False
-------------- End ----------------
CustomDatasetDataLoader
dataset [MIMLDataset] was created
MIML dataset initialize suceed
bases num:  (309,)
labels num:  (309,)
#training images = 309
CustomDatasetDataLoader
dataset [MIMLDataset] was created
MIML dataset initialize suceed
bases num:  (133,)
labels num:  (133,)
#validation images = 133
MIML
initialization method [normal]
model [MIMLModel] was created
starting from epoch  0
Display training progress at (epoch 4, total_steps 1280)
loss: 0.4458510220050812
mAP: 0.7441037735849056
average data loading time: 0.14000186920166016
average forward time: 0.16139776706695558
average backward time: 0.3032341837882996
end of display 

current learning rate: 0.00094
decreased learning rate by  0.94
Display training progress at (epoch 8, total_steps 2560)
loss: 0.3390870228409767
mAP: 0.8079617482311321
average data loading time: 0.12588245868682862
average forward time: 0.1586700439453125
average backward time: 0.27106685638427735
end of display 

Display validation results at (epoch 10, total_steps 3200)
validation mAP is: 0.849609375
validation loss is: 0.3080424964427948
end of display 

current learning rate: 0.0008836
decreased learning rate by  0.94
Display training progress at (epoch 12, total_steps 3840)
loss: 0.3195096507668495
mAP: 0.8082455041273585
average data loading time: 0.1306501507759094
average forward time: 0.16040346622467042
average backward time: 0.2803885817527771
end of display 

current learning rate: 0.0008305839999999999
decreased learning rate by  0.94
Display training progress at (epoch 16, total_steps 5120)
loss: 0.3094945758581161
mAP: 0.8061173349056603
average data loading time: 0.13844603300094604
average forward time: 0.17250050306320192
average backward time: 0.3037108302116394
end of display 

Display training progress at (epoch 20, total_steps 6400)
loss: 0.30283419191837313
mAP: 0.8076779923349056
average data loading time: 0.1350002884864807
average forward time: 0.16699861288070678
average backward time: 0.2801023244857788
end of display 

Display validation results at (epoch 20, total_steps 6400)
validation mAP is: 0.7958333333333334
validation loss is: 0.299868106842041
end of display 

saving the model at the end of epoch 20, total_steps 6400
current learning rate: 0.0007807489599999998
decreased learning rate by  0.94
Display training progress at (epoch 24, total_steps 7680)
loss: 0.2974235638976097
mAP: 0.8082455041273585
average data loading time: 0.12516261339187623
average forward time: 0.15050959587097168
average backward time: 0.263190495967865
end of display 

current learning rate: 0.0007339040223999998
decreased learning rate by  0.94
Display training progress at (epoch 28, total_steps 8960)
loss: 0.29245949238538743
mAP: 0.8064010908018868
average data loading time: 0.14937926530838014
average forward time: 0.16867467164993286
average backward time: 0.30123833417892454
end of display 

Display validation results at (epoch 30, total_steps 9600)
validation mAP is: 0.849609375
validation loss is: 0.28794528047243756
end of display 

current learning rate: 0.0006898697810559998
decreased learning rate by  0.94
Display training progress at (epoch 32, total_steps 10240)
loss: 0.2885809078812599
mAP: 0.8073942364386794
average data loading time: 0.14783505201339722
average forward time: 0.17532113790512086
average backward time: 0.29672995805740354
end of display 

current learning rate: 0.0006484775941926397
decreased learning rate by  0.94
Display training progress at (epoch 36, total_steps 11520)
loss: 0.2852136492729187
mAP: 0.8081036261792451
average data loading time: 0.1397741675376892
average forward time: 0.1673653841018677
average backward time: 0.29090683460235595
end of display 

Display training progress at (epoch 40, total_steps 12800)
loss: 0.2818480536341667
mAP: 0.8081036261792454
average data loading time: 0.14286803007125853
average forward time: 0.17808369398117066
average backward time: 0.30935063362121584
end of display 

Display validation results at (epoch 40, total_steps 12800)
validation mAP is: 0.7958333333333334
validation loss is: 0.2803030212720235
end of display 

saving the model at the end of epoch 40, total_steps 12800
current learning rate: 0.0006095689385410813
decreased learning rate by  0.94
Display training progress at (epoch 44, total_steps 14080)
loss: 0.2786350056529045
mAP: 0.8085292600235849
average data loading time: 0.13976250886917113
average forward time: 0.1676965594291687
average backward time: 0.28515444993972777
end of display 

current learning rate: 0.0005729948022286164
decreased learning rate by  0.94
Display training progress at (epoch 48, total_steps 15360)
loss: 0.2759883373975754
mAP: 0.8064010908018868
average data loading time: 0.13466564416885377
average forward time: 0.17140285968780516
average backward time: 0.2854537606239319
end of display 

Display validation results at (epoch 50, total_steps 16000)
validation mAP is: 0.7958333333333334
validation loss is: 0.27357326944669086
end of display 

current learning rate: 0.0005386151140948994
decreased learning rate by  0.94
Display training progress at (epoch 52, total_steps 16640)
loss: 0.2735125094652176
mAP: 0.8076779923349056
average data loading time: 0.14578814506530763
average forward time: 0.15744292736053467
average backward time: 0.2906817436218262
end of display 

current learning rate: 0.0005062982072492054
decreased learning rate by  0.94
Display training progress at (epoch 56, total_steps 17920)
loss: 0.27118924260139465
mAP: 0.8075361143867925
average data loading time: 0.1331526517868042
average forward time: 0.16096526384353638
average backward time: 0.27770941257476806
end of display 

Display training progress at (epoch 60, total_steps 19200)
loss: 0.2691687226295471
mAP: 0.8075361143867925
average data loading time: 0.1317366123199463
average forward time: 0.16306225061416627
average backward time: 0.27693341970443724
end of display 

Display validation results at (epoch 60, total_steps 19200)
validation mAP is: 0.7958333333333334
validation loss is: 0.26840562621752423
end of display 

saving the model at the end of epoch 60, total_steps 19200
current learning rate: 0.0004759203148142531
decreased learning rate by  0.94
Display training progress at (epoch 64, total_steps 20480)
loss: 0.2673369377851486
mAP: 0.8073942364386794
average data loading time: 0.1368353009223938
average forward time: 0.15920015573501586
average backward time: 0.2914857268333435
end of display 

current learning rate: 0.00044736509592539786
decreased learning rate by  0.94
Display training progress at (epoch 68, total_steps 21760)
loss: 0.26556334644556046
mAP: 0.8052660672169811
average data loading time: 0.1312267780303955
average forward time: 0.15386947393417358
average backward time: 0.2693093538284302
end of display 

Display validation results at (epoch 70, total_steps 22400)
validation mAP is: 0.7958333333333334
validation loss is: 0.26423782110214233
end of display 

current learning rate: 0.00042052319016987395
decreased learning rate by  0.94
Display training progress at (epoch 72, total_steps 23040)
loss: 0.2639279827475548
mAP: 0.8075361143867925
average data loading time: 0.12803170680999756
average forward time: 0.1600271940231323
average backward time: 0.2753926396369934
end of display 

current learning rate: 0.00039529179875968153
decreased learning rate by  0.94
Display training progress at (epoch 76, total_steps 24320)
loss: 0.26243826895952227
mAP: 0.8076779923349058
average data loading time: 0.12929899692535402
average forward time: 0.14952757358551025
average backward time: 0.2691307902336121
end of display 

Display training progress at (epoch 80, total_steps 25600)
loss: 0.26111724972724915
mAP: 0.8071104805424529
average data loading time: 0.1300284743309021
average forward time: 0.15572504997253417
average backward time: 0.2662570357322693
end of display 

Display validation results at (epoch 80, total_steps 25600)
validation mAP is: 0.7420572916666667
validation loss is: 0.2613527178764343
end of display 

saving the model at the end of epoch 80, total_steps 25600
current learning rate: 0.0003715742908341006
decreased learning rate by  0.94
Display training progress at (epoch 84, total_steps 26880)
loss: 0.25994176119565965
mAP: 0.8075361143867925
average data loading time: 0.12946076393127443
average forward time: 0.14919828176498412
average backward time: 0.2687303781509399
end of display 

current learning rate: 0.00034927983338405456
decreased learning rate by  0.94
Display training progress at (epoch 88, total_steps 28160)
loss: 0.25889841467142105
mAP: 0.8066848466981134
average data loading time: 0.13036186695098878
average forward time: 0.15626102685928345
average backward time: 0.27187892198562624
end of display 

Display validation results at (epoch 90, total_steps 28800)
validation mAP is: 0.7420572916666667
validation loss is: 0.2587013940016429
end of display 

current learning rate: 0.0003283230433810113
decreased learning rate by  0.94
Display training progress at (epoch 92, total_steps 29440)
loss: 0.25785001665353774
mAP: 0.807252358490566
average data loading time: 0.12819337844848633
average forward time: 0.15157424211502074
average backward time: 0.2772209644317627
end of display 

current learning rate: 0.00030862366077815057
decreased learning rate by  0.94
Display training progress at (epoch 96, total_steps 30720)
loss: 0.25691063702106476
mAP: 0.8071104805424529
average data loading time: 0.12817556858062745
average forward time: 0.152892804145813
average backward time: 0.2675378918647766
end of display 

Display training progress at (epoch 100, total_steps 32000)
loss: 0.2559579536318779
mAP: 0.8081036261792454
average data loading time: 0.1286548852920532
average forward time: 0.16789218187332153
average backward time: 0.2735914349555969
end of display 

saving the latest model (epoch 100, total_steps 32000)
Display validation results at (epoch 100, total_steps 32000)
validation mAP is: 0.7420572916666667
validation loss is: 0.2564217944939931
end of display 

saving the model at the end of epoch 100, total_steps 32000
current learning rate: 0.0002901062411314615
decreased learning rate by  0.94
Display training progress at (epoch 104, total_steps 33280)
loss: 0.255076938867569
mAP: 0.807252358490566
average data loading time: 0.12467489242553711
average forward time: 0.1509113311767578
average backward time: 0.2820068597793579
end of display 

current learning rate: 0.00027269986666357375
decreased learning rate by  0.94
Display training progress at (epoch 108, total_steps 34560)
loss: 0.2542307674884796
mAP: 0.8062592128537738
average data loading time: 0.13373461961746216
average forward time: 0.16602877378463746
average backward time: 0.28494776487350465
end of display 

Display validation results at (epoch 110, total_steps 35200)
validation mAP is: 0.849609375
validation loss is: 0.25311313072840375
end of display 

current learning rate: 0.00025633787466375937
decreased learning rate by  0.94
Display training progress at (epoch 112, total_steps 35840)
loss: 0.25346970558166504
mAP: 0.8075361143867925
average data loading time: 0.14058021306991578
average forward time: 0.17031234502792358
average backward time: 0.29089864492416384
end of display 

current learning rate: 0.00024095760218393377
decreased learning rate by  0.94
Display training progress at (epoch 116, total_steps 37120)
loss: 0.252754183113575
mAP: 0.8085292600235847
average data loading time: 0.1271503210067749
average forward time: 0.1601827025413513
average backward time: 0.27700008153915406
end of display 

Display training progress at (epoch 120, total_steps 38400)
loss: 0.2520826175808907
mAP: 0.8071104805424529
average data loading time: 0.15400017499923707
average forward time: 0.18187006711959838
average backward time: 0.3363611102104187
end of display 

Display validation results at (epoch 120, total_steps 38400)
validation mAP is: 0.7958333333333334
validation loss is: 0.25201596816380817
end of display 

saving the model at the end of epoch 120, total_steps 38400
current learning rate: 0.00022650014605289773
decreased learning rate by  0.94
Display training progress at (epoch 124, total_steps 39680)
loss: 0.2514459311962128
mAP: 0.8066848466981134
average data loading time: 0.13590633869171143
average forward time: 0.18644611835479735
average backward time: 0.3140923500061035
end of display 

current learning rate: 0.00021291013728972385
decreased learning rate by  0.94
Display training progress at (epoch 128, total_steps 40960)
loss: 0.25074262991547586
mAP: 0.8083873820754717
average data loading time: 0.14010063409805298
average forward time: 0.15743998289108277
average backward time: 0.27575241327285765
end of display 

Display validation results at (epoch 130, total_steps 41600)
validation mAP is: 0.7420572916666667
validation loss is: 0.2511085669199626
end of display 

current learning rate: 0.00020013552905234042
decreased learning rate by  0.94
Display training progress at (epoch 132, total_steps 42240)
loss: 0.25019505321979524
mAP: 0.8082455041273585
average data loading time: 0.14150846004486084
average forward time: 0.17615214586257935
average backward time: 0.3080379366874695
end of display 

current learning rate: 0.00018812739730919998
decreased learning rate by  0.94
Display training progress at (epoch 136, total_steps 43520)
loss: 0.24966789931058883
mAP: 0.807252358490566
average data loading time: 0.12900420427322387
average forward time: 0.15582865476608276
average backward time: 0.2871135830879211
end of display 

Display training progress at (epoch 140, total_steps 44800)
loss: 0.24916457235813141
mAP: 0.8076779923349056
average data loading time: 0.13167209625244142
average forward time: 0.16754032373428346
average backward time: 0.3090473055839539
end of display 

Display validation results at (epoch 140, total_steps 44800)
validation mAP is: 0.7958333333333334
validation loss is: 0.2491804858048757
end of display 

saving the model at the end of epoch 140, total_steps 44800
current learning rate: 0.000176839753470648
decreased learning rate by  0.94
Display training progress at (epoch 144, total_steps 46080)
loss: 0.24871228039264678
mAP: 0.807252358490566
average data loading time: 0.15108431577682496
average forward time: 0.1800657629966736
average backward time: 0.32610833644866943
end of display 

current learning rate: 0.0001662293682624091
decreased learning rate by  0.94
Display training progress at (epoch 148, total_steps 47360)
loss: 0.24834269434213638
mAP: 0.8059754569575472
average data loading time: 0.15100088119506835
average forward time: 0.17858444452285765
average backward time: 0.3589709997177124
end of display 

Display validation results at (epoch 150, total_steps 48000)
validation mAP is: 0.7958333333333334
validation loss is: 0.24827301998933157
end of display 

current learning rate: 0.00015625560616666453
decreased learning rate by  0.94
Display training progress at (epoch 152, total_steps 48640)
loss: 0.2480149708688259
mAP: 0.8068267246462264
average data loading time: 0.1554833769798279
average forward time: 0.17823292016983033
average backward time: 0.3195009231567383
end of display 

current learning rate: 0.00014688026979666467
decreased learning rate by  0.94
Display training progress at (epoch 156, total_steps 49920)
loss: 0.2477064996957779
mAP: 0.8066848466981131
average data loading time: 0.16577950716018677
average forward time: 0.18933603763580323
average backward time: 0.35121973752975466
end of display 

Display training progress at (epoch 160, total_steps 51200)
loss: 0.24740346074104308
mAP: 0.8069686025943396
average data loading time: 0.15404781103134155
average forward time: 0.17902846336364747
average backward time: 0.31300048828125
end of display 

Display validation results at (epoch 160, total_steps 51200)
validation mAP is: 0.7420572916666667
validation loss is: 0.24806014200051626
end of display 

saving the model at the end of epoch 160, total_steps 51200
current learning rate: 0.00013806745360886476
decreased learning rate by  0.94
Display training progress at (epoch 164, total_steps 52480)
loss: 0.247146075963974
mAP: 0.8075361143867925
average data loading time: 0.13244377374649047
average forward time: 0.16005733013153076
average backward time: 0.27555676698684695
end of display 

current learning rate: 0.00012978340639233289
decreased learning rate by  0.94
Display training progress at (epoch 168, total_steps 53760)
loss: 0.24687583968043328
mAP: 0.8076779923349056
average data loading time: 0.1367969512939453
average forward time: 0.16866458654403688
average backward time: 0.2992641806602478
end of display 

Display validation results at (epoch 170, total_steps 54400)
validation mAP is: 0.7420572916666667
validation loss is: 0.24742801984151205
end of display 

current learning rate: 0.00012199640200879289
decreased learning rate by  0.94
Display training progress at (epoch 172, total_steps 55040)
loss: 0.2466549538075924
mAP: 0.8071104805424527
average data loading time: 0.12562084197998047
average forward time: 0.15331737995147704
average backward time: 0.2690020322799683
end of display 

current learning rate: 0.00011467661788826532
decreased learning rate by  0.94
Display training progress at (epoch 176, total_steps 56320)
loss: 0.24642159715294837
mAP: 0.8068267246462264
average data loading time: 0.12965162992477416
average forward time: 0.16088775396347046
average backward time: 0.2728996157646179
end of display 

Display training progress at (epoch 180, total_steps 57600)
loss: 0.24620093852281572
mAP: 0.8071104805424529
average data loading time: 0.13904125690460206
average forward time: 0.1574360966682434
average backward time: 0.28531551361083984
end of display 

Display validation results at (epoch 180, total_steps 57600)
validation mAP is: 0.7420572916666667
validation loss is: 0.24682230750719705
end of display 

saving the model at the end of epoch 180, total_steps 57600
current learning rate: 0.00010779602081496939
decreased learning rate by  0.94
Display training progress at (epoch 184, total_steps 58880)
loss: 0.245987119525671
mAP: 0.8081036261792451
average data loading time: 0.13700172901153565
average forward time: 0.15718464851379393
average backward time: 0.2796048879623413
end of display 

current learning rate: 0.00010132825956607122
decreased learning rate by  0.94
Display training progress at (epoch 188, total_steps 60160)
loss: 0.24581878930330275
mAP: 0.8061173349056604
average data loading time: 0.1330374598503113
average forward time: 0.1557095766067505
average backward time: 0.27449253797531126
end of display 

Display validation results at (epoch 190, total_steps 60800)
validation mAP is: 0.7958333333333334
validation loss is: 0.24585324029127756
end of display 

current learning rate: 9.524856399210693e-05
decreased learning rate by  0.94
Display training progress at (epoch 192, total_steps 61440)
loss: 0.2456067256629467
mAP: 0.8081036261792451
average data loading time: 0.13183021545410156
average forward time: 0.15812186002731324
average backward time: 0.28479150533676145
end of display 

current learning rate: 8.95336501525805e-05
decreased learning rate by  0.94
Display training progress at (epoch 196, total_steps 62720)
loss: 0.2454427659511566
mAP: 0.807252358490566
average data loading time: 0.13634997606277466
average forward time: 0.16339619159698487
average backward time: 0.2962247967720032
end of display 

Display training progress at (epoch 200, total_steps 64000)
loss: 0.2452661633491516
mAP: 0.8081036261792454
average data loading time: 0.1308892250061035
average forward time: 0.15813039541244506
average backward time: 0.2781710743904114
end of display 

saving the latest model (epoch 200, total_steps 64000)
Display validation results at (epoch 200, total_steps 64000)
validation mAP is: 0.7420572916666667
validation loss is: 0.24588215351104736
end of display 

saving the model at the end of epoch 200, total_steps 64000
current learning rate: 8.416163114342567e-05
decreased learning rate by  0.94
Display training progress at (epoch 204, total_steps 65280)
loss: 0.2451140306890011
mAP: 0.8076779923349058
average data loading time: 0.12674283981323242
average forward time: 0.15506333112716675
average backward time: 0.26276302337646484
end of display 

current learning rate: 7.911193327482013e-05
decreased learning rate by  0.94
Display training progress at (epoch 208, total_steps 66560)
loss: 0.24497902616858483
mAP: 0.8066848466981134
average data loading time: 0.13614752292633056
average forward time: 0.17456587553024291
average backward time: 0.2949017405509949
end of display 

Display validation results at (epoch 210, total_steps 67200)
validation mAP is: 0.7958333333333334
validation loss is: 0.24505537251631418
end of display 

current learning rate: 7.436521727833093e-05
decreased learning rate by  0.94
Display training progress at (epoch 212, total_steps 67840)
loss: 0.24483143165707588
mAP: 0.8075361143867925
average data loading time: 0.1324462890625
average forward time: 0.1559978485107422
average backward time: 0.27213751077651976
end of display 

current learning rate: 6.990330424163106e-05
decreased learning rate by  0.94
Display training progress at (epoch 216, total_steps 69120)
loss: 0.24469186514616012
mAP: 0.8068267246462264
average data loading time: 0.12825480699539185
average forward time: 0.15091352462768554
average backward time: 0.2651731610298157
end of display 

Display training progress at (epoch 220, total_steps 70400)
loss: 0.24454509988427162
mAP: 0.8071104805424529
average data loading time: 0.13768492937088012
average forward time: 0.15910285711288452
average backward time: 0.28432278633117675
end of display 

Display validation results at (epoch 220, total_steps 70400)
validation mAP is: 0.7958333333333334
validation loss is: 0.24471438427766165
end of display 

saving the model at the end of epoch 220, total_steps 70400
current learning rate: 6.57091059871332e-05
decreased learning rate by  0.94
Display training progress at (epoch 224, total_steps 71680)
loss: 0.2444388449192047
mAP: 0.8055498231132076
average data loading time: 0.13091607093811036
average forward time: 0.15903714895248414
average backward time: 0.2747377038002014
end of display 

current learning rate: 6.17665596279052e-05
decreased learning rate by  0.94
Display training progress at (epoch 228, total_steps 72960)
loss: 0.24431212544441222
mAP: 0.8076779923349056
average data loading time: 0.12749848365783692
average forward time: 0.15199931859970092
average backward time: 0.2650013566017151
end of display 

Display validation results at (epoch 230, total_steps 73600)
validation mAP is: 0.849609375
validation loss is: 0.24401348332564035
end of display 

current learning rate: 5.806056605023088e-05
decreased learning rate by  0.94
Display training progress at (epoch 232, total_steps 74240)
loss: 0.24421474412083627
mAP: 0.8073942364386791
average data loading time: 0.12847861051559448
average forward time: 0.15260164737701415
average backward time: 0.2665215492248535D:\Anaconda3\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters

end of display 

current learning rate: 5.457693208721703e-05
decreased learning rate by  0.94
Display training progress at (epoch 236, total_steps 75520)
loss: 0.2441091239452362
mAP: 0.8085292600235849
average data loading time: 0.14230417013168334
average forward time: 0.16217451095581054
average backward time: 0.29707677364349366
end of display 

Display training progress at (epoch 240, total_steps 76800)
loss: 0.24402410313487052
mAP: 0.8071104805424529
average data loading time: 0.13203953504562377
average forward time: 0.15550711154937744
average backward time: 0.29269587993621826
end of display 

Display validation results at (epoch 240, total_steps 76800)
validation mAP is: 0.849609375
validation loss is: 0.24377745389938354
end of display 

saving the model at the end of epoch 240, total_steps 76800
current learning rate: 5.1302316161984e-05
decreased learning rate by  0.94
Display training progress at (epoch 244, total_steps 78080)
loss: 0.2439413510262966
mAP: 0.8066848466981131
average data loading time: 0.13025693893432616
average forward time: 0.15339351892471315
average backward time: 0.27348282337188723
end of display 

current learning rate: 4.822417719226496e-05
decreased learning rate by  0.94
Display training progress at (epoch 248, total_steps 79360)
loss: 0.24385245218873025
mAP: 0.8075361143867925
average data loading time: 0.14361664056777954
average forward time: 0.1707632064819336
average backward time: 0.30114452838897704
end of display 

Display validation results at (epoch 250, total_steps 80000)
validation mAP is: 0.849609375
validation loss is: 0.24357280631860098
end of display 

current learning rate: 4.533072656072906e-05
decreased learning rate by  0.94
Display training progress at (epoch 252, total_steps 80640)
loss: 0.2437764547765255
mAP: 0.8068267246462263
average data loading time: 0.1570129871368408
average forward time: 0.18947697877883912
average backward time: 0.33116616010665895
end of display 

current learning rate: 4.261088296708532e-05
decreased learning rate by  0.94
Display training progress at (epoch 256, total_steps 81920)
loss: 0.24370394125580788
mAP: 0.80654296875
average data loading time: 0.136622416973114
average forward time: 0.16587464809417723
average backward time: 0.3255118131637573
end of display 

Display training progress at (epoch 260, total_steps 83200)
loss: 0.24362087473273278
mAP: 0.8078198702830189
average data loading time: 0.14970169067382813
average forward time: 0.19550514221191406
average backward time: 0.3622539281845093
end of display 

Display validation results at (epoch 260, total_steps 83200)
validation mAP is: 0.849609375
validation loss is: 0.24339220921198526
end of display 

saving the model at the end of epoch 260, total_steps 83200
current learning rate: 4.005422998906019e-05
decreased learning rate by  0.94
Display training progress at (epoch 264, total_steps 84480)
loss: 0.24355678334832193
mAP: 0.807252358490566
average data loading time: 0.14194073677062988
average forward time: 0.1904950499534607
average backward time: 0.31421101093292236
end of display 

current learning rate: 3.765097618971658e-05
decreased learning rate by  0.94
Display training progress at (epoch 268, total_steps 85760)
loss: 0.2434828132390976
mAP: 0.8082455041273585
average data loading time: 0.1468302607536316
average forward time: 0.17614233493804932
average backward time: 0.29641964435577395
end of display 

Display validation results at (epoch 270, total_steps 86400)
validation mAP is: 0.849609375
validation loss is: 0.2432310233513514
end of display 

current learning rate: 3.539191761833358e-05
decreased learning rate by  0.94
Display training progress at (epoch 272, total_steps 87040)
loss: 0.2434348739683628
mAP: 0.8064010908018867
average data loading time: 0.14867340326309203
average forward time: 0.18016377687454224
average backward time: 0.3218544960021973
end of display 

current learning rate: 3.326840256123357e-05
decreased learning rate by  0.94
Display training progress at (epoch 276, total_steps 88320)
loss: 0.24336563795804977
mAP: 0.8075361143867925
average data loading time: 0.1516831636428833
average forward time: 0.1724238157272339
average backward time: 0.34344197511672975
end of display 

Display training progress at (epoch 280, total_steps 89600)
loss: 0.2433094158768654
mAP: 0.8075361143867925
average data loading time: 0.15109710693359374
average forward time: 0.18044098615646362
average backward time: 0.30067169666290283
end of display 

Display validation results at (epoch 280, total_steps 89600)
validation mAP is: 0.849609375
validation loss is: 0.24308626850446066
end of display 

saving the model at the end of epoch 280, total_steps 89600
current learning rate: 3.1272298407559555e-05
decreased learning rate by  0.94
Display training progress at (epoch 284, total_steps 90880)
loss: 0.24325366094708442
mAP: 0.8076779923349056
average data loading time: 0.13931108713150026
average forward time: 0.16056860685348512
average backward time: 0.29547255039215087
end of display 

current learning rate: 2.9395960503105978e-05
decreased learning rate by  0.94
Display training progress at (epoch 288, total_steps 92160)
loss: 0.24320650175213815
mAP: 0.8071104805424529
average data loading time: 0.14497096538543702
average forward time: 0.1711873769760132
average backward time: 0.3263198733329773
end of display 

Display validation results at (epoch 290, total_steps 92800)
validation mAP is: 0.849609375
validation loss is: 0.24295899271965027
end of display 

current learning rate: 2.7632202872919617e-05
decreased learning rate by  0.94
Display training progress at (epoch 292, total_steps 93440)
loss: 0.24316507428884507
mAP: 0.8061173349056604
average data loading time: 0.12882790565490723
average forward time: 0.16884886026382445
average backward time: 0.2839716672897339
end of display 

current learning rate: 2.597427070054444e-05
decreased learning rate by  0.94
Display training progress at (epoch 296, total_steps 94720)
loss: 0.24310561940073966
mAP: 0.8076779923349056
average data loading time: 0.13093029260635375
average forward time: 0.1573602318763733
average backward time: 0.2769938349723816
end of display 

Display training progress at (epoch 300, total_steps 96000)
loss: 0.24306916892528535
mAP: 0.80654296875
average data loading time: 0.12935057878494263
average forward time: 0.15796879529953003
average backward time: 0.2768484950065613
end of display 

saving the latest model (epoch 300, total_steps 96000)
Display validation results at (epoch 300, total_steps 96000)
validation mAP is: 0.849609375
validation loss is: 0.242845485607783
end of display 

saving the model at the end of epoch 300, total_steps 96000
current learning rate: 2.441581445851177e-05
decreased learning rate by  0.94
